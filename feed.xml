<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="/Content/RSS.xslt" type="text/xsl" media="screen"?>
<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">

	<channel>
		
		<title>Productive Rage</title>
		<link>http://www.productiverage.com/</link>
		<atom:link href="http://www.productiverage.com/feed" rel="self" type="application/rss+xml"/>
		<description>Dan's techie ramblings</description>
		<language>en-gb</language>

		<lastBuildDate>Sat, 22 Aug 2020 21:34:00 GMT</lastBuildDate>
		<docs>http://blogs.law.harvard.edu/tech/rss</docs>
		
		<image>
			<title>Productive Rage</title>
			<url>http://www.productiverage.com/Content/Images/Grouch.jpg</url>
			<width>142</width>
			<height>142</height>
			<link>http://www.productiverage.com/</link>
		</image>

		<xhtml:meta xmlns:xhtml="http://www.w3.org/1999/xhtml" name="robots" content="noindex" /> 

		<item>
			<title>Monitoring my garden&#39;s limited sunlight time period with an Arduino (and some tupperware)</title>
            <link>http://www.productiverage.com/monitoring-my-gardens-limited-sunlight-time-period-with-an-arduino-and-some-tupperware</link>
			<guid>http://www.productiverage.com/monitoring-my-gardens-limited-sunlight-time-period-with-an-arduino-and-some-tupperware</guid>
			<description>&lt;p&gt;My house has a lovely little garden out front. The house and garden itself are elevated one storey above the street (and so my basement is really more of a bizarre ground floor because it has natural light windows but is full of dust and my life-accumulated rubbish is in one room of it while my covid-times &quot;trying to stay fit, not fat&quot; home gym is in the other) and there was no fence around it when I moved in. Meaning that that the &lt;em&gt;interesting characters&lt;/em&gt; that amble past (suffice to say that I went for a nicer house in a slightly on-the-cusp between classy and rougher neighbourhoods as opposed to a less nice house in a posh place) could see in and converse between sips on their 9am double-strength lager. Once fenced off, kitted out with a cute little table and chairs that my friendly neighbours found at a tip and with some lovely raised flower beds installed, it is a &lt;em&gt;delight&lt;/em&gt; in Summer.. only problem is that my house faces the wrong way and so only gets direct sunlight at certain hours of the day. And this time period varies greatly depending upon the time of year - in March, it might not get the light until almost 5pm whilst in July and August it&#39;s getting warm and light and beautiful (well, on the days that English weather allows) more in time for a late lunch.&lt;/p&gt;

&lt;p&gt;The problem is that, even after four years here, I still don&#39;t really have any idea when it&#39;s going to be sunny there for a given time of year and I want to be able to plan opportunities around it - late evening drinks outside with friends, lunch time warm weather meals for myself, just any general chance top up my vitamin D!&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;Rare English sun in my garden (plus cats)&quot; src=&quot;http://www.productiverage.com/Content/Images/Posts/SunnyGardenAndCats.jpg&quot; class=&quot;NoBorder AlwaysFullWidth&quot;&gt;&lt;/p&gt;

&lt;p&gt;I guess that one way to sort this out would be to just keep an eye out on sunny days and take the opportunity whenever it strikes. A more organised plan would be to start a little diary and mark down every fortnight or so through the year when the sun hits the garden and when it leaves.&lt;/p&gt;

&lt;p&gt;But I work in technology, damnnit, and so I expect to be able to solve this using that electronics and magic! (Cue comments about everything looking like a nail when you&#39;re holding a hammer).&lt;/p&gt;

&lt;p&gt;To be &lt;em&gt;really&lt;/em&gt; honest, maybe I&#39;m describing this situation back to front. My friend gave me an &lt;a href=&quot;https://store.arduino.cc/arduino-uno-rev3&quot;&gt;Arduino UNO r3&lt;/a&gt; because he had a kit spare from the coding club that he runs for kids locally and I&#39;d been looking for a use for it.. and this seemed like it!&lt;/p&gt;

&lt;h3&gt;What I needed&lt;/h3&gt;

&lt;p&gt;Being a total Arduino noob (and, since my Electronics GCSE was over 20 years ago now, I&#39;m basically a total hardware noob.. you should have seen the trouble that I had trying to build a custom PC a few years ago; I swear it was easier when I was 14!) I wanted something nice and simple to begin with.&lt;/p&gt;

&lt;p&gt;So I had the starter kit, which included the Arduino board and some jumper cables, a prototyping breadboard and some common components (including, essentially, a photoresistor) and so I figured that all I&#39;d then need is a way to record the light levels periodically, a power source and some sort of container for when it rains (again; England).&lt;/p&gt;

&lt;p&gt;I considered having some sort of fancy wifi server in it that would record the values somehow and let me either poll it from somewhere else or have it push those results to the cloud somewhere but eventually decided to go for what seemed like a simpler, more robust and (presumably) more power efficient mechanism of storing the light values throughout the day - using an SD card. Because I&#39;d got the kit for free (on the agreement that I would try to do something useful with it), I was looking for something cheap to write to an SD card that I&#39;d had lying around since.. well, I guess since whenever SD cards were useful. Could it have been a digital camera? The very concept seems absurd these days, with the quality of camera that even phones from three or four generations ago have.&lt;/p&gt;

&lt;p&gt;I came across something called a &quot;&lt;strong&gt;Deek Robot SD/RTC datalogging shield&lt;/strong&gt;&quot; that would not only write to an SD card but would also keep time due to a small battery mounted on it.&lt;/p&gt;

&lt;p&gt;These are cheap (mine was less than &#163;5 delivered, new from eBay) but documentation is somewhat.. spotty. There is a lot of documentation for the &quot;Adafruit Assembled Data Logging shield&quot; but they cost more like &#163;13+ and I was looking for the cheap option. Considering how much time I spent trying to make it work and find good information, it probably would have made more sense to buy a better supported shield than a knock-off from somewhere.. but I &lt;em&gt;did&lt;/em&gt; get it working eventually, so I&#39;ll share all the code throughout this post!&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;The Arduino UNO r3 with a Deek Robot SD/RTC shield installed&quot; src=&quot;http://www.productiverage.com/Content/Images/Posts/ArduinoAndShield.jpg&quot; class=&quot;NoBorder AlwaysFullWidth&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note: I found a warning that when using this particular shield, &quot;If you have a UNO with a USB type B connector this shield may NOT WORK because the male pins are NOT LONG ENOUGH&quot; on a &lt;a href=&quot;https://forum.arduino.cc/index.php?topic=649395.0&quot;&gt;forum page&lt;/a&gt; - my UNO r3 does have the USB B connector but I&#39;ve not had this problem.. though if you do encounter this problem then maybe some sort of pin extenders or raisers would fix it.&lt;/em&gt;&lt;/p&gt;

&lt;h3&gt;Step 1: Writing to the SD card&lt;/h3&gt;

&lt;p&gt;After reading around, I settled on a library called &lt;a href=&quot;https://github.com/greiman/SdFat&quot;&gt;SdFat&lt;/a&gt; that should handle the disk access for me. I downloaded it from the Github repo and followed the &quot;Importing a .zip Library&quot; instructions on the &lt;a href=&quot;https://www.arduino.cc/en/guide/libraries&quot;&gt;Installing Additional Arduino Libraries&lt;/a&gt; page.&lt;/p&gt;

&lt;p&gt;This allowed me to stack the data logging shield on top of the UNO, put an SD card into the shield, connect the UNO to my PC via a USB lead and upload the following code -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#include &amp;lt;SdFat.h&amp;gt; // https://github.com/greiman/SdFat

// chipSelect = 10 according to &quot;Item description&quot; section of
// https://www.play-zone.ch/de/dk-data-logging-shield-v1-0.html
#define SD_CHIP_SELECT 10

void setup() {
  Serial.begin(9600);

  // See &quot;Note 1&quot; further down about SPI_HALF_SPEED
  SdFat sd;
  if (!sd.begin(SD_CHIP_SELECT, SPI_HALF_SPEED)) {
    Serial.println(&quot;ERROR: sd.begin() failed&quot;);
  }
  else {
    SdFile file;
    if (!file.open(&quot;TestData.txt&quot;, O_WRITE | O_APPEND | O_CREAT)) {
      Serial.println(&quot;ERROR: file.open() failed - unable to write&quot;);
    }
    else {
      file.println(&quot;Hi!&quot;);
      file.close();
      Serial.println(&quot;Successfully wrote to file!&quot;);
    }
  }
}

void loop() { }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The Arduino IDE has an option to view the serial output (the messages written to &quot;Serial.println&quot;) by going to Tools / Serial Monitor. Ensure that the baud rate shown near the bottom right of the window is set to 9600 to match the setting in the code above.&lt;/p&gt;

&lt;p&gt;This happily showed&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Successfully wrote to file!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;in the Serial Monitor&#39;s output and when I yanked the card out and put it into my laptop to see if it had worked, it did indeed have a file on it called &quot;TestData.txt&quot; with a single line saying &quot;Hi!&quot; - an excellent start!&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note 1: In the &quot;sd.begin&quot; call, I specify &lt;strong&gt;SPI_HALF_SPEED&lt;/strong&gt; primarily because that&#39;s what most of the examples that I&#39;ve found use - there is an option &lt;strong&gt;SPI_FULL_SPEED&lt;/strong&gt; but I read in &lt;a href=&quot;https://community.particle.io/t/has-anyone-had-success-hooking-up-an-sd-card-to-the-photon-and-writing-reading-data/24026/41&quot;&gt;an Arduino forum thread&lt;/a&gt; that: &quot;You should be able to use &lt;strong&gt;SPI_FULL_SPEED&lt;/strong&gt; instead, but if that produces communication errors you can use SD_SCK_HZ(4 * MHZ) instead of &lt;strong&gt;SPI_HALF_SPEED&lt;/strong&gt;&quot; and I&#39;m not sure what might be the limiting factor with said communication errors; whether it&#39;s the card or the shield or something else and I&#39;m only going to be writing small amounts of data at relatively infrequent intervals and so I thought that I would err on the safe side and stick with &lt;strong&gt;SPI_HALF_SPEED&lt;/strong&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note 2: In a lot of code samples, in the &quot;setup&quot; method you will see code after the &quot;Serial.begin(..)&quot; call that looks like this:&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;while (!Serial) {
  // wait for serial port to connect - needed for native USB
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;^ This is only needed for particular variants of the Arduino - the &quot;Leonardo&quot;, I believe - and is not required for the UNO and so I haven&#39;t included it in my code.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Gotcha One:&lt;/strong&gt; Initially, I had formatted my SD card (branded as &quot;Elgetec&quot;, who I can&#39;t remember ever hearing of other than on this card) on my Windows laptop - doing a full format, to make absolutely sure that it was as ready for action as possible. However, not only did that full format take a long time, I found that when I left my Arduino shield writing files over a period of a few hours then it would often get reported as being corrupted when I tried to read it. I&#39;ve found that if the &lt;a href=&quot;https://github.com/greiman/SdFat/blob/master/examples/SdFormatter/SdFormatter.ino&quot;&gt;SdFormatter.ino&lt;/a&gt; (from the examples folder of the SdFat GitHub repo) is used then these corruption problems have stopped occurring (and the formatting is much faster!).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Gotcha Two:&lt;/strong&gt; While I was fiddling around with writing to the SD card, particularly when connected to a battery instead of the USB port (where I could use the Serial Monitor to see what was happening), I tried setting the LED_BUILTIN to be on while writing and then go off again when the file was closed. This didn&#39;t work. And it &lt;em&gt;can&#39;t&lt;/em&gt; work, though it took me a lot of reading to find out why. It turns out that the SPI (the &lt;a href=&quot;https://www.arduino.cc/en/reference/SPI&quot;&gt;Serial Peripheral Library&lt;/a&gt;) connection from the Arduino to the Deek Robot shield will use IO pins 10, 11, 12 and 13 for its own communications. 13 happens to be the output used to set the LED_BUILTIN state and so you lose access to setting that built-in LED while this shield is connected. Specifically, &quot;&lt;a href=&quot;https://forum.arduino.cc/index.php?topic=533606.msg3637549#msg3637549&quot;&gt;pin 13 is the SPI clock. Pin 13 is also the built-in led and hence you have a conflict&lt;/a&gt;&quot;.&lt;/p&gt;

&lt;h3&gt;Step 2: Keeping time&lt;/h3&gt;

&lt;p&gt;Since I want to record light levels throughout the day, it&#39;s important to know at what time the recording is being made. The shield that I&#39;m using also includes an &quot;RTC&quot; (a real-time clock) and so I needed to work out how to set that once and then read from it each time I took a light level reading.&lt;/p&gt;

&lt;p&gt;The UNO board itself can do some basic form of time keeping, such as telling you how long it&#39;s been since the board started / was last reset (via the &lt;a href=&quot;https://www.arduino.cc/reference/en/language/functions/time/millis/&quot;&gt;millis()&lt;/a&gt; function) but there are a few limitations with this. You can bake into the compiled code the time at which it was compiled and you &lt;em&gt;could&lt;/em&gt; then use that, in combination with &quot;millis()&quot;, to work out the current time but you will hit problems if power is temporarily lost or if the board is reset (because &quot;millis()&quot; will start from zero again and timing will start again from that baked-in &quot;compiled at&quot; time).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Gotcha Three:&lt;/strong&gt; I didn&#39;t realise when I was first fiddling with this that any time you connected the USB lead, it would reset the board and the program (the &quot;sketch&quot;, in Arduino-speak) would start all over again. (This will only make a difference if you&#39;re using an external power source because otherwise the program would &lt;em&gt;stop&lt;/em&gt; whenever you disconnected the USB lead and there would be nothing running to reset when plugging the USB lead back in! I&#39;ll be talking about external power supplies further down).&lt;/p&gt;

&lt;p&gt;So the next step was using the clock on the shield that I had bought, instead of relying on the clock on the Arduino board itself. To do this, I&#39;d inserted a CR1220 battery and then tested with the following code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#include &amp;lt;Wire.h&amp;gt;
#include &amp;lt;RTClib.h&amp;gt; // https://github.com/adafruit/RTClib

RTC_DS1307 rtc;

void setup() {
  // The clock won&#39;t work with this (thanks https://arduino.stackexchange.com/a/44305!)
  Wire.begin();

  bool rtcWasAlreadyConfigured;
  if (rtc.isrunning()) {
    rtcWasAlreadyConfigured = true;
  }
  else {
    rtc.adjust(DateTime(__DATE__, __TIME__));
    rtcWasAlreadyConfigured = false;
  }

  Serial.begin(9600);

  if (rtcWasAlreadyConfigured) {
    Serial.println(&quot;setup: RTC is already running&quot;);
  }
  else {
    Serial.println(&quot;setup: RTC was not running, so it was set to the time of compilation&quot;);
  }
}

void loop() {
  DateTime now = rtc.now();
  Serial.print(&quot;Year: &quot;);
  Serial.print(now.year());
  Serial.print(&quot; Month: &quot;);
  Serial.print(now.month());
  Serial.print(&quot; Day: &quot;);
  Serial.print(now.day());
  Serial.print(&quot; Hour: &quot;);
  Serial.print(now.hour());
  Serial.print(&quot; Minutes: &quot;);
  Serial.print(now.minute());
  Serial.print(&quot; Seconds: &quot;);
  Serial.print(now.second());
  Serial.println();

  delay(1000);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The first time you run this, you&#39;ll see the first line say:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;setup: RTC was not running, so it was set to the time of compilation&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;.. and then you&#39;ll see the date and time shown every second.&lt;/p&gt;

&lt;p&gt;If you remove the USB cable and then re-insert it then you&#39;ll see the message:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;setup: RTC is already running&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;.. and then the date and time will continue to show every second &lt;em&gt;and it will be the correct date and time&lt;/em&gt; (it won&#39;t have reset each time that the USB cable is connected and the &quot;setup&quot; function is run again).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Gotcha Four:&lt;/strong&gt; When disconnecting and reconnecting the USB lead, sometimes (if not always) I need to close the Serial Monitor and then re-open it otherwise it won&#39;t update and it will say that the COM port is busy if I try to upload a sketch to the board.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Gotcha Five:&lt;/strong&gt; I&#39;ve seen a lot of examples use &quot;RTC_Millis&quot; instead of &quot;RTC_DS1307&quot; in timing code samples. This is &lt;em&gt;not&lt;/em&gt; what we want! That is a timer that is simulated by the board and it just uses the &quot;millis()&quot; function to track time which, as I explained earlier, is no good for persisting time across resets. We want to use &quot;RTC_DS1307&quot; because that uses the RTC on the shield, which &lt;em&gt;will&lt;/em&gt; maintain the time between power cycles due to the battery on the board.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Gotcha Six:&lt;/strong&gt; If you don&#39;t include &quot;Wire.h&quot; and call &quot;Wire.begin();&quot; at the start of setup then the RTC won&#39;t work properly and you will always get the same weird date displayed when you read it:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Year: 2165 Month: 165 Day: 165 Hour: 165 Minutes: 165 Seconds: 85&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3&gt;Step 3: An external power source&lt;/h3&gt;

&lt;p&gt;So far, the board has only been powered up when connected to the USB lead but this is not the only option. There are a few approaches that you can take; a regulated 5V input, the barrel-shaped power jack and the option of applying power to the vin and gnd pins on the board.&lt;/p&gt;

&lt;p&gt;The power jack makes most sense when you are connecting to some sort of wall wart but I want a &quot;disconnected&quot; power supply for outside. I did a bunch of reading on this and some people are just connecting a simple 9V battery to the vin/gnd pins but apparently that&#39;s not very efficient - the amount of power stored in a standard MN1604 9V battery (the common kind that you might use in a smoke alarm) is comparatively low and the vin/gnd pins will be happy with something in the 6V-12V range and there is said to be more loss in regulating 9V to the internal 5V than there would be from a 6V supply.&lt;/p&gt;

&lt;p&gt;So I settled on a rechargable 6V sealed lead acid battery, which I believe is often used in big torches or in remote control cars. I got one for &#163;8 delivered from ebay that is stated to have 4.5Ah (which is a measure, essentially, of how much energy it stores) - for reference, a 9V battery will commonly have about 0.5Ah and so will run out much more quickly. Whatever battery you select, there are ways to eke out more life from them, which I&#39;ll cover shortly.&lt;/p&gt;

&lt;p&gt;It&#39;s completely safe to connect the battery to the vin/gnd ports at the same time as the USB lead is inserted, so you don&#39;t have to worry about only providing power by the battery &lt;em&gt;or&lt;/em&gt; the USB lead and you can safely connect and disconnect the USB lead while the battery is connected as often as you like.&lt;/p&gt;

&lt;h3&gt;Step 4: Capturing light levels&lt;/h3&gt;

&lt;p&gt;The starter kit that I&#39;ve got conveniently included an LDR (a &quot;Light Dependent Resistor&quot; aka a &quot;photo-resistor&quot;) and so I just had to work out how to connect that. I knew that the Arduino has a range of digital input/output pins and that it has some analog input pins but I had to remind myself of some basic electronics to put it all together.&lt;/p&gt;

&lt;p&gt;What you &lt;em&gt;can&#39;t&lt;/em&gt; do is just put 5V into one pin of the LDR and connect the other end of the LDR straight into an analog pin. I&#39;m going to try to make a stab at a simple explanation here and then refer you to someone who can explain it better!&lt;/p&gt;

&lt;p&gt;The analog pin will read a voltage value from between 0 and 5V and allow this to be read in code as a numeric value between 0 and 1023 (inclusive). When we talk about the 5V output pin, this only makes sense in the context of the ground of the board - so the concept of a 5V output with no gnd pin connection makes no sense, there is nothing for that 5V to be measured relative to. So what we need to do is use the varying resistance of the LDR and somehow translate that into a varying voltage to provide to an analog pin (I chose A0 in my build).&lt;/p&gt;

&lt;p&gt;The way to do this is with a &quot;voltage divider&quot;, which is essentially a circuit that looks a bit like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gnd &amp;lt;--&amp;gt; resistor &amp;lt;--&amp;gt; connection-to-analog-input &amp;lt;--&amp;gt; LDR &amp;lt;--&amp;gt; 5V
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If the resistance of the LDR happens to precisely match that resistance of the fixed resistor then precisely 2.5V will be delivered to the analog input. But if the LDR resistance is higher or lower than the fixed resistor&#39;s value then a higher or lower voltage will be delivered to analog pin.&lt;/p&gt;

&lt;p&gt;There is a &lt;a href=&quot;https://learn.adafruit.com/photocells/using-a-photocell&quot;&gt;tutorial on learn.adafruit.com&lt;/a&gt; that does a much better job of explaining it! It also suggests what fixed resistor values that you might use for different environments (eg. are you more interested in granular light level readings at low light levels but don&#39;t mind saturation at high levels or are you more interested in more granular readings at high levels and less granular at lower?) - at the moment, I&#39;m still experimenting with a few different fixed resistor values to see which ones work for my particular climate.&lt;/p&gt;

&lt;p&gt;The shield that I&#39;m using solder pads for mounting components onto but I wasn&#39;t brave enough for that, so I&#39;ve been using the pass-through pins and connecting them to the bread board that came with my starter kit.&lt;/p&gt;

&lt;p&gt;When it&#39;s not connected to a power supply, it looks a bit like this:&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;The Arduino-plus-shield connected to an LDR on a breadboard&quot; src=&quot;http://www.productiverage.com/Content/Images/Posts/ArduinoWithBreadboardAlongside.jpg&quot; class=&quot;NoBorder AlwaysFullWidth&quot;&gt;&lt;/p&gt;

&lt;p&gt;The code to read the light level value looks like this (while running this code, try slowly moving your hand closer and further from covering the sensor to see the value change when it&#39;s read each second) -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;void setup() {
  Serial.begin(9600);
}

void loop() {
  Serial.print(&quot;Light level reading: &quot;);
  Serial.print(analogRead(0));
  Serial.println();

  delay(1000);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In an effort to start putting all of this together into a more robust package, I picked up a pack of self-adhesive felt pads from the supermarket and stuck them to appropriate points under the breadboard -&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;Felt pads to more easily align the breadboard on top of the Arduino and shield&quot; src=&quot;http://www.productiverage.com/Content/Images/Posts/ArduinoBreadboardFeltPads.jpg&quot; class=&quot;NoBorder AlwaysFullWidth&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;Felt pads attached to the underside of the breadboard&quot; src=&quot;http://www.productiverage.com/Content/Images/Posts/ArduinoBreadboardWithFeltPadsAttached.jpg&quot; class=&quot;NoBorder AlwaysFullWidth&quot;&gt;&lt;/p&gt;

&lt;p&gt;.. and then I secured it all together with an elastic band:&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;Arduino plus shield plus breadboard secured in a tower&quot; src=&quot;http://www.productiverage.com/Content/Images/Posts/ArduinoTower.jpg&quot; class=&quot;NoBorder AlwaysFullWidth&quot;&gt;&lt;/p&gt;

&lt;h3&gt;Step 5: Sleeping when not busy&lt;/h3&gt;

&lt;p&gt;In my ideal dream world, I would be able to leave my light level monitoring box outside for a few months. As I explained earlier, due to the direction that my garden faces, the hours at which the sun hits it fully varies by several hours depending upon the time of year. However, NO battery is going to last forever and even with this 4.5Ah battery that is at a 6V output (which is only a small jump down to regulate to 5V), the time that it can keep things running is limited.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note: Recharging via a solar panel sounds interesting but it&#39;s definitely a future iteration possibility at this point!&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;There are, however, some things that can be done to eke out the duration of the battery by reducing the power usage of the board. There are ways to put the board into a &quot;power down&quot; state where it will do less - its timers will stop and its CPU can have a rest. There are tutorials out there about how to put it into this mode and have it only wake up on an &quot;interrupt&quot;, which can be an external circuit setting an input pin (maybe somehow using the RTC on the shield I&#39;m using) &lt;em&gt;or&lt;/em&gt; using something called the &quot;&lt;a href=&quot;https://create.arduino.cc/projecthub/rafitc/what-is-watchdog-timer-fffe20&quot;&gt;Watchdog Timer&lt;/a&gt;&quot; that stays running on the Arduino even when it&#39;s in power down mode.&lt;/p&gt;

&lt;p&gt;I read &lt;em&gt;a lot&lt;/em&gt; of posts and tutorials on this and I really struggled to get it to work. Until, finally, I came across this one: &lt;a href=&quot;https://circuitdigest.com/microcontroller-projects/arduino-sleep-modes-and-how-to-use-them-to-reduce-power-consumption&quot;&gt;Arduino Sleep Modes and How to use them to Save the Power&lt;/a&gt;. It explains in a clear table the difference between the different power-reduced modes (idle, power-save, power-down, etc..) &lt;em&gt;and&lt;/em&gt; it recommends a library called &quot;&lt;a href=&quot;https://github.com/rocketscream/Low-Power&quot;&gt;Low-Power&lt;/a&gt;&quot; that takes all of the hard work out of it.&lt;/p&gt;

&lt;p&gt;Whereas other tutorials talked about calling &quot;sleep_enable()&quot; and &quot;set_sleep_mode(..)&quot; functions and then using &quot;attachInterrupt(..)&quot; and adding some magic method to then undo all of those things, this library allows you to write a one-liner as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;LowPower.powerDown(SLEEP_8S, ADC_OFF, BOD_OFF);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This willl cause the board to go into its most power-saving mode for eight seconds (which is the longest that&#39;s possible when relying upon its internal Watchdog Timer to wake it up).&lt;/p&gt;

&lt;p&gt;No muss, no fuss.&lt;/p&gt;

&lt;p&gt;I haven&#39;t measured yet how long that my complete device can sit outside in its waterproof box on a single charge of a battery but I&#39;m confident that it&#39;s definitely measured in days, not hours - and that was &lt;em&gt;before&lt;/em&gt; introducing this &quot;LowPower.powerDown(..)&quot; call.&lt;/p&gt;

&lt;p&gt;Since I only want a reading every 30-60s, I call &quot;LowPower.powerDown(..)&quot; in a loop so that there are several 8s power down delays. While I haven&#39;t confirmed this yet, I would be astonished if it didn&#39;t last &lt;em&gt;at least&lt;/em&gt; a week out there on one charge. And if I have to bring it in some nights (when it&#39;s dark and I don&#39;t care about light measurements) to charge it, then that&#39;s fine by me (though I&#39;d like to be as infrequently as possible).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Gotcha Seven:&lt;/strong&gt; When entering power-down mode, if you are connected to the USB port in order to use the Serial Monitor to watch what&#39;s going on, ensure that you call &quot;Serial.flush()&quot; before entering power-down, otherwise the message might get buffered up and not fully sent through the serial connection before the board takes a nap.&lt;/p&gt;

&lt;h3&gt;Step 6: Preparing for the outdoors (in the British weather!)&lt;/h3&gt;

&lt;p&gt;I always associate the brand &quot;&lt;a href=&quot;https://www.independent.co.uk/life-style/food-and-drink/how-tupperware-s-fate-was-sealed-a7899771.html&quot;&gt;Tupperware&lt;/a&gt;&quot; as being a very British thing - it&#39;s what we get packed lunches put into and what we get takeaway curries in. At least, I &lt;em&gt;think&lt;/em&gt; that it is - maybe it&#39;s like &quot;hoover&quot;, where everyone uses the phrase &quot;hoover&quot; when they mean their generic vacuum cleaner. Regardless the origin, this seemed like the simplest way to make my device waterproof. The containers are not completely transparent but they shouldn&#39;t make a significant impact on the light levels being recorded by the photo-resistor because they&#39;re also far from opaque. And these containers are sealable, waterproof and come in all shapes and sizes!&lt;/p&gt;

&lt;p&gt;I took my elastic-band-wrapped &quot;stack&quot; of Arduino-plus-shield-plus-breadboard and connected it to the battery -&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;The Arduino &#39;stack&#39; connected to a battery&quot; src=&quot;http://www.productiverage.com/Content/Images/Posts/ArduinoStackWithBattery.jpg&quot; class=&quot;NoBorder AlwaysFullWidth&quot;&gt;&lt;/p&gt;

&lt;p&gt;.. and put in a plastic box. By turning the battery so that it was length-side-up, it was quite a snug fit and meant that the battery wouldn&#39;t slide around inside the box. There wasn&#39;t a lot of space for the stack to move around and so it seemed like quite a secure arrangement:&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;The Arduino &#39;stack&#39; and battery in its waterproof container&quot; src=&quot;http://www.productiverage.com/Content/Images/Posts/ArduinoInBox.jpg&quot; class=&quot;NoBorder AlwaysFullWidth&quot;&gt;&lt;/p&gt;

&lt;h3&gt;Step 7: The final code&lt;/h3&gt;

&lt;p&gt;So far, each code sample has demonstrated &lt;em&gt;aspects&lt;/em&gt; of what I want to do but now it&#39;s time to bring it all fully together.&lt;/p&gt;

&lt;p&gt;In trying to write the following code, I was reminded how much I&#39;ve taken for granted in C# (and other higher level languages) with their string handling! I tried a little C and C++ &lt;em&gt;maaaaany&lt;/em&gt; years ago and so writing Arduino code was a bit of a throwback for me - at first, I was trying to make a char array for a filename and I set the length of the array to be the number of characters that were required for the filename.. silly me, I had forgotten that C strings need to be null-terminated and so you need an extra zero character at the end in order for things to work properly! Failing to do so would not result in a compile or run time error, it would just mean that the files weren&#39;t written properly. Oh, how I&#39;ve been spoilt! But, on the other hand, it also feels kinda good being this close to the &quot;bare metal&quot; :)&lt;/p&gt;

&lt;p&gt;The following sketch will record the light level about twice a minute to a file on the SD card where the filename is based upon the current date (as maintained by the RTC module and its CR1220 battery) -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#include &amp;lt;Wire.h&amp;gt;
#include &amp;lt;SdFat.h&amp;gt;    // https://github.com/greiman/SdFat
#include &amp;lt;RTClib.h&amp;gt;   // https://github.com/adafruit/RTClib
#include &amp;lt;LowPower.h&amp;gt; // https://github.com/rocketscream/Low-Power

// chipSelect = 10 according to &quot;Item description&quot; section of
// https://www.play-zone.ch/de/dk-data-logging-shield-v1-0.html
#define SD_CHIP_SELECT 10

RTC_DS1307 rtc;

void setup() {
  // The clock won&#39;t work with this (thanks https://arduino.stackexchange.com/a/44305!)
  Wire.begin();

  bool rtcWasAlreadyConfigured;
  if (rtc.isrunning()) {
    rtcWasAlreadyConfigured = true;
  }
  else {
    rtc.adjust(DateTime(__DATE__, __TIME__));
    rtcWasAlreadyConfigured = false;
  }

  Serial.begin(9600);

  if (rtcWasAlreadyConfigured) {
    Serial.println(&quot;setup: RTC is already running&quot;);
  }
  else {
    Serial.println(&quot;setup: RTC was not running, so it was set to the time of compilation&quot;);
  }
}

void loop() {
  // Character arrays need to be long enough to store the number of &quot;real&quot; characters plus the
  // null terminator
  char filename[13]; // yyyyMMdd.txt = 12 chars + 1 null
  char timestamp[9]; // 00:00:00     =  8 chars + 1 null
  DateTime now = rtc.now();
  snprintf(filename, sizeof(filename), &quot;%04u%02u%02u.txt&quot;, now.year(), now.month(), now.day());
  snprintf(timestamp, sizeof(timestamp), &quot;%02u:%02u:%02u&quot;, now.hour(), now.minute(), now.second());

  int sensorValue = analogRead(0);

  Serial.print(filename);
  Serial.print(&quot; &quot;);
  Serial.print(timestamp);
  Serial.print(&quot; &quot;);
  Serial.println(sensorValue);

  SdFat sd;
  if (!sd.begin(SD_CHIP_SELECT, SPI_HALF_SPEED)) {
    Serial.println(&quot;ERROR: sd.begin() failed&quot;);
  }
  else {
    SdFile file;
    if (!file.open(filename, O_WRITE | O_APPEND | O_CREAT)) {
      Serial.println(&quot;ERROR: file.open() failed - unable to write&quot;);
    }
    else {
      file.print(timestamp);
      file.print(&quot; Sensor value: &quot;);
      file.println(sensorValue);
      file.close();
    }
  }

  Serial.flush(); // Ensure we finish sending serial messages before going to sleep

  // 4x 8s is close enough to a reading every 30s, which gives me plenty of data
  // - Using this instead of &quot;delay&quot; should mean that the battery will power the device for longer
  for (int i = 0; i &amp;lt; 3; i++) {
    LowPower.powerDown(SLEEP_8S, ADC_OFF, BOD_OFF);
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;At the moment, I&#39;m bringing the box inside each night and then disconnecting the battery, pulling out the card and looking at the values recorded in the file to see if it&#39;s clear when the sun was fully hitting the table that I had placed the box on.&lt;/p&gt;

&lt;p&gt;I&#39;ve only started doing this in the last couple of days and each day has been rather grey and so there haven&#39;t been any sunny periods so that I can confirm that the readings clearly distinguish between &quot;regular daylight&quot; and &quot;sun directly on the table&quot;. Once I get some sun again, I&#39;ll be able to get a better idea - and if I &lt;em&gt;can&#39;t&lt;/em&gt; distinguish well enough then I&#39;ll adjust the pull-down resistor that splits the voltage with the LDR and keep experimenting!&lt;/p&gt;

&lt;p&gt;When I&#39;m happy with the configuration, &lt;em&gt;then&lt;/em&gt; I&#39;ll start experimenting with leaving the box outside for longer to see how long this battery can last in conjunction with the &quot;LowPower.powerDown(..)&quot; calls. One obvious optimisation for my use case would be to continue keeping it in power-down mode between the hours of 10pm and 8am - partly because I know that it will definitely be dark after 10pm and partly because I am &lt;em&gt;not&lt;/em&gt; a morning person and so would not want to be outside before 8am, even if it &lt;em&gt;was&lt;/em&gt; streaming with light (which it wouldn&#39;t be due to when my yard actually gets direct sunlight).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Gotcha Eight:&lt;/strong&gt; The RTC has no awareness of daylight savings time and so I&#39;ll need to take this into account when the clocks change in the UK. I&#39;ll worry about this another day!&lt;/p&gt;

&lt;h3&gt;Step 8: Draw some graphs (one day)&lt;/h3&gt;

&lt;p&gt;As you can tell from the above, I&#39;m still very much in the early phases of gathering data. But, at some point, I&#39;m going to have to &lt;em&gt;use&lt;/em&gt; this data to predict when the yard will get sun for future dates - once I&#39;ve got a few months of data for different times of year, hopefully I&#39;ll be able to do so!&lt;/p&gt;

&lt;p&gt;I foresee a little bit of data-reading and Excel-graph-drawing in my future! There&#39;s just something about seeing &lt;a href=&quot;http://www.productiverage.com/when-a-disk-cache-performs-better-than-an-inmemory-cache-befriending-the-net-gc&quot;&gt;results on a graph&lt;/a&gt; that make everything feel so much more real. As much as I&#39;d like to be able to stare at 1000s of numbers and read them like the Matrix, seeing trends and curves plotted out just feels so much more satisfying and definitive. Maybe there will be a follow-up post with the results, though I feel that they would be much more personal and less useful to the general populace than even &lt;em&gt;my&lt;/em&gt; standard level of esoteric and niche blog posts! Maybe there are some graphs in my Twitter stream&#39;s future!&lt;/p&gt;

&lt;p&gt;On the other hand.. if I learn any more power-saving techniques or have any follow-up information about how long these rechargeable torch-or-remote-control batteries last then maybe &lt;em&gt;that&lt;/em&gt; will be grounds for a follow-up!&lt;/p&gt;

&lt;p&gt;In the meantime, I hope you&#39;ve enjoyed this little journey - and if you&#39;ve tried to do anything similar with these cheap Deek Robot boards, then maybe the code samples here have been of use to you. I hope so! (Because, goodness knows, feeling like a beginner again and getting onto those new forums has been &lt;em&gt;quite&lt;/em&gt; an experience!)&lt;/p&gt;</description>
			<pubDate>Sat, 22 Aug 2020 21:34:00 GMT</pubDate>
		</item>
		<item>
			<title>How are barcodes read?? (Library-less image processing in C#)</title>
            <link>http://www.productiverage.com/how-are-barcodes-read-libraryless-image-processing-in-c-sharp</link>
			<guid>http://www.productiverage.com/how-are-barcodes-read-libraryless-image-processing-in-c-sharp</guid>
			<description>&lt;p&gt;I&#39;ve been using MyFitnessPal and it has the facility to load nutrition information by scanning the barcode on the product. I can guess how the retrieval works once the barcode number is obtained (a big database somewhere) but it struck me that I had no idea how the reading of the barcode &lt;em&gt;itself&lt;/em&gt; worked and.. well, I&#39;m curious and enjoy the opportunity to learn something new to me by writing the code to do it. I do enjoy being able to look up (almost) anything on the internet to find out how it works!&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(For anyone who wants to either play along but not copy-paste the code themselves or for anyone who wants to jump to the end result, I&#39;ve put the code - along with the example image I&#39;ve used in this post - up on a &lt;a href=&quot;https://github.com/ProductiveRage/BarcodeReader&quot;&gt;GitHub repo&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;h3&gt;The plan of attack&lt;/h3&gt;

&lt;p&gt;There are two steps required here:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Read image and try to identify areas that look like barcodes&lt;/li&gt;
&lt;li&gt;Try to extract numbers from the looks-like-a-barcode regions&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;As with anything, these steps may be broken down into smaller tasks. The first step can be done like this:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Barcodes are black and white regions that have content that has steep &quot;gradients&quot; in image intensity horizontally (where there is a change from a black bar to a white space) and little change in intensity vertically (as each bar is a vertical line), so first we greyscale the image and then generate horizontal and vertical intensity gradients values for each point in the image and combine the values by subtracting vertical gradient from horizontal gradient&lt;/li&gt;
&lt;li&gt;These values are normalised so that they are all on the scale zero to one - this data could be portrayed as another greyscale image where the brightest parts are most likely to be within barcodes&lt;/li&gt;
&lt;li&gt;These values are then &quot;spread out&quot; or &quot;blurred&quot; and then a threshold value is applied where every value about it is changed into a 1 and every value below it a 0&lt;/li&gt;
&lt;li&gt;This &quot;mask&quot; (where every value is a 0 or 1) should have identified many of the pixels within the barcodes and we want to group these pixels into distinct objects&lt;/li&gt;
&lt;li&gt;There is a chance, though, that there could be gaps between bars that mean that a single barcode is spread across multiple masked-out objects and we need to try to piece them back together into one area (since the bars are tall and narrow, this may be done by considering a square area over every object and then combining objects whose squared areas overlap into one)&lt;/li&gt;
&lt;li&gt;This process will result in a list of areas that may be barcodes - any that are taller than they are wide are ignored (because barcode regions are always wider than they are tall)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The second step can be broken down into:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Take the maybe-barcode region of the image, greyscale it and then turn into a mask by setting any pixel with an intensity less than a particular threshold to zero and otherwise to one&lt;/li&gt;
&lt;li&gt;Take a horizontal slice across the image region - all of the pixels on the first row of the image - and change the zero-or-one raw data into a list of line lengths where a new line starts at any transition from zero-to-one or one-to-zero (so &quot;01001000110&quot; becomes &quot;1,1,2,1,3,2,1&quot; because there is 1x zero and then 1x one and then 2x zero and then 1x one, etc..)&lt;/li&gt;
&lt;li&gt;These line lengths should correspond to bar sizes (and space-between-bar sizes) if we&#39;ve found a barcode - so run the values through the magic barcode bar-size-reading algorithm (see section 2.1 in &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2859730/&quot;&gt;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2859730/&lt;/a&gt;) and if we get a number (and the checksum is correct) then we&#39;re done, hurrah!&lt;/li&gt;
&lt;li&gt;If we couldn&#39;t get a number from this horizontal slice then move one pixel down and go back around&lt;/li&gt;
&lt;li&gt;If it was not possible to extract a number from any of the slices through the image region then it&#39;s either not a barcode or it&#39;s somehow so distorted in the image that we can&#39;t read it&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This approach is fairly resilient to changes in lighting and orientation because the barcode regions are still likely to have the highest horizontal intensity gradient whether the image is dark or light (and even if &lt;em&gt;part&lt;/em&gt; of the image is light and part of it is dark) and the barcode-reading algorithm works on ratios of bar/space-between-bar widths and these remain constant if the image is rotated.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(Some of the techniques are similar to things that I did in my &lt;a href=&quot;http://www.productiverage.com/face-or-no-face-finding-faces-in-photos-using-c-sharp-and-accordnet&quot;&gt;Face or no face (finding faces in photos using C# and Accord.NET)&lt;/a&gt; and so I&#39;ll be using some of the same code shortly that I described then)&lt;/em&gt;&lt;/p&gt;

&lt;h3&gt;Identifying maybe-barcode images&lt;/h3&gt;

&lt;p&gt;Let&#39;s this image as an example to work with (peanut butter.. I &lt;em&gt;do&lt;/em&gt; love peanut butter) -&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;Delicious peanut butter&quot; src=&quot;http://www.productiverage.com/Content/Images/Posts/PeanutBarcode.jpg&quot; class=&quot;NoBorder AlwaysFullWidth&quot;&gt;&lt;/p&gt;

&lt;p&gt;Before looking at any code, let&#39;s visualise the process.&lt;/p&gt;

&lt;p&gt;We&#39;re going to consider horizontal and vertical gradient intensity maps - at every point in the image we either look to the pixels to the left and to the right (for the horizontal gradient) or we look at the pixels above and below (for the vertical gradient) and the larger the change, the brighter the pixel in the gradient intensity map&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;Horizontal gradient intensity&quot; src=&quot;http://www.productiverage.com/Content/Images/Posts/PeanutBarcode-HorizontalAndVerticalGradients.jpg&quot; class=&quot;NoBorder AlwaysFullWidth&quot;&gt;&lt;/p&gt;

&lt;p&gt;And when they&#39;re combined by subtracting the vertical gradient at each point from the horizontal gradient, it looks lke this:&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;Combined gradient intensity&quot; src=&quot;http://www.productiverage.com/Content/Images/Posts/PeanutBarcode-CombinedGradients.jpg&quot; class=&quot;NoBorder AlwaysFullWidth&quot;&gt;&lt;/p&gt;

&lt;p&gt;If this image is blurred then we get this:&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;Blurred combined gradient intensity&quot; src=&quot;http://www.productiverage.com/Content/Images/Posts/PeanutBarcode-Blurred.jpg&quot; class=&quot;NoBorder AlwaysFullWidth&quot;&gt;&lt;/p&gt;

&lt;p&gt;.. and if we create a binary mask by saying &quot;normalise the intensity values so that their range goes from zero (for the darkest pixel) to one (for the brightest) and then set any pixels that are in the bottom third in terms of intensity to 0 and set the rest to 1&quot; then we get this:&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;Mask of possibly-part-of-a-barcode areas&quot; src=&quot;http://www.productiverage.com/Content/Images/Posts/PeanutBarcode-Mask.jpg&quot; class=&quot;NoBorder AlwaysFullWidth&quot;&gt;&lt;/p&gt;

&lt;p&gt;If each distinct area (where an &quot;area&quot; means &quot;a group of pixels that are connected&quot;) is identified and squares overlaid and centered around the areas then we see this:&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;Mask of possibly-part-of-a-barcode areas, extended into squared areas&quot; src=&quot;http://www.productiverage.com/Content/Images/Posts/PeanutBarcode-MaskSquares.jpg&quot; class=&quot;NoBorder AlwaysFullWidth&quot;&gt;&lt;/p&gt;

&lt;p&gt;.. and if the areas whose bounding squares overlap are combined and then cropped around the white pixels then we end up with this:&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;Combined possibly-a-barcode areas&quot; src=&quot;http://www.productiverage.com/Content/Images/Posts/PeanutBarcode-MaskSquaresCombined.jpg&quot; class=&quot;NoBorder AlwaysFullWidth&quot;&gt;&lt;/p&gt;

&lt;p&gt;This has identified the area around the barcode and also two tiny other areas - when we come to trying to read barcode numbers out of these, the tiny regions will result in no value while the area around the genuine barcode content &lt;em&gt;should&lt;/em&gt; result in a number successfully being read. But I&#39;m getting ahead of myself.. let&#39;s look at the code required to perform the above transformations.&lt;/p&gt;

&lt;p&gt;I&#39;m going to start with a &lt;strong&gt;DataRectangle&lt;/strong&gt; for performing transformations -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public static class DataRectangle
{
    public static DataRectangle&amp;lt;T&amp;gt; For&amp;lt;T&amp;gt;(T[,] values) =&amp;gt; new DataRectangle&amp;lt;T&amp;gt;(values);
}

public sealed class DataRectangle&amp;lt;T&amp;gt;
{
    private readonly T[,] _protectedValues;
    public DataRectangle(T[,] values) : this(values, isolationCopyMayBeBypassed: false) { }
    private DataRectangle(T[,] values, bool isolationCopyMayBeBypassed)
    {
        if ((values.GetLowerBound(0) != 0) || (values.GetLowerBound(1) != 0))
            throw new ArgumentException(&quot;Both dimensions must have lower bound zero&quot;);
        var arrayWidth = values.GetUpperBound(0) + 1;
        var arrayHeight = values.GetUpperBound(1) + 1;
        if ((arrayWidth == 0) || (arrayHeight == 0))
            throw new ArgumentException(&quot;zero element arrays are not supported&quot;);

        Width = arrayWidth;
        Height = arrayHeight;

        if (isolationCopyMayBeBypassed)
            _protectedValues = values;
        else
        {
            _protectedValues = new T[Width, Height];
            Array.Copy(values, _protectedValues, Width * Height);
        }
    }

    /// &amp;lt;summary&amp;gt;
    /// This will always be greater than zero
    /// &amp;lt;/summary&amp;gt;
    public int Width { get; }

    /// &amp;lt;summary&amp;gt;
    /// This will always be greater than zero
    /// &amp;lt;/summary&amp;gt;
    public int Height { get; }

    public T this[int x, int y]
    {
        get
        {
            if ((x &amp;lt; 0) || (x &amp;gt;= Width))
                throw new ArgumentOutOfRangeException(nameof(x));
            if ((y &amp;lt; 0) || (y &amp;gt;= Height))
                throw new ArgumentOutOfRangeException(nameof(y));
            return _protectedValues[x, y];
        }
    }

    public IEnumerable&amp;lt;Tuple&amp;lt;Point, T&amp;gt;&amp;gt; Enumerate(Func&amp;lt;Point, T, bool&amp;gt;? optionalFilter = null)
    {
        for (var x = 0; x &amp;lt; Width; x++)
        {
            for (var y = 0; y &amp;lt; Height; y++)
            {
                var value = _protectedValues[x, y];
                var point = new Point(x, y);
                if (optionalFilter?.Invoke(point, value) ?? true)
                    yield return Tuple.Create(point, value);
            }
        }
    }

    public DataRectangle&amp;lt;TResult&amp;gt; Transform&amp;lt;TResult&amp;gt;(Func&amp;lt;T, TResult&amp;gt; transformer)
    {
        return Transform((value, coordinates) =&amp;gt; transformer(value));
    }

    public DataRectangle&amp;lt;TResult&amp;gt; Transform&amp;lt;TResult&amp;gt;(Func&amp;lt;T, Point, TResult&amp;gt; transformer)
    {
        var transformed = new TResult[Width, Height];
        for (var x = 0; x &amp;lt; Width; x++)
        {
            for (var y = 0; y &amp;lt; Height; y++)
                transformed[x, y] = transformer(_protectedValues[x, y], new Point(x, y));
        }
        return new DataRectangle&amp;lt;TResult&amp;gt;(transformed, isolationCopyMayBeBypassed: true);
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And then I&#39;m going to add a way to load image data into this structure -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public static class BitmapExtensions
{
    /// &amp;lt;summary&amp;gt;
    /// This will return values in the range 0-255 (inclusive)
    /// &amp;lt;/summary&amp;gt;
    // Based on http://stackoverflow.com/a/4748383/3813189
    public static DataRectangle&amp;lt;double&amp;gt; GetGreyscale(this Bitmap image)
    {
        var values = new double[image.Width, image.Height];
        var data = image.LockBits(
            new Rectangle(0, 0, image.Width, image.Height),
            ImageLockMode.ReadOnly,
            PixelFormat.Format24bppRgb
        );
        try
        {
            var pixelData = new Byte[data.Stride];
            for (var lineIndex = 0; lineIndex &amp;lt; data.Height; lineIndex++)
            {
                Marshal.Copy(
                    source: data.Scan0 + (lineIndex * data.Stride),
                    destination: pixelData,
                    startIndex: 0,
                    length: data.Stride
                );
                for (var pixelOffset = 0; pixelOffset &amp;lt; data.Width; pixelOffset++)
                {
                    // Note: PixelFormat.Format24bppRgb means the data is stored in memory as BGR
                    const int PixelWidth = 3;
                    var r = pixelData[pixelOffset * PixelWidth + 2];
                    var g = pixelData[pixelOffset * PixelWidth + 1];
                    var b = pixelData[pixelOffset * PixelWidth];
                    values[pixelOffset, lineIndex] = (0.2989 * r) + (0.5870 * g) + (0.1140 * b);
                }
            }
        }
        finally
        {
            image.UnlockBits(data);
        }
        return DataRectangle.For(values);
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With these classes, we can load an image and calculate the combined horizontal-gradient-minus-vertical-gradient value like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;private static IEnumerable&amp;lt;Rectangle&amp;gt; GetPossibleBarcodeAreasForBitmap(Bitmap image)
{
    var greyScaleImageData = image.GetGreyscale();
    var combinedGradients = greyScaleImageData.Transform((intensity, pos) =&amp;gt;
    {
        // Consider gradients to be zero at the edges of the image because there aren&#39;t pixels
        // both left/right or above/below and so it&#39;s not possible to calculate a real value
        var horizontalChange = (pos.X == 0) || (pos.X == greyScaleImageData.Width - 1)
            ? 0
            : greyScaleImageData[pos.X + 1, pos.Y] - greyScaleImageData[pos.X - 1, pos.Y];
        var verticalChange = (pos.Y == 0) || (pos.Y == greyScaleImageData.Height - 1)
            ? 0
            : greyScaleImageData[pos.X, pos.Y + 1] - greyScaleImageData[pos.X, pos.Y - 1];
        return Math.Max(0, Math.Abs(horizontalChange) - Math.Abs(verticalChange));
    });        

    // .. more will go here soon
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Before jumping straight into the image analysis, though, it&#39;s worth resizing the source image if it&#39;s large. Since this stage of the processing is looking for areas that look approximately like barcodes, we don&#39;t require a lot of granularity - I&#39;m envisaging (as with the MyFitnessPal use case) source images where the barcode takes up a significant space in the image and is roughly aligned with the view port* and so resizing the image such that the largest side is 300px should work well. If you wanted to scan an image where there were many barcodes to process (or even where there was only one but it was very small) then you might want to allow larger inputs than this - the more data that there is, though, the more work that must be done and the slower that the processing will be!&lt;/p&gt;

&lt;p&gt;* &lt;em&gt;(The barcode has to be roughly aligned with the viewport because the approaching of looking for areas with large horizontal variance in intensity with minor vertical variance would not work - as we&#39;ll see later, though, there is considerable margin for error in this approach and perfect alignment is not required)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;A naive approach to this would be force the image so that its largest side is 300px, regardless of what it was originally. However, this is unnecessary if the largest side is already less than 300px (scaling it up will actually give us more work to do) and if the largest side is not much more than 300px then it&#39;s probably not worth doing either - scaling it down may make any barcodes areas fuzzy and risk reducing the effectiveness of the processing while not actually reducing the required work. So I&#39;m going to say that if the largest side of the image is 450px or larger than resize it so that its largest side is 300px and do nothing otherwise. To achieve that, we need a method like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;private static DataRectangle&amp;lt;double&amp;gt; GetGreyscaleData(
    Bitmap image,
    int resizeIfLargestSideGreaterThan,
    int resizeTo)
{
    var largestSide = Math.Max(image.Width, image.Height);
    if (largestSide &amp;lt;= resizeIfLargestSideGreaterThan)
        return image.GetGreyscale();

    int width, height;
    if (image.Width &amp;gt; image.Height)
    {
        width = resizeTo;
        height = (int)(((double)image.Height / image.Width) * width);
    }
    else
    {
        height = resizeTo;
        width = (int)(((double)image.Width / image.Height) * height);
    }
    using var resizedImage = new Bitmap(image, width, height);
    return resizedImage.GetGreyscale();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The next steps are to &quot;normalise&quot; the combined intensity variance values so that they fit the range zero-to-one, to &quot;blur&quot; this data and to then create a binary mask where the brighter pixels get set to one and the darker pixels get set to zero. In other words, to extend the code earlier (that calculated the intensity variance values) like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;private static IEnumerable&amp;lt;Rectangle&amp;gt; GetPossibleBarcodeAreasForBitmap(Bitmap image)
{
    var greyScaleImageData = GetGreyscaleData(
        image,
        resizeIfLargestSideGreaterThan: 450,
        resizeTo: 300
    );
    var combinedGradients = greyScaleImageData.Transform((intensity, pos) =&amp;gt;
    {
        // Consider gradients to be zero at the edges of the image because there aren&#39;t pixels
        // both left/right or above/below and so it&#39;s not possible to calculate a real value
        var horizontalChange = (pos.X == 0) || (pos.X == greyScaleImageData.Width - 1)
            ? 0
            : greyScaleImageData[pos.X + 1, pos.Y] - greyScaleImageData[pos.X - 1, pos.Y];
        var verticalChange = (pos.Y == 0) || (pos.Y == greyScaleImageData.Height - 1)
            ? 0
            : greyScaleImageData[pos.X, pos.Y + 1] - greyScaleImageData[pos.X, pos.Y - 1];
        return Math.Max(0, Math.Abs(horizontalChange) - Math.Abs(verticalChange));
    });

    const int maxRadiusForGradientBlurring = 2;
    const double thresholdForMaskingGradients = 1d / 3;

    var mask = Blur(Normalise(combinedGradients), maxRadiusForGradientBlurring)
        .Transform(value =&amp;gt; (value &amp;gt;= thresholdForMaskingGradients));

    // .. more will go here soon
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To do that we, need a &quot;Normalise&quot; method - which is simple:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;private static DataRectangle&amp;lt;double&amp;gt; Normalise(DataRectangle&amp;lt;double&amp;gt; values)
{
    var max = values.Enumerate().Max(pointAndValue =&amp;gt; pointAndValue.Item2);
    return (max == 0)
        ? values
        : values.Transform(value =&amp;gt; (value / max));
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. and a &quot;Blur&quot; method - which is a little less simple but hopefully still easy enough to follow &lt;em&gt;(for every point, look at the points around it and take an average of all of them; it just looks for a square area, which is fine for small &quot;maxRadius&quot; values but which might be better implemented as a circular area if large &quot;maxRadius&quot; values might be needed, which they aren&#39;t in this code):&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;private static DataRectangle&amp;lt;double&amp;gt; Blur(DataRectangle&amp;lt;double&amp;gt; values, int maxRadius)
{
    return values.Transform((value, point) =&amp;gt;
    {
        var valuesInArea = new List&amp;lt;double&amp;gt;();
        for (var x = -maxRadius; x &amp;lt;= maxRadius; x++)
        {
            for (var y = -maxRadius; y &amp;lt;= maxRadius; y++)
            {
                var newPoint = new Point(point.X + x, point.Y + y);
                if ((newPoint.X &amp;lt; 0) || (newPoint.Y &amp;lt; 0)
                || (newPoint.X &amp;gt;= values.Width) || (newPoint.Y &amp;gt;= values.Height))
                    continue;
                valuesInArea.Add(values[newPoint.X, newPoint.Y]);
            }
        }
        return valuesInArea.Average();
    });
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This gets us to this point:&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;Mask of possibly-part-of-a-barcode areas&quot; src=&quot;http://www.productiverage.com/Content/Images/Posts/PeanutBarcode-Mask.jpg&quot; class=&quot;NoBorder AlwaysFullWidth&quot;&gt;&lt;/p&gt;

&lt;p&gt;.. which feels like good progress!&lt;/p&gt;

&lt;p&gt;Now we need to try to identify distinct &quot;islands&quot; of pixels where each &quot;island&quot; or &quot;object&quot; is a set of points that are within a single connected area. A straightforward way to do that is to look at every point in the mask that is set to 1 and either:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Perform a pixel-style &quot;flood fill&quot; starting at this point in order to find other points in an object&lt;/li&gt;
&lt;li&gt;If this pixel has already been included in such a fill operation, do nothing (because it&#39;s already been accounted for)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This was made easier for me by reading the article &lt;a href=&quot;https://simpledevcode.wordpress.com/2015/12/29/flood-fill-algorithm-using-c-net/&quot;&gt;Flood Fill algorithm (using C#.Net)&lt;/a&gt;..&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;private static IEnumerable&amp;lt;IEnumerable&amp;lt;Point&amp;gt;&amp;gt; GetDistinctObjects(DataRectangle&amp;lt;bool&amp;gt; mask)
{
    // Flood fill areas in the looks-like-bar-code mask to create distinct areas
    var allPoints = new HashSet&amp;lt;Point&amp;gt;(
        mask.Enumerate(optionalFilter: (point, isMasked) =&amp;gt; isMasked).Select(point =&amp;gt; point.Item1)
    );
    while (allPoints.Any())
    {
        var currentPoint = allPoints.First();
        var pointsInObject = GetPointsInObject(currentPoint).ToArray();
        foreach (var point in pointsInObject)
            allPoints.Remove(point);
        yield return pointsInObject;
    }

    // Inspired by code at
    // https://simpledevcode.wordpress.com/2015/12/29/flood-fill-algorithm-using-c-net/
    IEnumerable&amp;lt;Point&amp;gt; GetPointsInObject(Point startAt)
    {
        var pixels = new Stack&amp;lt;Point&amp;gt;();
        pixels.Push(startAt);

        var valueAtOriginPoint = mask[startAt.X, startAt.Y];
        var filledPixels = new HashSet&amp;lt;Point&amp;gt;();
        while (pixels.Count &amp;gt; 0)
        {
            var currentPoint = pixels.Pop();
            if ((currentPoint.X &amp;lt; 0) || (currentPoint.X &amp;gt;= mask.Width)
            || (currentPoint.Y &amp;lt; 0) || (currentPoint.Y &amp;gt;= mask.Height))
                continue;

            if ((mask[currentPoint.X, currentPoint.Y] == valueAtOriginPoint)
            &amp;amp;&amp;amp; !filledPixels.Contains(currentPoint))
            {
                filledPixels.Add(new Point(currentPoint.X, currentPoint.Y));
                pixels.Push(new Point(currentPoint.X - 1, currentPoint.Y));
                pixels.Push(new Point(currentPoint.X + 1, currentPoint.Y));
                pixels.Push(new Point(currentPoint.X, currentPoint.Y - 1));
                pixels.Push(new Point(currentPoint.X, currentPoint.Y + 1));
            }
        }
        return filledPixels;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The problem is that, even with the blurring we performed, there will likely be some groups of distinct objects that are actually part of a single barcode. These areas need to be joined together. It&#39;s quite possible for there to be relatively large gaps in the middle of barcodes (there is in the example that we&#39;ve been looking at) and so we might not easily be able to just take the distinct objects that we&#39;ve got and join together areas that seem &quot;close enough&quot;.&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;Areas that are possibly part of a barcode&quot; src=&quot;http://www.productiverage.com/Content/Images/Posts/PeanutBarcode-MaskObjects.jpg&quot; class=&quot;NoBorder AlwaysFullWidth&quot;&gt;&lt;/p&gt;

&lt;p&gt;On the basis that individual bars in a barcode are tall compared to the largest possible width that any of them can be (which I&#39;ll go into more detail about later on), it seems like a reasonable idea to take any areas that are taller than they are wide and expand their width until they become square. That would give us this:&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;Mask of possibly-part-of-a-barcode areas, extended into squared areas&quot; src=&quot;http://www.productiverage.com/Content/Images/Posts/PeanutBarcode-MaskSquares.jpg&quot; class=&quot;NoBorder AlwaysFullWidth&quot;&gt;&lt;/p&gt;

&lt;p&gt;We&#39;d then work out which of these &quot;squared off&quot; rectangles overlap (if any) and replace overlapping rectangles with rectangles that cover their combined areas, which would look like this:&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;Overlapping squared-off areas that have been combined&quot; src=&quot;http://www.productiverage.com/Content/Images/Posts/PeanutBarcode-MaskSquaresCombinedPreTrimmed.jpg&quot; class=&quot;NoBorder AlwaysFullWidth&quot;&gt;&lt;/p&gt;

&lt;p&gt;The only problem with this is that the combined rectangles extend too far to the left and right of the areas, so we need to trim them down. The will be fairly straightforward because we have the information about what distinct objects there are and each object is just a list of points - so we work out which objects have points within each of the combined bounding areas and then we work out which out of all of the objects for each combined area has the smallest &quot;x&quot; value and smallest &quot;y&quot; value and which have the largest values. That way, we can change the combined bounding areas to only cover actual barcode pixels. Which would leave us with this:&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;Combined possibly-a-barcode areas&quot; src=&quot;http://www.productiverage.com/Content/Images/Posts/PeanutBarcode-MaskSquaresCombined.jpg&quot; class=&quot;NoBorder AlwaysFullWidth&quot;&gt;&lt;/p&gt;

&lt;p&gt;That might sound like a lot of complicated work but if we take a bit of a brute force* approach to it then it can be expressed like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;private static IEnumerable&amp;lt;Rectangle&amp;gt; GetOverlappingObjectBounds(
    IEnumerable&amp;lt;IEnumerable&amp;lt;Point&amp;gt;&amp;gt; objects)
{
    // Translate each &quot;object&quot; (a list of connected points) into a bounding box (squared off if
    // it was taller than it was wide)
    var squaredOffBoundedObjects = new HashSet&amp;lt;Rectangle&amp;gt;(
        objects.Select((points, index) =&amp;gt;
        {
            var bounds = Rectangle.FromLTRB(
                points.Min(p =&amp;gt; p.X),
                points.Min(p =&amp;gt; p.Y),
                points.Max(p =&amp;gt; p.X) + 1,
                points.Max(p =&amp;gt; p.Y) + 1
            );
            if (bounds.Height &amp;gt; bounds.Width)
                bounds.Inflate((bounds.Height - bounds.Width) / 2, 0);
            return bounds;
        })
    );

    // Loop over the boundedObjects and reduce the collection by merging any two rectangles
    // that overlap and then starting again until there are no more bounds merges to perform
    while (true)
    {
        var combinedOverlappingAreas = false;
        foreach (var bounds in squaredOffBoundedObjects)
        {
            foreach (var otherBounds in squaredOffBoundedObjects)
            {
                if (otherBounds == bounds)
                    continue;

                if (bounds.IntersectsWith(otherBounds))
                {
                    squaredOffBoundedObjects.Remove(bounds);
                    squaredOffBoundedObjects.Remove(otherBounds);
                    squaredOffBoundedObjects.Add(Rectangle.FromLTRB(
                        Math.Min(bounds.Left, otherBounds.Left),
                        Math.Min(bounds.Top, otherBounds.Top),
                        Math.Max(bounds.Right, otherBounds.Right),
                        Math.Max(bounds.Bottom, otherBounds.Bottom)
                    ));
                    combinedOverlappingAreas = true;
                    break;
                }
            }
            if (combinedOverlappingAreas)
                break;
        }
        if (!combinedOverlappingAreas)
            break;
    }

    return squaredOffBoundedObjects.Select(bounds =&amp;gt;
    {
        var allPointsWithinBounds = objects
            .Where(points =&amp;gt; points.Any(point =&amp;gt; bounds.Contains(point)))
            .SelectMany(points =&amp;gt; points)
            .ToArray(); // Don&#39;t re-evaluate in the four accesses below
        return Rectangle.FromLTRB(
            left: allPointsWithinBounds.Min(p =&amp;gt; p.X),
            right: allPointsWithinBounds.Max(p =&amp;gt; p.X) + 1,
            top: allPointsWithinBounds.Min(p =&amp;gt; p.Y),
            bottom: allPointsWithinBounds.Max(p =&amp;gt; p.Y) + 1
        );
    });
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;* &lt;em&gt;(There are definitely more efficient ways that this could be done but since we&#39;re only looking at 300px images then we&#39;re not likely to end up with huge amounts of data to deal with)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;To complete the process, we need to do three more things:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Since barcodes are wider than they are tall, we can discard any regions that don&#39;t fit this shape (of which there are two in the example image)&lt;/li&gt;
&lt;li&gt;The remaining regions are expanded a little across so that they more clearly surround the barcode region, rather than being butted right up to it (this will make the barcode reading process a little easier)&lt;/li&gt;
&lt;li&gt;As the regions that have been identified may well be on a resized version of the source image, they may need to scaled up so that they correctly apply to the source&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To do that, we&#39;ll start from this code that we saw earlier:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var mask = Blur(Normalise(combinedGradients), maxRadiusForGradientBlurring)
    .Transform(value =&amp;gt; (value &amp;gt;= thresholdForMaskingGradients));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. and expand it like so (removing the &quot;// .. more will go here soon&quot; comment), using the methods above:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Determine how much the image was scaled down (if it had to be scaled down at all)
// by comparing the width of the potentially-scaled-down data to the source image
var reducedImageSideBy = (double)image.Width / greyScaleImageData.Width;

var mask = Blur(Normalise(combinedGradients), maxRadiusForGradientBlurring)
    .Transform(value =&amp;gt; (value &amp;gt;= thresholdForMaskingGradients));

return GetOverlappingObjectBounds(GetDistinctObjects(mask))
    .Where(boundedObject =&amp;gt; boundedObject.Width &amp;gt; boundedObject.Height)
    .Select(boundedObject =&amp;gt;
    {
        var expandedBounds = boundedObject;
        expandedBounds.Inflate(width: expandedBounds.Width / 10, height: 0);
        expandedBounds.Intersect(
            Rectangle.FromLTRB(0, 0, greyScaleImageData.Width, greyScaleImageData.Height)
        );
        return new Rectangle(
            x: (int)(expandedBounds.X * reducedImageSideBy),
            y: (int)(expandedBounds.Y * reducedImageSideBy),
            width: (int)(expandedBounds.Width * reducedImageSideBy),
            height: (int)(expandedBounds.Height * reducedImageSideBy)
        );
    });
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The final result is that the barcode has been successfully located on the image - hurrah!&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;Barcode located!&quot; src=&quot;http://www.productiverage.com/Content/Images/Posts/PeanutBarcode-Identified.jpg&quot; class=&quot;NoBorder AlwaysFullWidth&quot;&gt;&lt;/p&gt;

&lt;p&gt;With this information, we should be able to extract regions or &quot;sub images&quot; from the source image and attempt to decipher the barcode value in it (presuming that there IS a bar code in it and we haven&#39;t got a false positive match).&lt;/p&gt;

&lt;p&gt;As we&#39;ll see in a moment, the barcode doesn&#39;t have to be perfectly lined up - some rotation is acceptable (depending upon the image, up to around 20 or 30 degrees should be fine). The MyFitnessPal app has a couple of fallbacks that I&#39;ve noticed, such as being able to read barcodes that are upside down or even back to front (which can happen if a barcode is scanned from the wrong side of a transparent wrapper). While I won&#39;t be writing code here for either of those approaches, I&#39;m sure that you could envisage how it could be done - the source image data could be processed as described here and then, if no barcode is read, rotated 180 degrees and re-processed and reversed and re-processed, etc..&lt;/p&gt;

&lt;h3&gt;How to read a bar code&lt;/h3&gt;

&lt;p&gt;A barcode is comprised of both black and white bars - so it&#39;s not just the black parts that are significant, it is the spaces between them as well.&lt;/p&gt;

&lt;p&gt;The format of a barcode is as follows:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Three single-width bars (a black one, a white one and another black one) that are used to gauge what is considered to be a &quot;single width&quot;&lt;/li&gt;
&lt;li&gt;Information for six numbers then appears, where each number is encoded by a sequence of four bars (white, black, white, black) - particular combinations of bar widths relate to particular digits (see below)&lt;/li&gt;
&lt;li&gt;Another guard section appears with five single width bars (white, black, white, black, white)&lt;/li&gt;
&lt;li&gt;Six more numbers appear (using the same bar-width-combinations encoding as before but the groups of four bars are now black, white, black, white)&lt;/li&gt;
&lt;li&gt;A final guard section of three single width bards (black, white, black)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The numbers are encoded using the following system:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; Digit      Bar widths

   0        3, 2, 1, 1
   1        2, 2, 2, 1
   2        2, 1, 2, 2
   3        1, 4, 1, 1
   4        1, 1, 3, 2
   5        1, 2, 3, 1
   6        1, 1, 1, 4
   7        1, 3, 1, 2
   8        1, 2, 1, 3
   9        3, 1, 1, 2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;(Note that every combination of values totals 7 when they added up - this is very helpful later!)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;To see what that looks like in the real world, here&#39;s a slice of that barcode from the jar of peanut butter with each section and each numberic value identified:&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;Barcode numbers interpreted&quot; src=&quot;http://www.productiverage.com/Content/Images/Posts/PeanutBarcode-AnnotatedValues.png&quot; class=&quot;NoBorder AlwaysFullWidth&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(I should point out that the article &lt;a href=&quot;https://habr.com/en/post/439768/&quot;&gt;How does a barcode work?&lt;/a&gt; was extremely helpful in the research I did for this post and I&#39;m very grateful to the author for having written it in such an approachable manner!)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Any combination of bar widths that is not found in the table is considered to be invalid. On the one hand, you might think that this a potential loss; the format could support more combinations of bar widths to encode more values and then more data could be packed into the same space. There is an advantage, however, to having relatively few valid combinations of bar widths - it makes easier to tell whether the information being read appears to be correct. If a combination is encountered that seems incorrect then the read attempt should be aborted and retried. The format has existed for decades and it would make sense, bearing that in mind, to prioritise making it easier for the hardware to read rather prioritising trying to cram as much data in there as possible. There is &lt;em&gt;also&lt;/em&gt; a checksum included in the numerical data to try to catch any &quot;misreads&quot; but when working with low resolutions or hardware with little computing power, the easier that it is to bail out of a scan and to retry the better.&lt;/p&gt;

&lt;p&gt;The way to tackle the reading is to:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Convert the sub image to greyscale&lt;/li&gt;
&lt;li&gt;Create a binary mask so that the darker pixels become 0 and the lighter ones become 1&lt;/li&gt;
&lt;li&gt;Take a single line across the area&lt;/li&gt;
&lt;li&gt;Change the individual 1s and 0s into lengths of continuous &quot;runs&quot; of values

&lt;ul&gt;
&lt;li&gt;eg. 0001100 would become 3, 2, 2 because there are three 0s then two 1s and then two 0s&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;These runs of values will represent the different sized (black and white) bars that were encountered

&lt;ul&gt;
&lt;li&gt;For a larger image, each run length will be longer than for a small image but that won&#39;t matter because when we encounter runs of four bar length values that we think should be interpreted as a single digit, we&#39;ll do some dividing to try to guess the average size of a single width bar&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Take these runs of values, skip through the expected guard regions and try to interpret each set of four bars that is thought to represent a digit of the bar code as that digit&lt;/li&gt;
&lt;li&gt;If successful then perform a checksum calculation on the output and return the value ass a success if it meets expectations&lt;/li&gt;
&lt;li&gt;If the line couldn&#39;t be interpreted as a barcode or the checksum calculation fails then take the next line down and go back to step 4&lt;/li&gt;
&lt;li&gt;If there are no more lines to attempt then a barcode could not be identified in the image&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This processing is fairly light computationally and so there is no need to resize the &quot;may be a barcode&quot; image region before attempting the work. In fact, it&#39;s benefical to &lt;em&gt;not&lt;/em&gt; shrink it as shrinking it will likely make the barcode section fuzzier and that makes the above steps less likely to work - the ideal case for creating a binary mask is where there is no significant &quot;seepage&quot; of pixel intensity between the black bar areas and the white bar areas. That&#39;s not to say that the images have to be crystal clear or perfectly aligned with the camera because the redundancy built into the format works in our favour here - if one line across the image can&#39;t be read because it&#39;s fuzzy then there&#39;s a good chance that one of the other lines will be legible.&lt;/p&gt;

&lt;p&gt;60 length values is the precise number that we expect to find - there is expected to be some blank space before the barcode starts (1) and then a guard section of three single-width lines that we use to gauge bar width (3) and then six numbers that are encoded in four bars each (6x4=24) and then a guard section of five single-width lines (5) and then six numbers (6x4=24) and then a final guard region of three single-width bars, giving 1+3+24+5+24+3=60.&lt;/p&gt;

&lt;p&gt;There will likely be another section of blank content after the barcode that we ignore&lt;/p&gt;

&lt;p&gt;If we don&#39;t want to validate the final guard region then we can work with a barcode image where some of the end of cut off, so long as the data for the 12 digits is there; in this case, 57 lengths if the minimum number that we can accept&lt;/p&gt;

&lt;h3&gt;Reading the numeric value with code&lt;/h3&gt;

&lt;p&gt;I&#39;m going to try to present the code in approximately the same order as the steps presented above. So, firstly we need to convert the sub image to greyscale and create a binary mark from it. Then we&#39;ll go line by line down the image data and try to read a value. So we&#39;ll take this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public static string? TryToReadBarcodeValue(Bitmap subImage)
{
    const double threshold = 0.5;

     // Black lines are considered 1 and so we set to true if it&#39;s a dark pixel (and 0 if light)
    var mask = subImage.GetGreyscale().Transform(intensity =&amp;gt; intensity &amp;lt; (256 * threshold));
    for (var y = 0; y &amp;lt; mask.Height; y++)
    {
        var value = TryToReadBarcodeValueFromSingleLine(mask, y);
        if (value is object)
            return value;
    }
    return null;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. and the read-each-slice-of-the-image code looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;private static string? TryToReadBarcodeValueFromSingleLine(
    DataRectangle&amp;lt;bool&amp;gt; barcodeDetails,
    int sliceY)
{
    if ((sliceY &amp;lt; 0) || (sliceY &amp;gt;= barcodeDetails.Height))
        throw new ArgumentOutOfRangeException(nameof(sliceY));

    var lengths = GetBarLengthsFromBarcodeSlice(barcodeDetails, sliceY).ToArray();
    if (lengths.Length &amp;lt; 57)
    {
        // As explained, we&#39;d like 60 bars (which would include the final guard region) but we
        // can still make an attempt with 57 (but no fewer)
        // - There will often be another section of blank content after the barcode that we ignore
        // - If we don&#39;t want to validate the final guard region then we can work with a barcode
        //   image where some of the end is cut off, so long as the data for the 12 digits is
        //   there (this will be the case where there are only 57 lengths)
        return null;
    }

    var offset = 0;
    var extractedNumericValues = new List&amp;lt;int&amp;gt;();
    for (var i = 0; i &amp;lt; 14; i++)
    {
        if (i == 0)
        {
            // This should be the first guard region and it should be a pattern of three single-
            // width bars
            offset += 3;
        }
        else if (i == 7)
        {
            // This should be the guard region in the middle of the barcode and it should be a
            // pattern of five single-width bars
            offset += 5;
        }
        else
        { 
            var value = TryToGetValueForLengths(
                lengths[offset],
                lengths[offset + 1],
                lengths[offset + 2],
                lengths[offset + 3]
            );
            if (value is null)
                return null;
            extractedNumericValues.Add(value.Value);
            offset += 4;
        }
    }

    // Calculate what the checksum should be based upon the first 11 numbers and ensure that
    // the 12th matches it
    if (extractedNumericValues.Last() != CalculateChecksum(extractedNumericValues.Take(11)))
        return null;

    return string.Join(&quot;&quot;, extractedNumericValues);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With the code below, we find the runs of continous 0 or 1 lengths that will represent bars are return that list (again, for larger images each run will be longer and for smaller images each run will be shorter but this will be taken care of later) -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;private static IEnumerable&amp;lt;int&amp;gt; GetBarLengthsFromBarcodeSlice(
    DataRectangle&amp;lt;bool&amp;gt; barcodeDetails,
    int sliceY)
{
    if ((sliceY &amp;lt; 0) || (sliceY &amp;gt;= barcodeDetails.Height))
        throw new ArgumentOutOfRangeException(nameof(sliceY));

    // Take the horizontal slice of the data
    var values = new List&amp;lt;bool&amp;gt;();
    for (var x = 0; x &amp;lt; barcodeDetails.Width; x++)
        values.Add(barcodeDetails[x, sliceY]);

    // Split the slice into bars - we only care about how long each segment is when they
    // alternate, not whether they&#39;re dark bars or light bars
    var segments = new List&amp;lt;Tuple&amp;lt;bool, int&amp;gt;&amp;gt;();
    foreach (var value in values)
    {
        if ((segments.Count == 0) || (segments[^1].Item1 != value))
            segments.Add(Tuple.Create(value, 1));
        else
            segments[^1] = Tuple.Create(value, segments[^1].Item2 + 1);
    }
    if ((segments.Count &amp;gt; 0) &amp;amp;&amp;amp; !segments[0].Item1)
    {
        // Remove the white space before the first bar
        segments.RemoveAt(0);
    }
    return segments.Select(segment =&amp;gt; segment.Item2);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we need to implement the &quot;TryToGetValueForLengths&quot; method that &quot;TryToReadBarcodeValueFromSingleLine&quot; calls. This takes four bar lengths that are thought to represent a single digit in the bar code value (they are not part of a guard region or anything like that). It take those four bar lengths and guesses how many pixels across a single bar would be - which is made my simpler by the fact that all of the possible combinations of bar lengths in the lookup chart that we saw earlier add up to 7.&lt;/p&gt;

&lt;p&gt;There&#39;s a little flexibility introduced here to try to account for a low quality image or if the threshold was a bit strong in the creation of the binary mask; we&#39;ll take that calculated expected width of a single bar and tweak it up or down a little if apply that division to the bar lengths means that we made some of the bars too small that they disappeared or too large and it seemed like the total width would be more than seven single estimated-width bars. There&#39;s only a &lt;em&gt;little&lt;/em&gt; flexibility here because if we fail then we can always try another line of the image! (Or maybe it will turn out that this sub image was a false positive match and there isn&#39;t a bar code in it at all).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;private static int? TryToGetValueForLengths(int l0, int l1, int l2, int l3)
{
    if (l0 &amp;lt;= 0)
        throw new ArgumentOutOfRangeException(nameof(l0));
    if (l1 &amp;lt;= 0)
        throw new ArgumentOutOfRangeException(nameof(l1));
    if (l2 &amp;lt;= 0)
        throw new ArgumentOutOfRangeException(nameof(l2));
    if (l3 &amp;lt;= 0)
        throw new ArgumentOutOfRangeException(nameof(l3));

    // Take a guess at what the width of a single bar is based upon these four values
    // (the four bars that encode a number should add up to a width of seven)
    var raw = new[] { l0, l1, l2, l3 };
    var singleWidth = raw.Sum() / 7d;
    var adjustment = singleWidth / 10;
    var attemptedSingleWidths = new HashSet&amp;lt;double&amp;gt;();
    while (true)
    {
        var normalised = raw.Select(x =&amp;gt; Math.Max(1, (int)Math.Round(x / singleWidth))).ToArray();
        var sum = normalised.Sum();
        if (sum == 7)
            return TryToGetNumericValue(normalised[0], normalised[1], normalised[2], normalised[3]);

        attemptedSingleWidths.Add(singleWidth);
        if (sum &amp;gt; 7)
            singleWidth += adjustment;
        else
            singleWidth -= adjustment;
        if (attemptedSingleWidths.Contains(singleWidth))
        {
            // If we&#39;ve already tried this width-of-a-single-bar value then give up -
            // it doesn&#39;t seem like we can make the input values make sense
            return null;
        }
    }

    static int? TryToGetNumericValue(int i0, int i1, int i2, int i3)
    {
        var lookFor = string.Join(&quot;&quot;, new[] { i0, i1, i2, i3 });
        var lookup = new[]
        {
            // These values correspond to the lookup chart shown earlier
            &quot;3211&quot;, &quot;2221&quot;, &quot;2122&quot;, &quot;1411&quot;, &quot;1132&quot;, &quot;1231&quot;, &quot;1114&quot;, &quot;1312&quot;, &quot;1213&quot;, &quot;3112&quot;
        };
        for (var i = 0; i &amp;lt; lookup.Length; i++)
        {
            if (lookFor == lookup[i])
                return i;
        }
        return null;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally we need the CalculateChecksum method (as noted in the code, there&#39;s a great explanation of how to do this in &lt;a href=&quot;https://en.wikipedia.org/wiki/Check_digit#UPC&quot;&gt;wikipedia&lt;/a&gt;) -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;private static int CalculateChecksum(IEnumerable&amp;lt;int&amp;gt; values)
{
    if (values == null)
        throw new ArgumentNullException(nameof(values));
    if (values.Count() != 11)
        throw new ArgumentException(&quot;Should be provided with precisely 11 values&quot;);

    // See https://en.wikipedia.org/wiki/Check_digit#UPC
    var checksumTotal = values
        .Select((value, index) =&amp;gt; (index % 2 == 0) ? (value * 3) : value)
        .Sum();
    var checksumModulo = checksumTotal % 10;
    if (checksumModulo != 0)
        checksumModulo = 10 - checksumModulo;
    return checksumModulo;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With this code, we have executed all of the planned steps outlined before.&lt;/p&gt;

&lt;p&gt;It should be noted that, even with the small amount of flexibility in the &quot;TryToGetValueForLengths&quot; method, in the peanut butter bar code example it requires 15 calls to &quot;GetBarLengthsFromBarcodeSlice&quot; until a bar code is successfully matched! Presumably, this is because there is a little more distortion further up the bar code due to the curve of the jar.&lt;/p&gt;

&lt;p&gt;That&#39;s not to say, however, that this approach to bar reading is particularly fussy. The redundancy and simplicity, not to mention the &lt;em&gt;size&lt;/em&gt; of the average bar code, means that there is plenty of opportunity to try reading a sub image in multiple slices until one of them does match. In fact, I mentioned earlier that the barcode doesn&#39;t have to be perfectly at 90 degrees in order to be interpretable and that some rotation is acceptable. This hopefully makes some intuitive sense based upon the logic above and how it doesn&#39;t matter how long each individual bar code line is because they are averaged out - if a bar code was rotated a little and then a read was attempted of it line by line then the ratios between each line should remain consistent and the same data should be readable.&lt;/p&gt;

&lt;p&gt;To illustrate, here&#39;s a zoomed-in section of the middle of the peanut butter bar code in the orientation shown so far:&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;A strip of the peanut butter jar&#39;s bar code&quot; src=&quot;http://www.productiverage.com/Content/Images/Posts/PeanutBarcode-SingleStrip.jpg&quot; class=&quot;NoBorder AlwaysFullWidth&quot;&gt;&lt;/p&gt;

&lt;p&gt;If we then rotate it like this:&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;The peanut butter jar rotated slightly&quot; src=&quot;http://www.productiverage.com/Content/Images/Posts/PeanutBarcode-Rotated.png&quot; class=&quot;NoBorder AlwaysFullWidth&quot;&gt;&lt;/p&gt;

&lt;p&gt;.. then the code above will still read the value correctly because a strip across the rotated bar code looks like this:&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;A strip of the peanut butter jar&#39;s bar code from the rotated image&quot; src=&quot;http://www.productiverage.com/Content/Images/Posts/PeanutBarcode-SingleStripFromRotated.png&quot; class=&quot;NoBorder AlwaysFullWidth&quot;&gt;&lt;/p&gt;

&lt;p&gt;Hopefully it&#39;s clear enough that, for each given line, the ratios are essentially the same as for the non-rotated strip:&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;A strip of the peanut butter jar&#39;s bar code&quot; src=&quot;http://www.productiverage.com/Content/Images/Posts/PeanutBarcode-SingleStrip.jpg&quot; class=&quot;NoBorder AlwaysFullWidth&quot;&gt;&lt;/p&gt;

&lt;p&gt;To get a reading from an image that is rotated more than this requires a very clear source image and will still be limited by the first stage of processing - that tried to find sections where the horizontal image intensity changed with steep gradients but the vertical intensity did not. If the image is rotated too much then there will be more vertical image intensity differences encountered and it is less likely to identify it as a &quot;maybe a bar code&quot; region.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(Note: I experimented with rotated images that were produced by an online barcode generator and had more success - meaning that I could rotate them more than I could with real photographs - but that&#39;s because those images are generated with stark black and white and the horizontal / vertical intensity gradients are maintained for longer when the image is rotated if they start with such a high level of clarity.. I&#39;m more interested in reading values from real photographs and so I would suggest that only fairly moderate rotation will work - though it would still be plenty for an MyFitnessPal-type app that expects the User to hold the bar code in roughly the right orientation!)&lt;/em&gt;&lt;/p&gt;

&lt;h3&gt;Tying it all together&lt;/h3&gt;

&lt;p&gt;We&#39;ve looked at the separate steps involved in the whole reading process, all that is left is to combine them. The &quot;GetPossibleBarcodeAreasForBitmap&quot; and &quot;TryToReadBarcodeValue&quot; methods can be put together into a fully functioning program like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;static void Main()
{
    using var image = new Bitmap(&quot;Source.jpg&quot;);

    var barcodeValues = new List&amp;lt;string&amp;gt;();
    foreach (var area in GetPossibleBarcodeAreasForBitmap(image))
    {
        using var areaBitmap = new Bitmap(area.Width, area.Height);
        using (var g = Graphics.FromImage(areaBitmap))
        {
            g.DrawImage(
                image,
                destRect: new Rectangle(0, 0, areaBitmap.Width, areaBitmap.Height),
                srcRect: area,
                srcUnit: GraphicsUnit.Pixel
            );
        }
        var valueFromBarcode = TryToReadBarcodeValue(areaBitmap);
        if (valueFromBarcode is object)
            barcodeValues.Add(valueFromBarcode);
    }

    if (!barcodeValues.Any())
        Console.WriteLine(&quot;Couldn&#39;t read any bar codes from the source image :(&quot;);
    else
    {
        Console.WriteLine(&quot;Read the following bar code(s) from the image:&quot;);
        foreach (var barcodeValue in barcodeValues)
            Console.WriteLine(&quot;- &quot; + barcodeValue);
    }

    Console.WriteLine();
    Console.WriteLine(&quot;Press [Enter] to terminate..&quot;);
    Console.ReadLine();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;Finito!&lt;/h3&gt;

&lt;p&gt;And with that, we&#39;re finally done! I must admit that I started writing this post about three years ago and it&#39;s been in my TODO list for a loooooong time now. But I&#39;ve taken a week off work and been able to catch up with a few things and have finally been able to cross it off the list. And I&#39;m quite relieved that I didn&#39;t give up on it entirely because it was a fun little project and coming back to it now allowed me to tidy it up a bit with the newer C# 8 syntax and even enable the nullable reference types option on the project (I sure do hate unintentional nulls being allowed to sneak in!)&lt;/p&gt;

&lt;p&gt;A quick reminder if you want to see it in action or play about it yourself, the &lt;a href=&quot;https://github.com/ProductiveRage/BarcodeReader&quot;&gt;GitHub repo is here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Thanks to anyone that read this far!&lt;/p&gt;</description>
			<pubDate>Fri, 07 Aug 2020 23:24:00 GMT</pubDate>
		</item>
		<item>
			<title>Removing ALL assembly names in Json.NET TypeNameHandling output</title>
            <link>http://www.productiverage.com/removing-all-assembly-names-in-jsonnet-typenamehandling-output</link>
			<guid>http://www.productiverage.com/removing-all-assembly-names-in-jsonnet-typenamehandling-output</guid>
			<description>&lt;p&gt;In some cases, it may be desirable to include type name information in &lt;a href=&quot;https://www.newtonsoft.com/json&quot;&gt;Json.NET&lt;/a&gt; output but for those type names to not include assembly names.&lt;/p&gt;

&lt;p&gt;In my case it&#39;s because I have a &lt;a href=&quot;https://dev.to/rionmonster/sharing-is-caring-using-shared-projects-in-aspnet-e17&quot;&gt;Shared Project&lt;/a&gt; that contains classes that I want to appear in my .NET Core C# server code and in my &lt;a href=&quot;https://bridge.net/&quot;&gt;Bridge.NET&lt;/a&gt; client code and this results in the class names existing in assemblies with different names (but there are also other people with their own cases, such as &lt;a href=&quot;https://stackoverflow.com/questions/8039910/how-do-i-omit-the-assembly-name-from-the-type-name-while-serializing-and-deseria&quot;&gt;How do I omit the assembly name from the type name while serializing and deserializing in JSON.Net?&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Json.NET has support for customising how the type names are emitted and there is an answer in the Stack Overflow question that I linked just above that points to an &lt;a href=&quot;http://james.newtonking.com/archive/2011/11/19/json-net-4-0-release-4-bug-fixes&quot;&gt;article&lt;/a&gt; written by the Json.NET author illustrating how to do it. Essentially, you create a custom serialization binder that looks a bit like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public sealed class TypeNameAssemblyExcludingSerializationBinder : ISerializationBinder
{
    public static TypeNameAssemblyExcludingSerializationBinder Instance { get; }
        = new TypeNameAssemblyExcludingSerializationBinder();

    private TypeNameAssemblyExcludingSerializationBinder() { }

    public void BindToName(Type serializedType, out string assemblyName, out string typeName)
    {
        assemblyName = null;
        typeName = serializedType.FullName;
    }

    public Type BindToType(string assemblyName, string typeName)
    {
        // Note: Some additional work may be required here if the assembly name has been removed
        // and you are not loading a type from the current assembly or one of the core libraries
        return Type.GetType(typeName);
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then you serialise your content something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var json = JsonConvert.SerializeObject(
    new ExampleClass(123, &quot;Test&quot;),
    new JsonSerializerSettings
    {
        Formatting = Formatting.Indented,
        TypeNameHandling = TypeNameHandling.All,
        SerializationBinder = TypeNameAssemblyExcludingSerializationBinder.Instance
    }
);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If the &lt;strong&gt;ExampleClass&lt;/strong&gt; looked like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public sealed class ExampleClass
{
    public ExampleClass(int key, string name)
    {
        Key = key;
        Name = name;
    }
    public int Key { get; }
    public string Name { get; }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. and was in a namespace called &quot;Tester&quot; then the resulting JSON would look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &quot;$type&quot;: &quot;Tester.ExampleClass&quot;,
  &quot;Key&quot;: 123,
  &quot;Name&quot;: &quot;Test&quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To make the difference clear, if the custom serialisation binder had not been used (and if the containing assembly was also called &quot;Tester&quot;) then the JSON would have looked like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &quot;$type&quot;: &quot;Tester.ExampleClass, Tester&quot;,
  &quot;Key&quot;: 123,
  &quot;Name&quot;: &quot;Test&quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So.. problem solved!&lt;/p&gt;

&lt;p&gt;Yes?&lt;/p&gt;

&lt;p&gt;No.&lt;/p&gt;

&lt;h3&gt;ISerializationBinder is not applied to generic type parameters&lt;/h3&gt;

&lt;p&gt;While everything was hunkydory in the example above, there are cases where it isn&#39;t. For example, if we wanted to serialise a &lt;em&gt;list&lt;/em&gt; of &lt;strong&gt;ExampleClass&lt;/strong&gt; instances then we&#39;d have code like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var json = JsonConvert.SerializeObject(
    new List&amp;lt;ExampleClass&amp;gt; { new ExampleClass(123, &quot;Test&quot;) },
    new JsonSerializerSettings
    {
        Formatting = Formatting.Indented,
        TypeNameHandling = TypeNameHandling.All,
        SerializationBinder = TypeNameAssemblyExcludingSerializationBinder.Instance
    }
);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. and the resulting JSON would look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &quot;$type&quot;: &quot;System.Collections.Generic.List`1[[Tester.ExampleClass, Tester]]&quot;,
  &quot;$values&quot;: [
    {
      &quot;$type&quot;: &quot;Tester.ExampleClass&quot;,
      &quot;Key&quot;: 123,
      &quot;Name&quot;: &quot;Test&quot;
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Without the custom serialisation binder, it would have looked like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &quot;$type&quot;: &quot;System.Collections.Generic.List`1[[Tester.ExampleClass, Tester]], System.Private.CoreLib&quot;,
  &quot;$values&quot;: [
    {
      &quot;$type&quot;: &quot;Tester.ExampleClass, Tester&quot;,
      &quot;Key&quot;: 123,
      &quot;Name&quot;: &quot;Test&quot;
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. and so we&#39;ve successfully removed &lt;em&gt;some&lt;/em&gt; of the assembly names as there is no mention of &quot;System.Private.CoreLib&quot; in the List&#39;s type and the $type string for the &lt;strong&gt;ExampleClass&lt;/strong&gt; instance no longer mentions the &quot;Tester&quot; assembly name but the generic type of the List &lt;em&gt;does&lt;/em&gt; mention the &quot;Tester&quot; assembly name and we were trying to prevent assembly names from appearing in the type data!&lt;/p&gt;

&lt;p&gt;I&#39;ve had a good Google around this and there doesn&#39;t seem to be a definitive answer anywhere and I had a need for one, so I&#39;ve put together a solution that does what I need. There is an answer to a similar(ish) stack overflow question &lt;a href=&quot;https://stackoverflow.com/a/19927484/3813189&quot;&gt;here&lt;/a&gt; but it ends with a disclaimer that the regex provided would need tweaking to support nested types and &lt;strong&gt;a)&lt;/strong&gt; I definitely wanted to support nested generic type parameters (eg. a Dictionary that maps string keys to List-of-int values) and &lt;strong&gt;b)&lt;/strong&gt; regexes and me are not the best of friends - hence my going about it my own way!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public sealed class TypeNameAssemblyExcludingSerializationBinder : ISerializationBinder
{
    public static TypeNameAssemblyExcludingSerializationBinder Instance { get; }
        = new TypeNameAssemblyExcludingSerializationBinder();
    private TypeNameAssemblyExcludingSerializationBinder() { }

    public void BindToName(Type serializedType, out string assemblyName, out string typeName)
    {
        // Note: Setting the assemblyName to null here will only remove it from the main type itself -
        // it won&#39;t remove it from any types specified as generic type parameters (that&#39;s what the
        // RemoveAssemblyNames method is needed for)
        assemblyName = null;
        typeName = RemoveAssemblyNames(serializedType.FullName);
    }

    public Type BindToType(string assemblyName, string typeName)
    {
        // Note: Some additional work may be required here if the assembly name has been removed
        // and you are not loading a type from the current assembly or one of the core libraries
        return Type.GetType(typeName);
    }

    private static string RemoveAssemblyNames(string typeName)
    {
        var index = 0;
        var content = new StringBuilder();
        RecusivelyRemoveAssemblyNames();
        return content.ToString();

        void RecusivelyRemoveAssemblyNames()
        {
            // If we started inside a type name - eg.
            //
            //   &quot;System.Int32, System.Private.CoreLib&quot;
            //
            // .. then we want to look for the comma that separates the type name from the assembly
            // information and ignore that content. If we started inside nested generic type content
            // - eg.
            //
            //  &quot;[System.Int32, System.Private.CoreLib], [System.String, System.Private.CoreLib]&quot;
            //
            // .. then we do NOT want to start ignoring content after any commas encountered. So
            // it&#39;s important to know here which case we&#39;re in.
            var insideTypeName = typeName[index] != &#39;[&#39;;

            var ignoreContent = false;
            while (index &amp;lt; typeName.Length)
            {
                var c = typeName[index];
                index++;

                if (insideTypeName &amp;amp;&amp;amp; (c == &#39;,&#39;))
                {
                    ignoreContent = true;
                    continue;
                }

                if (!ignoreContent)
                    content.Append(c);

                if (c == &#39;[&#39;)
                    RecusivelyRemoveAssemblyNames();
                else if (c == &#39;]&#39;)
                {
                    if (ignoreContent)
                    {
                        // If we encountered a comma that indicated that we were about to start
                        // an assembly name then we&#39;ll have stopped adding content to the string
                        // builder but we don&#39;t want to lose this closing brace, so explicitly
                        // add it in if that&#39;s the case
                        content.Append(c);
                    }
                    break;
                }
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;A note about resolving types from type names (without assemblies)&lt;/h3&gt;

&lt;p&gt;In .NET, the &quot;Type.GetType&quot; method will return null if it is given a type name that does not correspond to a type that exists in either the current assembly or in one of the core .NET libraries. In Bridge.NET, it doesn&#39;t appear that they maintained that requirement and I believe that all types are available, even if an assembly name is not specified - but whether it is or isn&#39;t, a similar approach could be used in both cases where you use reflection to look at all loaded assemblies and all of their available types and try to map assembly-name-less type names onto one of those. Getting into this would be completely out of the scope of this post and I&#39;m hoping that you already have an idea in mind if you had got to the point where you wanted to remove all assembly names from your type metadata!&lt;/p&gt;</description>
			<pubDate>Tue, 04 Aug 2020 17:25:00 GMT</pubDate>
		</item>
		<item>
			<title>Private / local C# analysers (without NuGet)</title>
            <link>http://www.productiverage.com/private-local-c-sharp-analysers-without-nuget</link>
			<guid>http://www.productiverage.com/private-local-c-sharp-analysers-without-nuget</guid>
			<description>&lt;p&gt;(&lt;strong&gt;Note:&lt;/strong&gt; The information here depends upon the &quot;new&quot; .csproj format being used.. but it&#39;s not that new any more, so hopefully that&#39;s not a limitation for too many people)&lt;/p&gt;

&lt;p&gt;I&#39;m a big fan of writing analysers to catch common mistakes at compile time rather than run time. For example, the &lt;a href=&quot;https://github.com/ProductiveRage/DanSerialiser&quot;&gt;DanSerialiser&lt;/a&gt;, &lt;a href=&quot;https://github.com/ProductiveRage/Bridge.Immutable&quot;&gt;Bridge.Immutable&lt;/a&gt; and &lt;a href=&quot;https://github.com/ProductiveRage/ProductiveRage.SealedClassVerification&quot;&gt;ProductiveRage.SealedClassVerification&lt;/a&gt; libraries that I&#39;ve published all include some. The way that they&#39;re traditionally distributed is as a NuGet package that installs the analyser into the desired project, which is great if you&#39;re publishing a public package that you expect to be installed via nuget.org. But what if you wanted to create a non-public analyser for something that you were working on, can you do so &lt;em&gt;without&lt;/em&gt; creating a NuGet package? Yes.. but with some caveats.&lt;/p&gt;

&lt;p&gt;If you&#39;re still interested then read on for the details!&lt;/p&gt;

&lt;p&gt;(For anyone who finds themselves in the &lt;strong&gt;&quot;too lazy; didn&#39;t read&quot;&lt;/strong&gt; category, hopefully this gives you enough information as to whether to continue or not)&lt;/p&gt;

&lt;h3&gt;What I wish existed&lt;/h3&gt;

&lt;p&gt;Actually, before I talk about what I wish already existed (but which, unfortunately, does &lt;em&gt;not&lt;/em&gt; exist), I&#39;ll get one option out of the way first; nuget.org is not the only place that NuGet packages can be published to. If you decided that you wanted to write an analyser for some conventions internal to your company then you could create a NuGet package and publish it on an &lt;em&gt;internal&lt;/em&gt; NuGet feed. It&#39;s pretty easy and you have a range of options such as a private NuGet feed service within your network, a private hosted service (possible with MyGet, I believe) or you can even chuck all of your private NuGet .nupkg files into a folder (on your local computer or, I presume, on a network - though I&#39;ve not tested that option) and then add that as a NuGet feed in Visual Studio. This &lt;em&gt;is&lt;/em&gt; straight forward but, still, occasionally I wish that it was possible to include an analyser project as part of a solution and have that analyser added to one of the other projects. Which brings me to..&lt;/p&gt;

&lt;p&gt;What I&#39;ve really wanted, from time to time, is to be able to have one project (say, &quot;MyLibrary&quot;) in a solution and another project (say, &quot;MyAnalyser&quot;) where the second project is added an analyser reference to the first project.&lt;/p&gt;

&lt;p&gt;I&#39;d like it to be as simple as clicking on References on the &quot;MyLibrary&quot; project, then &quot;Add an Analyzer&quot; and then choosing the &quot;MyAnalyser&quot; project. This, however, is not currently possible.&lt;/p&gt;

&lt;p&gt;It seems that I&#39;m not the only one that thinks that this would be nice, there is an issue on the &lt;a href=&quot;https://github.com/dotnet/roslyn/&quot;&gt;.NET Compiler Platform (&quot;Roslyn&quot;)&lt;/a&gt; repo relating to this: &lt;a href=&quot;https://github.com/dotnet/roslyn/issues/18093&quot;&gt;Adding Analyzers Via a Project Reference&lt;/a&gt;. The first reply is from a Senior Software Engineer at Microsoft who says:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;This would be one of the coolest features ever&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;.. which sounds like a great and promising start!&lt;/p&gt;

&lt;p&gt;However, the issue was raised in March 2017 and I don&#39;t think that any progress has been made on it, so I don&#39;t know when / if it will be tackled*.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(Having said that, just last month it was recategorised from &quot;Backlog&quot; to &quot;IDE: InternalPriority&quot; and even assigned Priority 1 - so maybe this &lt;strong&gt;will&lt;/strong&gt; change in the near future! We&#39;ll have to wait and see)&lt;/em&gt;&lt;/p&gt;

&lt;h3&gt;What &lt;em&gt;does&lt;/em&gt; exist&lt;/h3&gt;

&lt;p&gt;So the bad news is that there is no way in the UI to do what I want. But the good news is that there &lt;em&gt;is&lt;/em&gt; a way to move towards it with some manual .csproj editing.&lt;/p&gt;

&lt;p&gt;If I opened the MyLibrary.csproj from the example earlier then I could add the following section:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;ItemGroup&amp;gt;
  &amp;lt;ProjectReference Include=&quot;..\MyAnalyser\MyAnalyser.csproj&quot;&amp;gt;
    &amp;lt;ReferenceOutputAssembly&amp;gt;false&amp;lt;/ReferenceOutputAssembly&amp;gt;
    &amp;lt;OutputItemType&amp;gt;Analyzer&amp;lt;/OutputItemType&amp;gt;
  &amp;lt;/ProjectReference&amp;gt;
&amp;lt;/ItemGroup&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. and the MyAnalyser would now be added to MyLibrary and it would check over the code that I&#39;d written in MyLibrary project - reporting any resulting messages, warnings or error in the VS Error List. Hurrah!&lt;/p&gt;

&lt;p&gt;It seems like a pity that something seemingly so simple needs to be done by hand-editing the .csproj file instead of there being something in the VS GUI to do this but there are other features where you have to do the same. For example, if you want a project to target multiple frameworks when it&#39;s built then you have to manually edit the .csproj file and rename the &quot;targetframework&quot; node to &quot;targetframeworks&quot; and then type in a semi-colon-delimited list of IDs of frameworks that you&#39;re interested in - eg. from this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;TargetFramework&amp;gt;netcoreapp2.1&amp;lt;/TargetFramework&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. to this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;TargetFrameworks&amp;gt;netcoreapp2.1;net461&amp;lt;/TargetFrameworks&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(It&#39;s quite common to do this in &lt;a href=&quot;https://benchmarkdotnet.org/&quot;&gt;BenchmarkDotNet&lt;/a&gt; projects so that you can see how the results vary when your library is imported into different frameworks)&lt;/p&gt;

&lt;p&gt;The good news is that hand-editing the .csproj file is much easier with the file format that we have now than the old one! So having to do this is not the end of the world.&lt;/p&gt;

&lt;p&gt;It&#39;s not &lt;em&gt;all&lt;/em&gt; rainbows and unicorns, though..&lt;/p&gt;

&lt;h3&gt;What are the downsides?&lt;/h3&gt;

&lt;p&gt;The biggest (and only, so far as I can tell) downside is that it seem like Visual Studio will somehow cache the analyser assembly after it loads it. This means that when you first open the solution, the analyser(s) in the MyAnalyser project will be run against the MyLibrary code and any messages, warnings and errors displayed.. &lt;em&gt;but&lt;/em&gt;, if you then change the MyAnalyser code and rebuild then those changes won&#39;t affect the checks performed against MyLibrary.&lt;/p&gt;

&lt;p&gt;Even if you rebuild the entire solution (rebuilding MyAnalyser first and &lt;em&gt;then&lt;/em&gt; rebuilding MyLibrary, to try to force the new analyser assembly to be loaded).&lt;/p&gt;

&lt;p&gt;Even if you rebuild it and then unload the solution and then reload the solution and build &lt;em&gt;again&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;It seems like the only way to get it to reliably load the new analyser assembly is to close the Visual Studio instance entirely and start it again.&lt;/p&gt;

&lt;p&gt;A cryptic note in the &lt;a href=&quot;https://github.com/dotnet/roslyn/issues/18093&quot;&gt;GitHub issue&lt;/a&gt; that I referenced earlier made me wonder if changing the assembly version of the analyser project would help.. but it didn&#39;t.&lt;/p&gt;

&lt;p&gt;Now, &lt;em&gt;hopefully,&lt;/em&gt; in real world usage this isn&#39;t as bad as it sounds. The process of writing analysers lends itself very nicely to a test driven development style because you can set up a test suite where every test is of the format &quot;for code snippet, will I get the analyser messages that I expect?&quot; and you can build up a nice suite of tests for middle-of-the-road cases and edge cases and have them all run quickly. I actually find this to be the easiest way for me to debug things when I get myself into a situation where I don&#39;t understand why the analyser code isn&#39;t doing what I expect; I write a test with a snippet of code and then debug the test to step through the code. So you should be to get your analyser working nicely without having to test it against your &quot;MyLibrary&quot; code over and over.&lt;/p&gt;

&lt;p&gt;Of course, sometimes you&#39;ll want to run it against your entire code base (otherwise, what was the point of writing it!) and then you &lt;em&gt;will&lt;/em&gt; have to close VS and restart it. And this is inconvenient and I wish that it wasn&#39;t the case.&lt;/p&gt;

&lt;p&gt;I think, though, that you would be in the same situation if you decided to go down the NuGet distribution route (whether from a private or public feed) - in the past, I&#39;ve found that if a new version of a NuGet package includes a new version of an analyser then Visual Studio won&#39;t load the new version of the analyser without me restarting VS. Which is just as frustrating. Maybe this is part of what&#39;s delaying the work on Microsoft&#39;s side; they know that if they make adding analysers easier then they&#39;ll have to fix the cached-analyser-doesn&#39;t-get-updated problem at the same time.&lt;/p&gt;

&lt;h3&gt;To conclude&lt;/h3&gt;

&lt;p&gt;I&#39;m going to keep my eye on that GitHub issue. It would be great to see some movement on it but I have no idea how much weight &quot;IDE: InternalPriority&quot; cases have, even if they are listed as Priority 1 within that category.. to be honest, I&#39;m presuming that Priority 1 means &lt;strong&gt;top priority&lt;/strong&gt; but it&#39;s just as feasible that it means &lt;em&gt;lowest&lt;/em&gt; priority. There&#39;s a nice view of the &lt;a href=&quot;https://github.com/dotnet/roslyn/projects/35#card-16650341&quot;&gt;&quot;IDE: Internal Priority&quot; category in GitHub here&lt;/a&gt; in case you want to join in on the guessing game!&lt;/p&gt;

&lt;p&gt;At the end of the day, though, I still think that this is a powerful technology to have access to and I&#39;d still rather have it with these caveats than not have it at all. I really believe that analysers provide a way to improve code quality and I encourage everyone to have a play around with them!&lt;/p&gt;&lt;div class=&quot;Related&quot;&gt;&lt;h3&gt;You may also be interested in&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;http://www.productiverage.com/creating-a-c-sharp-roslyn-analyser-for-beginners-by-a-beginner&quot;&gt;Creating a C# (&amp;quot;Roslyn&amp;quot;) Analyser - For beginners by a beginner&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;</description>
			<pubDate>Wed, 10 Jul 2019 21:42:00 GMT</pubDate>
		</item>
		<item>
			<title>Type aliases in Bridge.NET (C#)</title>
            <link>http://www.productiverage.com/type-aliases-in-bridgenet-c-sharp</link>
			<guid>http://www.productiverage.com/type-aliases-in-bridgenet-c-sharp</guid>
			<description>&lt;p&gt;Back in 2016, I wrote &lt;a href=&quot;http://www.productiverage.com/writing-react-apps-using-bridgenet-the-dan-way-part-three&quot;&gt;Writing React apps using Bridge.NET - The Dan Way (Part Three)&lt;/a&gt; and I talked about trying to tighten up the representation of values in the type system. One of my pet peeves that I talked about was how &quot;no value&quot; is represented in reference types and, in particular, with strings.&lt;/p&gt;

&lt;p&gt;As a reminder, I was having a rant about how I hate the uncertainty of wondering &quot;should I expect to get null passed in here / returned from here&quot; and I decided to draw a hard line and say that &lt;strong&gt;no&lt;/strong&gt;, in &lt;em&gt;my&lt;/em&gt; code I would &lt;em&gt;never&lt;/em&gt; expect a reference type to have a null value - instead I would always use the &lt;strong&gt;Optional&amp;lt;T&amp;gt;&lt;/strong&gt; struct that I included in my NuGet package &lt;a href=&quot;https://github.com/ProductiveRage/Bridge.Immutable&quot;&gt;ProductiveRage.Immutable&lt;/a&gt;. This allows me to make it clear when a method may return a null value (because its return type would be something like &lt;strong&gt;Optional&amp;lt;PersonDetails&amp;gt;&lt;/strong&gt;) and it would allow me to make it clear when a method will and won&#39;t accept null arguments (it &lt;em&gt;will&lt;/em&gt; if the parameter type is &lt;strong&gt;Optional&amp;lt;T&amp;gt;&lt;/strong&gt; and it &lt;em&gt;won&#39;t&lt;/em&gt; if it&#39;s &lt;em&gt;not&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;Strings, however, have &lt;em&gt;another&lt;/em&gt; &quot;no value&quot; state - when they are blank. If I want to have a method argument whose type indicates &quot;this argument must be a string that is not null AND that is not blank&quot; then we can&#39;t communicate that. To address that, my blog post introduced &lt;em&gt;another&lt;/em&gt; type; the &lt;strong&gt;NonBlankTrimmedString&lt;/strong&gt; -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class NonBlankTrimmedString
{
    public NonBlankTrimmedString(string value)
    {
        if (string.IsNullOrWhiteSpace(value))
            throw new ArgumentException(&quot;Null, blank or whitespace-only value specified&quot;);
        Value = value.Trim();
    }

    /// &amp;lt;summary&amp;gt;
    /// This will never be null, blank or have any leading or trailing whitespace
    /// &amp;lt;/summary&amp;gt;
    public string Value { get; }

    /// &amp;lt;summary&amp;gt;
    /// It&#39;s convenient to be able to pass a NonBlankTrimmedString instance as any argument
    /// that requires a string
    /// &amp;lt;/summary&amp;gt;
    public static implicit operator string(NonBlankTrimmedString value)
    {
        if (value == null)
            throw new ArgumentNullException(&quot;value&quot;);
        return value.Value;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This would allow me to have a method that clearly indicates that it needs a string &lt;em&gt;with a real value&lt;/em&gt; - eg.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;void DoSomething(NonBlankTrimmedString value);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. and it could be combined with &lt;strong&gt;Optional&amp;lt;T&amp;gt;&lt;/strong&gt; to define a method whose type signature indicates that it will take a string with a real value OR it will accept a &quot;no value&quot; - eg.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;void DoSomething(Optional&amp;lt;NonBlankTrimmedString&amp;gt; value);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This method will &lt;em&gt;not&lt;/em&gt; accept a blank string because that&#39;s just another state that is not necessary; either you give me a real (non-blank) value or you don&#39;t. There is no half-way house of &quot;non-null but still with no value&quot;.&lt;/p&gt;

&lt;p&gt;As another example, I might want to write a &lt;a href=&quot;https://github.com/ProductiveRage/Bridge.React&quot;&gt;Bridge.React&lt;/a&gt; component whose &lt;strong&gt;Props&lt;/strong&gt; type can optionally take an additional class name to render as part of the component - in which case, I might write the class a bit like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public sealed class Props
{
    public Props(
        /* .. other property values, */
        Optional&amp;lt;NonBlankTrimmedString&amp;gt; className = new Optional&amp;lt;NonBlankTrimmedString&amp;gt;())
    {
        // .. other properties set here
        ClassName = className;
    }

    // .. other public properties exposed here

    public Optional&amp;lt;NonBlankTrimmedString&amp;gt; ClassName { get; }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is all fine and dandy and, pretty much, it just works. If I want to expand this richer type system so that it&#39;s used in API requests / responses as well then I can have &lt;strong&gt;Optional&amp;lt;T&amp;gt;&lt;/strong&gt; and &lt;strong&gt;NonBlankTrimmedString&lt;/strong&gt; types defined in the .NET code that runs on the server as well as in my Bridge project. And if I want to avoid code duplication then I can define the types in a &lt;a href=&quot;https://dev.to/rionmonster/sharing-is-caring-using-shared-projects-in-aspnet-e17&quot;&gt;Shared Project&lt;/a&gt; that is referenced by both the Bridge project and the server API project.&lt;/p&gt;

&lt;p&gt;One downside to this approach, though, is that JSON payloads from API calls are going to be larger if I wrap all of my strings in &lt;strong&gt;NonBlankTrimmedString&lt;/strong&gt; instances. And there will be more work for &lt;a href=&quot;https://github.com/bridgedotnet/Bridge.Newtonsoft.Json&quot;&gt;Bridge&#39;s version of Newtonsoft Json.NET&lt;/a&gt; to do because it has to parse more data and it has to deserialise more instances of types; for every string, instead of just deserialising a value into a string, it needs to deserialise that string value &lt;em&gt;and then&lt;/em&gt; create an instance of a &lt;strong&gt;NonBlankTrimmedString&lt;/strong&gt; to wrap it. If you have any API calls that return 100s or 1000s of strings then this can become a non-negligible cost.&lt;/p&gt;

&lt;p&gt;The full .NET version of Newtonsoft Json.NET has some flexibility with how types are serialised to/from JSON. For example, if I wanted to tell the serialiser that &lt;strong&gt;NonBlankTrimmedString&lt;/strong&gt; instances should appear in the JSON as plain strings then I could do so using a &lt;strong&gt;JsonConverter&lt;/strong&gt; (there is sample code in the Newtonsoft website that demonstrates how to do it for the &lt;strong&gt;Version&lt;/strong&gt; type and the principle would be exactly the same for &lt;strong&gt;NonBlankTrimmedString&lt;/strong&gt; - see &lt;a href=&quot;https://www.newtonsoft.com/json/help/html/CustomJsonConverterGeneric.htm&quot;&gt;Custom JsonConverter&amp;lt;T&amp;gt;&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;The Bridge version of the library has no support for custom JsonConverters, though, so we may appear to be a bit stuck.. if it weren&#39;t for the fact that Bridge has some low-level tricks that we can use to our advantage.&lt;/p&gt;

&lt;p&gt;In order to allow C# code to be written that interacts with JavaScript libraries, Bridge has a few escape hatches for the type system that we can use in a careful manner. For example, I could rewrite the Bridge version of &lt;strong&gt;NonBlankTrimmedString&lt;/strong&gt; to look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class NonBlankTrimmedString
{
    protected NonBlankTrimmedString() { }

    /// &amp;lt;summary&amp;gt;
    /// This will never be null, blank or have any leading or trailing whitespace
    /// &amp;lt;/summary&amp;gt;
    public extern string Value { [Template(&quot;{this}&quot;)] get; }

    /// &amp;lt;summary&amp;gt;
    /// Create a NonBlankTrimmedString instance by explicitly casting a string
    /// &amp;lt;/summary&amp;gt;
    public static explicit operator NonBlankTrimmedString(string value)
    {
        if (value == null)
            return null;
        value = value.Trim();
        if (value == &quot;&quot;)
            throw new ArgumentException(&quot;Can not cast from a blank or whitespace-only string&quot;);
        return Script.Write&amp;lt;NonBlankTrimmedString&amp;gt;(&quot;value&quot;);
    }

    /// &amp;lt;summary&amp;gt;
    /// It&#39;s convenient to be able to pass a NonBlankTrimmedString instance as any argument
    /// that requires a string
    /// &amp;lt;/summary&amp;gt;
    [Template(&quot;{value}&quot;)]
    public extern static implicit operator string(NonBlankTrimmedString value);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This changes things up a bit. Now there is no public constructor and the only way to get a &lt;strong&gt;NonBlankTrimmedString&lt;/strong&gt; instance from a plain string is to explicitly cast to it - eg.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var x = (NonBlankTrimmedString)&quot;hi!&quot;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If the source string is blank or whitespace-only then attempting to cast it to a &lt;strong&gt;NonBlankTrimmedString&lt;/strong&gt; will result in an exception being thrown.&lt;/p&gt;

&lt;p&gt;What&#39;s interesting about this class is that it exists only to provide type information to the C# compiler - there will never be an instance of &lt;strong&gt;NonBlankTrimmedString&lt;/strong&gt; alive runtime in JavaScript. The reason for this is that the explicit cast performs some validation but then, at runtime, returns the string instance directly back; it &lt;em&gt;doesn&#39;t&lt;/em&gt; wrap it in an instance of a &lt;strong&gt;NonBlankTrimmedString&lt;/strong&gt; class. Similarly, when the &quot;Value&quot; property is requested in C# code, this is translated into JS as a direct reference to &quot;this&quot; (which we know is a plain string). This is sounding complicated as I write this, so let me try to make it clear with an example!&lt;/p&gt;

&lt;p&gt;The following C# code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Start with a plain string
var source = &quot;Hi!&quot;;

// Create a NonBlankTrimmed by explicitly casting the string
var x = (NonBlankTrimmedString)source;

// Write the value of the NonBlankTrimmedString to the console
Console.WriteLine(x.Value);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. is translated into this JS:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Start with a plain string
var source = &quot;Hi!&quot;;

// Create a NonBlankTrimmed by explicitly casting the string
var x = Demo.NonBlankTrimmedString.op_Explicit(source);

// Write the value of the NonBlankTrimmedString to the console
System.Console.WriteLine(x);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The reference &quot;x&quot; in the JS runtime is actually just a string (and so the C# &quot;x.Value&quot; is translated into simply &quot;x&quot;) and the explicit operator (the method call &quot;Demo.NonBlankTrimmedString.op_Explicit&quot;) performs some validation but then (if the validation passes) returns the string right back but claims (for the benefit of the C# compiler and type system) that it is now a &lt;strong&gt;NonBlankTrimmedString&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;This has a couple of benefits - now, plain string values that appear in JSON can be deserialised into &lt;strong&gt;NonBlankTrimmedString&lt;/strong&gt; instances by Bridge (while the Bridge version of Json.NET doesn&#39;t support type converters, it &lt;em&gt;does&lt;/em&gt; support deserialising types using implicit or explicit operators - so, here, it would see a string in the JSON and see that the target type was a &lt;strong&gt;NonBlankTrimmedString&lt;/strong&gt; and it would use &lt;strong&gt;NonBlankTrimmedString&lt;/strong&gt;&#39;s explicit operator to instantiate the target type), so the JSON returned from the server can be cleaner. &lt;em&gt;And&lt;/em&gt; it means that the JS runtime doesn&#39;t have to actually create instances of &lt;strong&gt;NonBlankTrimmedString&lt;/strong&gt; to wrap those strings up in, which makes the life of the garbage collector easier (again, may be important if you have API responses that need to return 1000s of &lt;strong&gt;NonBlankTrimmedString&lt;/strong&gt;).&lt;/p&gt;

&lt;p&gt;This is an interesting concept that I&#39;m referring to as a &quot;type alias&quot; - a type that exists only for the compiler and that doesn&#39;t affect the runtime. The phrase &quot;type alias&quot; exists in TypeScript and in F# (and in other languages, I&#39;m sure) but I think that it means something slightly different there.. which may mean that I&#39;ve chosen a confusing name for this C# / Bridge.NET concept! In TypeScript and F#, I don&#39;t believe that they allow the level of compiler validation that I&#39;m talking about - certainly in TypeScript, type aliases are more of a convenience that allow you say something like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Vector = number[];
type Vectors = Vector[];
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. so that you can then write a method signature that looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;function process(data: Vectors) {
    // ..
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. instead of:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;function process(data: number[][]) {
    // ..
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. but the two are identical. TypeScript &quot;type aliases&quot; make things more flexible, &lt;em&gt;not&lt;/em&gt; more constrained. To make that clearer, if you wrote:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type CustomerID = number;

function process(id: CustomerID) {
    // ..
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. then you could still call:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;process(1); // Passing a plain number into a method whose signature specifies type CustomerID
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In other words, the TypeScript alias means &quot;anywhere that you see CustomerID, you can pass in a &#39;number&#39;&quot;. This is the opposite of what I want, I want to be able to have methods that specify that they want a &lt;strong&gt;NonBlankTrimmedString&lt;/strong&gt; and &lt;em&gt;not&lt;/em&gt; just any old string.&lt;/p&gt;

&lt;p&gt;I go into this in a little more detail in the section &quot;Type aliases in other languages&quot; at the end of this blog post. My point here was that maybe &quot;type alias&quot; is not the best phrase to use and maybe I&#39;ll revisit this in the future.&lt;/p&gt;

&lt;p&gt;For now, though, let&#39;s get back to the &lt;strong&gt;NonBlankTrimmedString&lt;/strong&gt; definition that I&#39;ve proposed because it has some downsides, as well. As the type &lt;em&gt;only&lt;/em&gt; exists at compile time and &lt;em&gt;not&lt;/em&gt; at runtine, if I try to query the type of a &lt;strong&gt;NonBlankTrimmedString&lt;/strong&gt; instance at runtime then it will report that it is a &quot;System.String&quot; - this is to be expected, since part of the benefit of this approach is that no additional instances are required other than the plain string itself - but if you were wanted to do some crazy reflection for some reason then it might catch you off guard.&lt;/p&gt;

&lt;p&gt;Another downside is that if I wanted to create specialised versions of &lt;strong&gt;NonBlankTrimmedString&lt;/strong&gt; then I have to duplicate some code. For example, I might want to &lt;a href=&quot;https://andrewlock.net/using-strongly-typed-entity-ids-to-avoid-primitive-obsession-part-1/&quot;&gt;strongly type&lt;/a&gt; my entity IDs and define them as classes derived from &lt;strong&gt;NonBlankTrimmedString&lt;/strong&gt;. With the version of &lt;strong&gt;NonBlankTrimmedString&lt;/strong&gt; from my 2016 blog post, this would be as simple as this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// If NonBlankTrimmedString is a regular class then creating derived types is easy as this
public class OrderID : NonBlankTrimmedString
{
    public OrderID(string value) : base(value) { }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. but with this &quot;type alias&quot; approach, it becomes more verbose -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// The explicit operator needs to be reimplemented for each derived type with the type alias
// alias approach shown earlier :(
public class ClassName : NonBlankTrimmedString
{
    protected ClassName() { }

    public static explicit operator ClassName(string value)
    {
        if (value == null)
            return null;
        value = value.Trim();
        if (value == &quot;&quot;)
            throw new ArgumentException(&quot;Can not cast from a blank or whitespace-only string&quot;);
        return Script.Write&amp;lt;ClassName&amp;gt;(&quot;value&quot;);
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However, we could make this a little simpler by changing the &lt;strong&gt;NonBlankTrimmedString&lt;/strong&gt; type definition to this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class NonBlankTrimmedString
{
    protected NonBlankTrimmedString() { }

    /// &amp;lt;summary&amp;gt;
    /// This will never be null, blank or have any leading or trailing whitespace
    /// &amp;lt;/summary&amp;gt;
    public extern string Value { [Template(&quot;{this}&quot;)] get; }

    /// &amp;lt;summary&amp;gt;
    /// Create a NonBlankTrimmedString instance by explicitly casting a string
    /// &amp;lt;/summary&amp;gt;
    public static explicit operator NonBlankTrimmedString(string value)
        =&amp;gt; Wrap&amp;lt;NonBlankTrimmedString&amp;gt;(value);

    /// &amp;lt;summary&amp;gt;
    /// It&#39;s convenient to be able to pass a NonBlankTrimmedString instance as any argument
    /// that requires a string
    /// &amp;lt;/summary&amp;gt;
    [Template(&quot;{value}&quot;)]
    public extern static implicit operator string(NonBlankTrimmedString value);

    protected static T Wrap&amp;lt;T&amp;gt;(string value) where T : NonBlankTrimmedString
    {
        if (value == null)
            return null;
        value = value.Trim();
        if (value == &quot;&quot;)
            throw new ArgumentException(&quot;Can not cast from a blank or whitespace-only string&quot;);
        return Script.Write&amp;lt;T&amp;gt;(&quot;value&quot;);
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. and then derived types would look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class OrderID : NonBlankTrimmedString
{
    protected OrderID() { }
    public static explicit operator OrderID(string value) =&amp;gt; Wrap&amp;lt;OrderID&amp;gt;(value);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;(Sort-of-)immutability for &quot;free&quot; through type aliases&lt;/h3&gt;

&lt;p&gt;Another use case where this sort of approach seemed interesting was when I was writing some client-side code that received data in the form of arrays and then did some clever calculations and drew some pretty graphs. The API response data was 10s of 1000s of arrays, where each array was 100 floating point numbers. The calculation logic took those arrays and passed them through a bunch of methods to come up with the results but I got myself in a bit of a muddle when there were one or two places that had to manipulate a subset of the data and I realised that I was confusing myself as to whether the data should be altered in place or whether local copies of those parts of the data should be taken and then changed. To make the code easier to follow, I wanted those methods to take local copies to make the changes, rather than mutating them in-place and risking messing up calculations performed on the data later in the pipeline.&lt;/p&gt;

&lt;p&gt;What I really wanted was for those methods to have type signatures that would either take an immutable data type or a readonly data type. Immutable is the ideal because it means that not only can the receiving methods not change the data but &lt;em&gt;nothing&lt;/em&gt; can change the data. Having readonly types on the method signatures means that the methods can&#39;t change the data but it&#39;s still technically possible for the caller to change the data. To try to illustrate this, I&#39;ll use the &lt;strong&gt;ReadOnlyCollection&amp;lt;T&amp;gt;&lt;/strong&gt; type from .NET in an example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public static void Main()
{
    var items = new List&amp;lt;int&amp;gt; { 0, 1, 2, 3 };
    var readOnlyItems = items.AsReadOnly();
    DoSomething(
        readOnlyItems,
        halfwayPointCallback: () =&amp;gt; items.RemoveAt(0)
    );
}

static void DoSomething(ReadOnlyCollection&amp;lt;int&amp;gt; readOnlyItems, Action halfwayPointCallback)
{
    Console.WriteLine(&quot;Number of readonlyItems: &quot; + readOnlyItems.Count);
    halfwayPointCallback();
    Console.WriteLine(&quot;Number of readonlyItems: &quot; + readOnlyItems.Count);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here, the &quot;Main&quot; method declares a mutable list and then it create a readonly wrapper around it. The readonly wrapper is passed into the &quot;DoSomething&quot; method and this means &quot;DoSomething&quot; can &lt;em&gt;not&lt;/em&gt; directly alter that list. However, it&#39;s still possible for the &quot;Main&quot; method to change the underlying list while &quot;DoSomething&quot; is running.&lt;/p&gt;

&lt;p&gt;In practice, this is not something that I find commonly happens. As such, while I would &lt;em&gt;prefer&lt;/em&gt; immutable structures at all times (because then &quot;Main&quot; &lt;em&gt;couldn&#39;t&lt;/em&gt; change the contents of the list while &quot;DoSomething&quot; is working on it), being able to wrap the data in a readonly structure is still a significant improvement.&lt;/p&gt;

&lt;p&gt;So, some of the more obvious options available to me were:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Stick with using arrays and be careful not to write code that performs any alteration &quot;in place&quot; (&lt;strong&gt;I don&#39;t like this situation - C#&#39;s type system has great potential and I want it to help me and save me from myself where possible!&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;Pass the arrays into the methods as &lt;strong&gt;IEnumerable&amp;lt;float&amp;gt;&lt;/strong&gt; (&lt;strong&gt;this isn&#39;t a terrible idea in general - it quite clearly communicates that the provided data should be considered read only - but the calculations that I was doing wanted to get the length of the array and to read particular indexed values from the array in unpredictable orders and this isn&#39;t very efficient with enumerable types&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;Create an &quot;immutable list&quot; class that takes an array into the constructor, copies the data and then allows access to the copy only through tightly-controlled members; ie. Length and an indexed property (&lt;strong&gt;This is the most type-safe way but it felt expensive doing this for the 10s of 1000s of arrays that I had&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;Convert each array into a &lt;strong&gt;List&amp;lt;float&amp;gt;&lt;/strong&gt; and then call &quot;.AsReadOnly()&quot; on them (&lt;strong&gt;this is very little code but it also felt expensive with the amount of data that I had&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;Create a &quot;ReadOnlyArray&amp;lt;T&amp;gt;&quot; type that would be very similar in nature to the &lt;strong&gt;ReadOnlyCollection&amp;lt;T&amp;gt;&lt;/strong&gt; in that it would take an array into its constructor and then provide a read only interface for it, &lt;em&gt;without&lt;/em&gt; copying the array (&lt;strong&gt;This is a reasonable option and I might have gone this way were it not for liking the idea of option six&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;Create a &quot;ReadOnlyArray&amp;lt;T&amp;gt;&quot; type &lt;em&gt;alias&lt;/em&gt; that I could use to instruct the type system that the data should not be altered but without having to introduce &lt;em&gt;any&lt;/em&gt; new types or instances at runtime&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I went with the last one because I was all excited about experimenting with &quot;Bridge.NET type aliases&quot; and I wanted to see how well they could work! (In reality, the fifth option was also a good one and some of the others would also be perfectly fine for smaller data sets.. to be honest, there is a chance that they wouldn&#39;t have made &lt;em&gt;too&lt;/em&gt; much difference even with the data that I was looking at but, again, sometimes you need to make opportunity to experiment! :)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public sealed class ReadOnlyArray&amp;lt;T&amp;gt; : IEnumerable&amp;lt;T&amp;gt;
{
    [Template(&quot;{data}&quot;)]
    public extern ReadOnlyArray(T[] data);

    [External] // Required due to https://github.com/bridgedotnet/Bridge/issues/4015
    public extern T this[int index] { [Template(&quot;{this}[{index}]&quot;)] get; }

    public extern int Length { [Template(&quot;length&quot;)] get; }

    [External]
    public extern IEnumerator&amp;lt;T&amp;gt; GetEnumerator();

    [External]
    extern IEnumerator IEnumerable.GetEnumerator();

    [Template(&quot;{value}&quot;)]
    public extern static implicit operator ReadOnlyArray&amp;lt;T&amp;gt;(T[] value);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The structure of this class is similar in some ways to that of the &lt;strong&gt;NonBlankTrimmedString&lt;/strong&gt;. Unlike that class, there is no validation that is required - I &lt;em&gt;only&lt;/em&gt; want to provide access to an array in a limited manner and so it&#39;s fine to expose a public constructor (as opposed to the &lt;strong&gt;NonBlankTrimmedString&lt;/strong&gt;, where it&#39;s important to check that the value is neither null nor blank nor whitespace-only and the [Template] attribute on the constructor doesn&#39;t easily allow for any validation).&lt;/p&gt;

&lt;p&gt;Even though the constructor may be used on this class, there is still an operator to change an array into a &lt;strong&gt;ReadOnlyArray&lt;/strong&gt; so that the deserialisation process is able to read an array of items into a &lt;strong&gt;ReadOnlyArray&lt;/strong&gt; instance. I&#39;ve chosen to use an implicit operator (rather than en explicit operator) here because there is no validation to perform - the &lt;strong&gt;NonBlankTrimmedString&lt;/strong&gt; has an explicit operator because that&lt;em&gt;does&lt;/em&gt; perform some validation and so it&#39;s a casting action that could fail and so I want it to be explicit in code.&lt;/p&gt;

&lt;p&gt;As with the &lt;strong&gt;NonBlankTrimmedString&lt;/strong&gt;, this type will exist only at compile time and the compiled JavaScript will always be operating directly against the original array. As far as the JS code is aware, there &lt;em&gt;is no&lt;/em&gt; wrapper class involved at all. The following C# -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var values = new[] { 1, 2, 3 };
Console.WriteLine(values.Length);

var readOnlyValuesCtor = new ReadOnlyArray&amp;lt;int&amp;gt;(values);
Console.WriteLine(readOnlyValuesCtor.Length);

ReadOnlyArray&amp;lt;int&amp;gt; readOnlyValuesCast = values;
Console.WriteLine(readOnlyValuesCast.Length);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. is translated into this JS:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var values = System.Array.init([1, 2, 3], System.Int32);
System.Console.WriteLine(values.length);

var readOnlyValuesCtor = values;
System.Console.WriteLine(readOnlyValuesCtor.length);

var readOnlyValuesCast = values;
System.Console.WriteLine(readOnlyValuesCast.length);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Whether the &lt;strong&gt;ReadOnlyArray&amp;lt;int&amp;gt;&lt;/strong&gt; is created by calling its constructor or by an implicit cast in the C# code, the JS is unaware of any change required of the reference and continues to operate on the original array. This is the &quot;free&quot; part of this approach - there is no runtime cost in terms of type conversions or additional references.&lt;/p&gt;

&lt;p&gt;The other members of the class need a little more explanation, though. The indexer &lt;em&gt;should&lt;/em&gt; be implemented just like the &quot;Length&quot; property, by having an extern property that has a getter with a [Template] attribute on it. However, there is a bug in the Bridge compiler that necessitate an additional [External] attribute be added to the property. Not the end of the world and I&#39;m sure that the Bridge Team will fix it in a future version of the compiler.&lt;/p&gt;

&lt;p&gt;The &quot;GetEnumerator&quot; methods require a tiny bit more explanation. In order for the class to implement &lt;strong&gt;IEnumerable&amp;lt;T&amp;gt;&lt;/strong&gt;, these methods must be present. But we don&#39;t actually have to implement them ourselves. Whenever Bridge encouters a &quot;foreach&quot; in the source C# code, it translates it into JS that calls &quot;GetEnumerator&quot; and then steps through each value. For example, this C# code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;foreach (var value in readOnlyValuesCtor)
    Console.WriteLine(value);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. becomes this JS:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$t = Bridge.getEnumerator(readOnlyValuesCtor);
try {
    while ($t.moveNext()) {
        var value = $t.Current;
        System.Console.WriteLine(value);
    }
} finally {
    if (Bridge.is($t, System.IDisposable)) {
        $t.System$IDisposable$Dispose();
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Because Bridge needs to support enumerating over arrays, the function &quot;Bridge.getEnumerator&quot; knows what to do if it is given an array reference. And since a &lt;strong&gt;ReadOnlyArray&lt;/strong&gt; &lt;em&gt;is&lt;/em&gt; an array reference at runtime, we don&#39;t have to do anything special - we don&#39;t have to provide a GetEnumerator implementation.&lt;/p&gt;

&lt;p&gt;And there we go! As I explained above, I originally encountered this problem when passing an array into a complicated calculation process but this type could &lt;em&gt;also&lt;/em&gt; be used for deserialising JSON into a richer type model, just like the &lt;strong&gt;NonBlankTrimmedString&lt;/strong&gt; earlier - again, without any overhead in doing so (no instances of wrapper types will be present runtime and there will be no additional references for the garbage collector to track).&lt;/p&gt;

&lt;h3&gt;&lt;em&gt;Only&lt;/em&gt; possible in Bridge.NET?&lt;/h3&gt;

&lt;p&gt;I was wracking my brains about whether it would be possible to do something similar with C# running in a .NET environment and I couldn&#39;t think of anything. People sometimes think &lt;em&gt;&quot;structs!&quot;&lt;/em&gt; when trying to concoct ways to avoid adding references that the garbage collector needs to track but structs are only immune to this if they don&#39;t contain any object references within their fields and properties (and there are other edge cases besides this but they&#39;re not important right now).&lt;/p&gt;

&lt;p&gt;At the end of the day, this &quot;type alias&quot; concept might be a bit of a niche technique and it &lt;em&gt;might&lt;/em&gt; even be a case of me playing around, more than it being something that you might use in production.. but I thought that it was interesting nonetheless. And it has made me wish, again, that C# had support for something like this - I&#39;ve written code before that defines all variety of strongly typed IDs (strings) and Keys (integers) to avoid passing the wrong type of value into the wrong place but it&#39;s always felt cumbersome (it&#39;s felt worth the effort but that didn&#39;t stop wishing me that it was &lt;em&gt;less&lt;/em&gt; effort).&lt;/p&gt;

&lt;h3&gt;Type aliases in other languages&lt;/h3&gt;

&lt;p&gt;I&#39;ve linked above to an article &lt;a href=&quot;https://andrewlock.net/using-strongly-typed-entity-ids-to-avoid-primitive-obsession-part-1/&quot;&gt;Using strongly-typed entity IDs to avoid primitive obsession&lt;/a&gt;, which is excellent and eloquently expresses some of my thoughts, but I thought that I&#39;d add a summary in here as well (which also gives me an opportunity to go into more detail about the options in TypeScript and F#).&lt;/p&gt;

&lt;p&gt;I&#39;ll start with an anecdote to set the scene. In a company that I used to work at, we had systems that would retrieve and render different data for different language options. Sometimes data would vary only by language (&quot;English&quot;, &quot;French&quot;, etc..) but sometimes it would be more specific and vary by language &lt;em&gt;culture&lt;/em&gt; (eg. &quot;English - United Kingdom&quot;, &quot;English - United States&quot;, etc..). An older version of the system would pass around int values for the language or language culture keys. So there might be a method such as:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;private string GetTranslatedName(int languageKey)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A problem that occurred over and over again is that language keys and language culture keys would get mixed up in the code base - in other words, it was quite common for someone to accidentally pass a language key into a method where a language &lt;em&gt;culture&lt;/em&gt; key was expected (this situation was not helped by the fact that much of the developer testing was done in English and the language key and language culture key values in many of the databases were both 1 for English / English UK). Something that I was very keen to get into a new version of the system was to introduce &quot;strongly typed keys&quot; so that this sort of accident could no longer occur. The method&#39;s signature would be changed to something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;private string GetTranslatedName(LanguageKey languageKey)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. and we would not describe language or language culture keys as ints in the code base. They would always be either an instance of &lt;strong&gt;LanguageKey&lt;/strong&gt; or &lt;strong&gt;LanguageCultureKey&lt;/strong&gt; - this way, if you attempted to pass a key of the wrong type into a method then you would get a compile error.&lt;/p&gt;

&lt;p&gt;The downside is that each key type had to be defined as its own struct, with the following (quite verbose) structure:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public struct LanguageKey : IEquatable&amp;lt;LanguageKey&amp;gt;
{
    public LanguageKey(int value) =&amp;gt; Value = value;

    public int Value { get; }

    public bool Equals(LanguageKey other) =&amp;gt; Value.Equals(other.Value);
    public override bool Equals(object obj) =&amp;gt; (obj is LanguageKey key) &amp;amp;&amp;amp; (key.Value == Value);
    public override int GetHashCode() =&amp;gt; Value;

    public static bool operator ==(LanguageKey x, LanguageKey y) =&amp;gt; x.Value == y.Value;
    public static bool operator !=(LanguageKey x, LanguageKey y) =&amp;gt; !(x == y);

    public static explicit operator LanguageKey(int value) =&amp;gt; new LanguageKey(value);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Really, though, that is the &lt;em&gt;only&lt;/em&gt; downside. As the strongly typed keys are structs without any reference properties or fields, there is no additional work for the garbage collector and there is no memory overhead vs tracking a simple int. But it &lt;em&gt;does&lt;/em&gt; still feel a little arduous to have to have these definitions in the code base, particularly when the equivalent F# code looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[&amp;lt;Struct&amp;gt;] type LanguageKey = LanguageKey of int
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It&#39;s worth noting that this is not actually referred to as a &quot;type alias&quot; in F#; this is a &quot;single case union type&quot;. There &lt;em&gt;is&lt;/em&gt; a concept called a &quot;type alias&quot; in F# that looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type LanguageKey = int
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. but that code simply says &quot;allow me to use the word &#39;LanguageKey&#39; anywhere in place of int&quot; - eg. if I have the LanguageKey type alias specified as a method argument type in F# method, like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;let getTranslatedName (language: LanguageKey) =
    // (Real work to retrieve translated name would go here but we&#39;ll
    //  just return the string &quot;Whatever&quot; for the sake of this example)
    &quot;Whatever&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. then the compiler would allow me to pass an int into that method -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// A LanguageKey type alias lets me pass any old int into the method - rubbish!
let name = getTranslatedName 123
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. and that&#39;s exactly what I wanted to avoid!&lt;/p&gt;

&lt;p&gt;On the other hand, if the type &lt;strong&gt;LanguageKey&lt;/strong&gt; was a &quot;single case union type&quot; then the code above would not compile:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// error FS0001: This expression was expected to have type &#39;LanguageKey&#39; but here has type &#39;int&#39;
let name = getTranslatedName 123

// This DOES compile because the types match
let key = LanguageKey 123
let name = getTranslatedName 123
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. and that&#39;s exactly what I &lt;em&gt;did&lt;/em&gt; want!&lt;/p&gt;

&lt;p&gt;(TypeScript&#39;s type aliases are like F#&#39;s type aliases - they are more of a convenience and do not add the sort of type checking that I want)&lt;/p&gt;

&lt;p&gt;Things get a bit more awkward if we want to deal with reference types, such as strings, becase we could create a C# class similar to &lt;strong&gt;LanguageKey&lt;/strong&gt; (or we could create an F# single case union type) but that would introduce a new instance of a type that must be tracked by the garbage collector - every strongly typed ID involves &lt;em&gt;two&lt;/em&gt; references; the underlying string value and the strongly typed wrapper. Much of the time, that&#39;s no problem - I&#39;ve had the odd issue with the .NET GC in the past but, on the whole, it&#39;s an amazing and reliable tool.. but because I &lt;em&gt;have&lt;/em&gt; had these problems before, it makes me more aware of the trade-off when I introduce wrappers like this.&lt;/p&gt;

&lt;p&gt;I&#39;m convinced that using strongly typed IDs is the right thing to do in 99% of cases because it improves code quality and can eradicate a class of real-world mistake. But the concept became even more interesting to me as it appeared possible to introduce a form of type alias into Bridge.NET code that enables those compile time checks but with zero runtime cost. Granted, the type erasure that occurs means that &lt;em&gt;runtime&lt;/em&gt; type checking is not possible (the Bridge code can not differentiate between a &lt;strong&gt;string&lt;/strong&gt; or a &lt;strong&gt;NonBlankTrimmedString&lt;/strong&gt; or a type that is &lt;em&gt;derived&lt;/em&gt; from &lt;strong&gt;NonBlankTrimmedString&lt;/strong&gt;) but the main driver for me was to improve compile time checking and so that wasn&#39;t a problem for me. Maybe it would be a problem in other scenarios, in which case these Bridge.NET &quot;type aliases&quot; might not be appropriate.&lt;/p&gt;</description>
			<pubDate>Tue, 02 Jul 2019 21:59:00 GMT</pubDate>
		</item>
		<item>
			<title>I didn&#39;t understand why people struggled with (.NET&#39;s) async</title>
            <link>http://www.productiverage.com/i-didnt-understand-why-people-struggled-with-nets-async</link>
			<guid>http://www.productiverage.com/i-didnt-understand-why-people-struggled-with-nets-async</guid>
			<description>&lt;p&gt;Long story short (I know that some readers love a TL;DR), I have almost always worked with async/await in C# projects where it&#39;s been async from top-to-bottom and so I&#39;ve rarely tried to start integrating async code into a large project that is primarily &lt;em&gt;non&lt;/em&gt;-async. Due to this, I have never encountered any async problems.. so I&#39;ve occasionally wondered &quot;why would people be worried about getting deadlocks?&quot;&lt;/p&gt;

&lt;p&gt;Recently, this changed and I wanted to start integrating components that use async methods into a big project that doesn&#39;t. And it didn&#39;t take long until I got a hanging application!&lt;/p&gt;

&lt;p&gt;Before I get to my story (and my solution), let me quickly recap what all the fuss is about.&lt;/p&gt;

&lt;h3&gt;What is &quot;async&quot; and why is it good?&lt;/h3&gt;

&lt;p&gt;To put things into context, I&#39;m talking about a web application - code that hosts a website and spend 99% of its day &quot;read only&quot; and just rendering pages for people.&lt;/p&gt;

&lt;p&gt;These do not tend to be computationally-intensive applications and if a page is slow to render then it&#39;s probably because the code is waiting for something.. like a database call to complete or an external cache request or the loading of a file.&lt;/p&gt;

&lt;p&gt;Within IIS, when an ASP.NET application is hosted and is responding to requests, the simple model for synchronous code is that each request is allocated a thread to work on and it will keep hold of that thread for the duration of the request. A thread is an operating system construct and it occupies some resources - in other words, they&#39;re not free and, given the the choice, we would like to need less of them rather than more of them. The threads in .NET are an abstraction over OS threads but to avoid getting too far off course, we&#39;ll think of them as being equivalent because they&#39;re close enough for the purposes of this post.&lt;/p&gt;

&lt;p&gt;This model is very easy to understand but it&#39;s also easy to see how it could be quite wasteful. If the majority of our web server requests spend much of their time waiting for something external (database, other network I/O, local file system, etc..) then do we really need to tie up a thread for that time? Couldn&#39;t we free it up for another request (that &lt;em&gt;isn&#39;t&lt;/em&gt; waiting for something external) to use and then try to get it back when whatever we&#39;re waiting on has finished doing its thing?&lt;/p&gt;

&lt;p&gt;This is essentially what async / await is trying to solve. It introduces a simple way for us to write code that can say &quot;I&#39;m doing something that will perform an asynchronous action now - I&#39;m expecting to wait for a little bit and so you (the .NET hosting environment) can have my thread back for a little while&quot;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(Before async / await, it was possible to do this but it was much more convoluted and you had to deal with code that might follow a tangled web of callbacks and you would have to manually pass around any references that you would want to access in those callbacks - it was possible to do but made for code that was harder to read and write and, thusly, was more likely to contain mistakes)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;When the .NET environment deals with &quot;await&quot;, the thread that the await call happened on will be free&#39;d back up. Then, when the async work is completed, a thread will be given back to that request so that it can carry on doing its thing. You might be wondering &quot;how does .NET &lt;em&gt;know&lt;/em&gt; when the work has completed? Surely that requires another thread to monitor whether the external resource has responded and, if so, aren&#39;t we right back where we started because we&#39;re blocking threads?&quot; This is what I thought when I was first learning about async / await (and so I happen to think that it&#39;s an entirely reasonable question!) but it&#39;s not the case. The operating system and its drivers expose ways to say (and I&#39;m grossly simplifying again because it&#39;s only the gist that we need here, not the full nitty gritty) &quot;start sending this network data and notify me when data starts coming back in response&quot; (and similar mechanisms for other types of I/O). When that notification occurs, the .NET environment can provide a thread to the request that was awaiting and let it carry on.&lt;/p&gt;

&lt;p&gt;To try to illustrate this, imagine that the below image represents a web request. The blue parts are when computational work is being done on the thread and the white parts are when it&#39;s waiting on an external resource (like a database) -&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;Illustration of a web request with delays for external data shown&quot; src=&quot;http://www.productiverage.com/Content/Images/Posts/Threads-SingleExample.png&quot; class=&quot;NoBorder FullWidth&quot;&gt;&lt;/p&gt;

&lt;p&gt;At a glance, it&#39;s clear that there will be a lot of wasted time that a thread spends doing nothing (in a blocked state) if we&#39;re using the old model of &quot;one thread for the entirety of the request&quot;. What might be slightly less easy to envisage, though, is &lt;em&gt;just how many&lt;/em&gt; unnecessary threads that we might be occupying at any given time if all requests are like this.&lt;/p&gt;

&lt;p&gt;To try to illustrate that, I&#39;ve stacked eight identical web requests representations on top of each other, staggered slightly in time -&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;Illustration of a staggered concurrent web requests&quot; src=&quot;http://www.productiverage.com/Content/Images/Posts/Threads-Stacked.png&quot; class=&quot;NoBorder FullWidth&quot;&gt;&lt;/p&gt;

&lt;p&gt;Again, the blue represents time when each request is actively doing work and the white represents time when it&#39;s waiting for something (grey before a request is time before that request arrived at the server and grey after a request is time after it completed).&lt;/p&gt;

&lt;p&gt;With the classic &quot;one thread for the entirety of the request&quot;, we would be using up to eight threads for much of this time; initially only one thread would be active and then the second request would arrive and a second thread would get tied up and then a third thread would be used when the third request arrived and the threads wouldn&#39;t start getting free&#39;d until the first request completed.&lt;/p&gt;

&lt;p&gt;On the other hand, if we could free up a request&#39;s thread every time that it was waiting for an external resource then we would &lt;em&gt;never&lt;/em&gt; require eight threads at any one time for these eight requests because there is no point in time when all eight of the requests are actively doing work at the exact same time.&lt;/p&gt;

&lt;p&gt;Time for a graph!&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;Threads required for async vs non-async concurrent web requests&quot; src=&quot;http://www.productiverage.com/Content/Images/Posts/Threads-Graph.png&quot; class=&quot;NoBorder FullWidth&quot;&gt;&lt;/p&gt;

&lt;p&gt;The blue line shows the number of active requests. If we have one-thread-per-request then that blue line also shows how many threads would be required to handle those requests.&lt;/p&gt;

&lt;p&gt;The green line shows how many requests are actually doing work at any one time. If we are able to use the async / await model and only have web requests occupy threads while they&#39;re actively doing work then this is how many threads would be required. It&#39;s always less than the number of active requests and it&#39;s less than &lt;em&gt;half&lt;/em&gt; for nearly all of the time in this example.&lt;/p&gt;

&lt;p&gt;The async / await model means that we need to use less threads and that&#39;s less resources and that&#39;s a good thing!&lt;/p&gt;

&lt;h3&gt;A lightning overview of thread distribution&lt;/h3&gt;

&lt;p&gt;There was a lot of talk above of how &quot;each request is allocated a thread to work&quot; and &quot;a thread will be given back to that request&quot; and it&#39;s worth quickly reviewing how threads are created.&lt;/p&gt;

&lt;p&gt;A thread in C# &lt;em&gt;can&lt;/em&gt; be created using:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var thread = new Thread(nameOfMethodThatHasWorkToDoOnTheNewThread);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However, threads are a relatively expensive resource to new up and then discard over and over again and so .NET offers a way to &quot;pool&quot; threads. What this boils down to is that the &lt;strong&gt;ThreadPool&lt;/strong&gt; framework class will maintain a list of threads and reuse them when someone needs one. This is used internally in many places within the .NET framework and it may be used like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ThreadPool.QueueUserWorkItem(nameOfMethodThatHasWorkToDoOnTheNewThread);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;strong&gt;ThreadPool&lt;/strong&gt; will keep track of how many threads would need to exist at any given time to service all &quot;QueueUserWorkItem&quot; requests and it will track this over time so it can try to keep its pool at the optimium size for the application - too many means a waste of resources but too few means that it will take longer before the work requested via &quot;QueueUserWorkItem&quot; calls can be executed (if there is no thread free when a &quot;QueueUserWorkItem&quot; is made then that work will still happen but it will be queued up until the &lt;strong&gt;ThreadPool&lt;/strong&gt; has a thread become free).&lt;/p&gt;

&lt;p&gt;It would make for a fairly simple mental model if async / await always used the &lt;strong&gt;ThreadPool&lt;/strong&gt; - if, when a request made an &quot;await&quot; call then it gave its current thread back to the &lt;strong&gt;ThreadPool&lt;/strong&gt; and then, when the async work was completed, the request could continue on a thread provided by the &lt;strong&gt;ThreadPool&lt;/strong&gt;. This would be straight forward and easy to understand and sometimes it &lt;em&gt;is&lt;/em&gt; the case - Console Applications and Windows Services will work like this with async / await, for example. We can picture it a bit like this:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;A Windows Service receives a request and is given Thread &quot;A&quot; from the &lt;strong&gt;ThreadPool&lt;/strong&gt; to start working on&lt;/li&gt;
&lt;li&gt;At some point, the request needs to perform an asynchronous action and so there is an &quot;await&quot; in the code - when this happens, Thread &quot;A&quot; is released back to the &lt;strong&gt;ThreadPool&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;When that async task has completed, the request can carry on - however, Thread &quot;A&quot; was given to a &lt;em&gt;different&lt;/em&gt; request while this request was waiting for the async work and so the &lt;strong&gt;ThreadPool&lt;/strong&gt; gives it Thread &quot;B&quot;&lt;/li&gt;
&lt;li&gt;The request does some more synchronous work on Thread &quot;B&quot; and finishes, so Thread &quot;B&quot; is released back to the &lt;strong&gt;ThreadPool&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Easy peasy.&lt;/p&gt;

&lt;h3&gt;Thread distribution troublemakers&lt;/h3&gt;

&lt;p&gt;However.. some project types get a bit possessive about their threads - when a request starts on one thread then it wants to be able to continue to use that thread forever. I suspect that this is most commonly known about WinForms projects where it was common to see code that looked like the following (that I have borrowed from a &lt;a href=&quot;https://stackoverflow.com/a/661686/3813189&quot;&gt;Stack Overflow answer&lt;/a&gt;):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;private void OnWorkerProgressChanged(object sender, ProgressChangedArgs e)
{
    // Cross thread - so you don&#39;t get the cross-threading exception
    if (this.InvokeRequired)
    {
        this.BeginInvoke((MethodInvoker)delegate
        {
            OnWorkerProgressChanged(sender, e);
        });
        return;
    }

    // Change control
    this.label1.Text = e.Progress;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With WinForms, you must never block the main thread because then your whole application window will go into a &quot;not responding&quot; state. So, if you wanted to start a process that is unlikely to complete instantly - such as a file upload - then you might have a component that performs the work on a different thread and that has events for &quot;progress changed&quot; (so that it can report {x}% complete) and &quot;upload completed&quot;. When these events are raised, we&#39;ll want to update the UI of the application but there is a problem: when these callbacks are executed, they will be run on the thread that the file upload is running on and not the main UI thread. The reason that this is a problem is that UI components may &lt;em&gt;only&lt;/em&gt; be updated by code that is running on the UI thread. The way around this is to check the &quot;InvokeRequired&quot; property on a UI component before trying to update any of the component&#39;s properties. If &quot;InvokeRequired&quot; returns false then it meant that the current thread is the UI thread and that no funny business was required. However, if it returns true then it means that the current thread is &lt;em&gt;not&lt;/em&gt; the UI thread and that a special method &quot;BeginInvoke&quot; would have to be called, which was a way to say &quot;please execute this code on the UI thread&quot;.&lt;/p&gt;

&lt;p&gt;Eventually, people got used to this and would ensure that they used &quot;InvokeRequired&quot; and  &quot;BeginInvoke&quot; when updating UI elements if they were dealing with code that might do some &quot;background processing&quot;.&lt;/p&gt;

&lt;p&gt;When async / await were introduced, though, one of the aims was to make it easy and neat and tidy to write async code - basically, to be able to write code that &lt;em&gt;looked&lt;/em&gt; synchronous while still getting the benefits of being &lt;em&gt;asynchronous&lt;/em&gt;. That meant trying to avoid code that looked like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;private async void btnUpload_Click(object sender, EventArgs e)
{
    var filename = await UploadContent();

    // Why do I need to do this?! I haven&#39;t (explicitly) fired
    // up any new threads or anything! :S
    if (this.InvokeRequired)
    {
        this.BeginInvoke((MethodInvoker)delegate
        {
            this.lblFilename.Text = filename;
        });
        return;
    }
    this.lblFilename.Text = filename;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Instead, it should just look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;private async void btnUpload_Click(object sender, EventArgs e)
{
    var filename = await UploadContent();
    this.lblFilename.Text = filename;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The problem with this is that some magic will be required somewhere. If the &lt;strong&gt;ThreadPool&lt;/strong&gt; is responsible for providing a thread to execute on after async work has completed, things are going to go wrong if it provides one thread to start the request on and a different thread to continue on after the async work has completed. It was fine for the Windows Service example request above to start on Thread &quot;A&quot; and then change to working on Thread &quot;B&quot; because Windows Services don&#39;t have limitations on what threads can and can&#39;t do, whereas WinForms UI components &lt;em&gt;do&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The &quot;magic&quot; involved is that .NET provides a way for threads to be assigned a special &quot;Synchronization Context&quot;. This is a mechanism that changes how async / await interacts with the &lt;strong&gt;ThreadPool&lt;/strong&gt; and makes it possible for WinForms applications to say &quot;When I await an asynchronous task and that task completes, I want to carry on my work on the same thread&quot;. This is why there is no need to check InvokeRequired / BeginInvoke when writing async event handlers for WinForms UI components.&lt;/p&gt;

&lt;p&gt;One downside to this is that it puts constraints on how the &lt;strong&gt;ThreadPool&lt;/strong&gt; can and can&#39;t distribute threads and means that it&#39;s not as free to optimise usage solely for efficiency and throughput. It also means that either the request&#39;s thread must remain allocated to the request until the request completes (negating one of the benefits of await / async) &lt;em&gt;or&lt;/em&gt; the request may have to wait after an async call completes before the thread that it wants to continue on becomes free*.&lt;/p&gt;

&lt;p&gt;* &lt;em&gt;(I&#39;m not actually sure which of these two options happens in real world use but it feels like the sort of thing that is an implementation detail of the framework and it would be best to not presume that it would be one or the other)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;There is another downside, though, which is that it&#39;s quite easy to get into bother if you try to call async code from a &lt;em&gt;non&lt;/em&gt;-async method - as I&#39;m about to show you!&lt;/p&gt;

&lt;h3&gt;The classic deadlock (aka. &quot;why has my application hung?&quot;)&lt;/h3&gt;

&lt;p&gt;This problem has been encountered so many times that a lot of async&#39;ers recognise it straight away and there are plenty of questions on Stack Overflow about it. There is also good advice that  is often repeated about how to prevent it. However, I think that it&#39;s particularly nasty because the code might not look hideously wrong at a glance but it will be able to cause your application to hang when it&#39;s run - not throw an exception (which at least makes it clear where something has gone wrong), but to just &lt;em&gt;hang&lt;/em&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class HomeController
{
    public ActionResult Index()
    {
        return View(
            GetTitleAsync().Result
        );
    }

    private async Task&amp;lt;string&amp;gt; GetTitleAsync()
    {
        // This Task.Delay call simulates an async call that might go off to the
        // database or other external service
        await Task.Delay(1000);
        return &quot;Hello!&quot;;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When I encountered this problem, I was much deeper down the stack but the concept was the same -&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;I was in code that was being called by an MVC action method and that method was not async&lt;/li&gt;
&lt;li&gt;I needed to call an async method&lt;/li&gt;
&lt;li&gt;I tried to access &quot;.Result&quot; on the task that I got back from the async method - this will block the current thread until the task completes&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The key factor&lt;/strong&gt;: ASP.NET applications also have a special synchronization context, similar to the WinForms one in that it returns to the same thread after an async call completes&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you ran the code above then something like the following chain of events would occur:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Thread &quot;A&quot; would be given to the request to run on and &quot;Index&quot; would be called&lt;/li&gt;
&lt;li&gt;Thread &quot;A&quot; would call &quot;GetTitleAsync&quot; and get a &lt;strong&gt;Task&amp;lt;string&amp;gt;&lt;/strong&gt; reference&lt;/li&gt;
&lt;li&gt;Thread &quot;A&quot; would then request the &quot;.Result&quot; property of that task and would block until the task completed&lt;/li&gt;
&lt;li&gt;The &quot;Task.Delay&quot; call would complete and .NET would try to continue the &quot;GetTitleAsync&quot; work&lt;/li&gt;
&lt;li&gt;The ASP.NET synchronization context would require that work continue on Thread &quot;A&quot; and so the work would be placed on a queue for Thread &quot;A&quot; to deal with when it gets a chance (the &quot;work&quot; in this case is simply the line that returns the string &quot;Hello!&quot; but that code has to be executed somewhere)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;And this is how we become stuck!&lt;/p&gt;

&lt;p&gt;Thread &quot;A&quot; is waiting for the &quot;GetTitleAsync&quot; work to complete but the &quot;GetTitleAsync&quot; work can not complete until Thread &quot;A&quot; gets involved (which it can&#39;t because it&#39;s in a blocked state).&lt;/p&gt;

&lt;p&gt;This is the problem and it seem oh-so-obvious if you know how async / await work and about the ASP.NET synchronization context and if you&#39;re paying close attention when you&#39;re writing this sort of code. But if you &lt;em&gt;don&#39;t&lt;/em&gt; then you get a horrible runtime problem.&lt;/p&gt;

&lt;p&gt;So let&#39;s look at solutions.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Approach one: Don&#39;t mix async and non-async code&lt;/strong&gt;. This is good advice when starting new codes - begin with async and then it&#39;s async all the way down, no blocking of threads while accessing &quot;.Result&quot; and so no problem! However, with a big project it&#39;s not very helpful.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Approach two: Always use &quot;.ConfigureAwait(false)&quot;&lt;/strong&gt;. This is the oft-repeated good advice that I mentioned earlier. As a rule of thumb, many people recommend &lt;em&gt;always&lt;/em&gt; including &quot;.ConfigureAwait(false)&quot; when you use await, like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;await Task.Delay(1000).ConfigureAwait(false);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &quot;false&quot; value is for the &quot;continueOnCapturedContext&quot; parameter and this parameter effectively overrides the synchronization context about what thread the work must continue on when the async work has completed.&lt;/p&gt;

&lt;p&gt;If we changed our code to this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class HomeController
{
    public ActionResult Index()
    {
        return View(
            GetTitleAsync().Result
        );
    }

    private async Task&amp;lt;string&amp;gt; GetTitleAsync()
    {
        // This Task.Delay call simulates an async call that might go off to the
        // database or other external service
        await Task.Delay(1000).ConfigureAwait(false);
        return &quot;Hello!&quot;;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. then the chain of events goes more like this:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Thread &quot;A&quot; would be given to the request to run on and &quot;Index&quot; would be called&lt;/li&gt;
&lt;li&gt;Thread &quot;A&quot; would call &quot;GetTitleAsync&quot; and get a &lt;strong&gt;Task&amp;lt;string&amp;gt;&lt;/strong&gt; reference&lt;/li&gt;
&lt;li&gt;Thread &quot;A&quot; would then request the &quot;.Result&quot; property of that task and would block until the task completed&lt;/li&gt;
&lt;li&gt;The &quot;Task.Delay&quot; call would complete and .NET would try to continue the &quot;GetTitleAsync&quot; work&lt;/li&gt;
&lt;li&gt;Because we used &quot;.ConfigureAwait(false)&quot;, we are not restricted in terms of where can continue the &quot;GetTitleAsync&quot; work and so that will be done on Thread &quot;B&quot;&lt;/li&gt;
&lt;li&gt;The work for Thread &quot;B&quot; is simply to complete the &lt;strong&gt;Task&amp;lt;string&amp;gt;&lt;/strong&gt; by setting its result to &quot;Hello!&quot; (Thread &quot;B&quot; does this and then it is released back to the &lt;strong&gt;ThreadPool&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;Since the &lt;strong&gt;Task&amp;lt;string&amp;gt;&lt;/strong&gt; has completed, Thread &quot;A&quot; is no longer blocking on the &quot;.Result&quot; access and it can carry on with its work and return the &lt;strong&gt;ActionResult&lt;/strong&gt; from the method&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The good news here is that this solves the problem - there is no longer a deadlock that can occur!&lt;/p&gt;

&lt;p&gt;The bad news is that you must remember to add &quot;.ConfigureAwait(false)&quot; to your await calls. If you forget then there is a chance that your code will result in an application hang and you won&#39;t find out until runtime. I don&#39;t like this because one of the reasons that I enjoy C# as a strongly-typed language is that the compiler can catch so many mistakes and I &lt;em&gt;don&#39;t&lt;/em&gt; have to wait until runtime to find problems much of the time. One way to make life easier on this front is to help the compiler help you by installing an analyser, such as the &lt;a href=&quot;https://www.nuget.org/packages/ConfigureAwaitChecker.Analyzer/&quot;&gt;ConfigureAwaitChecker.Analyzer&lt;/a&gt;. Installing this should result in you getting warnings in Visual Studio if you don&#39;t include &quot;.ConfigureAwait(false)&quot; after any await.&lt;/p&gt;

&lt;p&gt;Another possible (and subjective) downside to this approach is that it makes the code &quot;noisier&quot; - if &quot;.ConfigureAwait(false)&quot; should be used almost every time you use &quot;await&quot; then shouldn&#39;t it be the default behaviour and it be the case that you should have to include extra code if you &lt;em&gt;don&#39;t&lt;/em&gt; want that behaviour? You may not agree with me but it feels like an extra burden that I&#39;d rather live without.&lt;/p&gt;

&lt;p&gt;Instead, we could consider..&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Approach three: Disabling the synchronization context before calling async code&lt;/strong&gt;. The .NET environment allows the host to specify its own synchronization context but it also allows &lt;em&gt;any&lt;/em&gt; code to specify a particular context. We could use this to our advantage by doing something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class HomeController
{
    public ActionResult Index()
    {
        // Get a reference to whatever the current context is, so that we can set
        // it back after the async work is done
        var currentSyncContext = SynchronizationContext.Current;
        string result;
        try
        {
            // Set the context to null so that any restrictions are removed that
            // relate to what threads async code can continue on
            SynchronizationContext.SetSynchronizationContext(null);

            // Block this thread until the async work is complete
            result = GetTitleAsync().Result;
        }
        finally
        {
            // Set the context back to whatever it was before
            SynchronizationContext.SetSynchronizationContext(currentSyncContext);
        }

        return View(result);
    }

    private async Task&amp;lt;string&amp;gt; GetTitleAsync()
    {
        // This Task.Delay call simulates an async call that might go off to the
        // database or other external service
        await Task.Delay(1000);
        return &quot;Hello!&quot;;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This means that we don&#39;t have to use &quot;.ConfigureAwait(false)&quot; and we &lt;em&gt;still&lt;/em&gt; don&#39;t get any deadlocks. We can include code like this at the boundary where non-async code calls async code and then we won&#39;t have to worry about whether the async code includes any await calls that do not specify &quot;.ConfigureAwait(false)&quot;.&lt;/p&gt;

&lt;p&gt;You wouldn&#39;t want to include this extra code &lt;em&gt;every&lt;/em&gt; time that you called async code from non-async code and so it would make sense to encapsulate the logic in a method. Something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class HomeController : Controller
{
    public ActionResult Index()
    {
        return View(
            AsyncCallHelpers.WaitForAsyncResult(GetTitleAsync())
        );
    }

    private async static Task&amp;lt;string&amp;gt; GetTitleAsync()
    {
        // This Task.Delay call simulates an async call that might go off to the
        // database or other external service
        await Task.Delay(1000);
        return &quot;Hello!&quot;;
    }
}

public static class AsyncCallHelpers
{
    /// &amp;lt;summary&amp;gt;
    /// Avoid the &#39;classic deadlock problem&#39; when blocking on async work from non-async
    /// code by disabling any synchronization context while the async work takes place
    /// &amp;lt;/summary&amp;gt;
    public static T WaitForAsyncResult&amp;lt;T&amp;gt;(Task&amp;lt;T&amp;gt; work)
    {
        var currentSyncContext = SynchronizationContext.Current;
        try
        {
            SynchronizationContext.SetSynchronizationContext(null);
            return work.Result;
        }
        finally
        {
            SynchronizationContext.SetSynchronizationContext(currentSyncContext);
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I think that this is quite an elegant solution and makes for clear code (it is hopefully fairly clear to a reader of the code that there is something interesting going on at the non-async / async boundary and there is a nice summary comment explaining why).&lt;/p&gt;

&lt;p&gt;A variation on this theme is..&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Approach four: Use a custom INotifyCompletion implementation&lt;/strong&gt;. When &lt;strong&gt;Task&lt;/strong&gt;/&lt;strong&gt;Task&amp;lt;T&amp;gt;&lt;/strong&gt; was added to .NET along with async / await, the design included ways to override how awaiting a task should be handled and this gives us another way to remove the synchronization context for async work. We can take advantage of this facility by doing something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class HomeController : Controller
{
    public ActionResult Index()
    {
        return View(
            GetTitleAsync().Result
        );
    }

    private async static Task&amp;lt;string&amp;gt; GetTitleAsync()
    {
        await new SynchronizationContextRemover();

        // This Task.Delay call simulates an async call that might go off to the
        // database or other external service
        await Task.Delay(1000);
        return &quot;Hello!&quot;;
    }
}

/// &amp;lt;summary&amp;gt;
/// This prevents any synchronization context from affecting what happens within
/// an async method and so we don&#39;t need to worry if a non-async caller wants to
/// block while waiting for the result of the async method
/// &amp;lt;/summary&amp;gt;
public struct SynchronizationContextRemover : INotifyCompletion
{
    public bool IsCompleted =&amp;gt; SynchronizationContext.Current == null;

    public void OnCompleted(Action continuation)
    {
        var prevContext = SynchronizationContext.Current;
        try
        {
            SynchronizationContext.SetSynchronizationContext(null);
            continuation();
        }
        finally
        {
            SynchronizationContext.SetSynchronizationContext(prevContext);
        }
    }

    public SynchronizationContextRemover GetAwaiter() =&amp;gt; this;

    public void GetResult() { }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;(This code comes from the article &quot;&lt;a href=&quot;https://blogs.msdn.microsoft.com/benwilli/2017/02/09/an-alternative-to-configureawaitfalse-everywhere/&quot;&gt;An alternative to ConfigureAwait(false) everywhere&lt;/a&gt;&quot;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This has the same effect as the previous approach - it removes any synchronization context until the async work has completed - but there is an important difference in how it is implemented:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;When &lt;strong&gt;disabling the synchronization context before calling async code&lt;/strong&gt;, the extra code is included in the non-async code that is calling the async code&lt;/li&gt;
&lt;li&gt;When we &lt;strong&gt;use a custom INotifyCompletion implementation&lt;/strong&gt;, the extra code is included in the async code and the non-async calling code does not need to be changed&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I prefer these two approaches and choosing which of them to use comes down to what code I&#39;m writing and what code I&#39;m integrating with. For example:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;If I was writing the non-async code and I needed to call into a trusted and battle-tested async library then I might be tempted to do nothing at all because I would expect such a library to follow recommended practices such as using &quot;.ConfigureAwait(false)&quot; internally&lt;/li&gt;
&lt;li&gt;If I was writing non-async code that had to call into async code that I was less confident about then I would call it using &quot;AsyncCallHelpers.WaitForAsyncResult&quot; to be sure that nothing was going to go awry

&lt;ul&gt;
&lt;li&gt;Note: This only applies to non-async code that will be hosted in an environment that uses a synchronization context that I need to be worried about (such as ASP.NET or WinForms but &lt;em&gt;not&lt;/em&gt; Console Applications or Windows Services)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;If I was writing async code that might be called from different environments (where an awkward synchronization context might come into play), then I would use the &lt;strong&gt;SynchronizationContextRemover&lt;/strong&gt; approach at the public boundaries (so that I wouldn&#39;t need to specify &quot;.ConfigureAwait(false)&quot; everytime that I await something in my code)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Two more &quot;solutions&quot; to round out the post&lt;/h3&gt;

&lt;p&gt;To quickly recap, the commonly-suggested recommendations for avoiding the &#39;classic deadlock problem&#39; are:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Don&#39;t mix async and non-async code&lt;/li&gt;
&lt;li&gt;Always use &quot;.ConfigureAwait(false)&quot;&lt;/li&gt;
&lt;li&gt;Disabling the synchronization context before calling async code&lt;/li&gt;
&lt;li&gt;Use a custom INotifyCompletion implementation&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;.. but there are two others that I think are worthy of mention.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Approach five: Use ASP.NET Core&lt;/strong&gt; - the synchronization context that was used for previous versions of ASP.NET is not present in ASP.NET Core and so you don&#39;t have to worry if you&#39;re able to use it. If you already have a large application using non-Core ASP.NET that you are trying to introduce some async code into then whether or not this approach is feasible will likely depend upon your current code base and how much time you are willing to spend on migrating to ASP.NET Core.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Approach six: Use Task.Run&lt;/strong&gt; - this is a workaround that I have seen in some Stack Overflow answers. We could change our example code to look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class HomeController
{
    public ActionResult Index()
    {
        var result = Task.Run(async () =&amp;gt; { return await GetTitleAsync(); }).Result;
        return View(result);
    }

    private async Task&amp;lt;string&amp;gt; GetTitleAsync()
    {
        // This Task.Delay call simulates an async call that might go off to the
        // database or other external service
        await Task.Delay(1000);
        return &quot;Hello!&quot;;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This works because &quot;Task.Run&quot; will result in work being performed on a &lt;strong&gt;ThreadPool&lt;/strong&gt; thread and so the thread that calls into GetTitleAsync will not be associated with an ASP.NET synchronization context and so the deadlock won&#39;t occur.&lt;/p&gt;

&lt;p&gt;It feels more like a workaround, rather than a real solution, and I don&#39;t like the way that it&#39;s not as obvious from reading the code &lt;em&gt;why&lt;/em&gt; it works. It &lt;em&gt;could&lt;/em&gt; be wrapped in a method like &quot;AsyncCallHelpers.WaitForAsyncResult&quot; so that comments could be added to explain why it&#39;s being used but I feel like if you were going to do that then you would be better to use one of the more explicit approaches (such as the &quot;AsyncCallHelpers.WaitForAsyncResult&quot; method shown earlier). I have included it in this post only for completeness and because it is presented as a solution sometimes!&lt;/p&gt;

&lt;h3&gt;Further reading&lt;/h3&gt;

&lt;p&gt;To try to keep this post focused, I&#39;ve skipped over and simplified some of the details involved in how async and await work. I think that it&#39;s testament to the C# language designers that it can be such a complicated topic while the code &quot;just works&quot; most of the time, without you having to be aware of how it works all the way down.&lt;/p&gt;

&lt;p&gt;If you would like to find more then I would recommend the following articles. I read and re-read all of them while writing this to try to make sure that I wasn&#39;t over-simplifying &lt;em&gt;too&lt;/em&gt; much (and to try ensure that I didn&#39;t say anything patently false!)..&lt;/p&gt;

&lt;p&gt;Stephen Cleary&#39;s &quot;&lt;a href=&quot;https://blog.stephencleary.com/2013/11/there-is-no-thread.html&quot;&gt;There is no thread&lt;/a&gt;&quot;&lt;/p&gt;

&lt;p&gt;Also Stephen Cleary&#39;s (this time published on msdn.microsoft.com) &quot;&lt;a href=&quot;https://msdn.microsoft.com/en-us/magazine/gg598924.aspx&quot;&gt;Parallel Computing - It&#39;s All About the SynchronizationContext&lt;/a&gt;&quot;&lt;/p&gt;

&lt;p&gt;Dixin&#39;s &quot;&lt;a href=&quot;https://weblogs.asp.net/dixin/understanding-c-sharp-async-await-2-awaitable-awaiter-pattern&quot;&gt;Understanding C# async / await: The Awaitable-Awaiter Pattern&lt;/a&gt;&quot;&lt;/p&gt;</description>
			<pubDate>Sun, 30 Sep 2018 20:09:00 GMT</pubDate>
		</item>
		<item>
			<title>Writing F# to implement &#39;The Single Layer Perceptron&#39;</title>
            <link>http://www.productiverage.com/writing-f-sharp-to-implement-the-single-layer-perceptron</link>
			<guid>http://www.productiverage.com/writing-f-sharp-to-implement-the-single-layer-perceptron</guid>
			<description>&lt;h3&gt;TL;DR&lt;/h3&gt;

&lt;p&gt;This picks up from my last post &lt;a href=&quot;http://www.productiverage.com/learning-f-sharp-via-some-machine-learning-the-single-layer-percepton&quot;&gt;Learning F# via some Machine Learning: The Single Layer Perceptron&lt;/a&gt; where I described a simple neural network (&quot;The Single Layer Perceptron&quot;) and took a C# implementation (from an article on the site &lt;a href=&quot;http://www.robosoup.com/2008/09/the-single-layer-perceptron-c.html&quot;&gt;Robosoup&lt;/a&gt;) and rewrote it into a style of &quot;functional C#&quot; with the intention of then translating it into F#. Trying to do that all in one post would have made for a very &lt;em&gt;very&lt;/em&gt; long read and so part two, here, picks things up from that point.&lt;/p&gt;

&lt;p&gt;I&#39;m still an F# beginner and so I&#39;m hoping that having the pain so fresh in my mind of trying to pick it up as a new language will make it easier for me to help others get started. I&#39;m going to assume zero knowledge from the reader.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(I&#39;m also going to try to dive straight right into things, rather than covering loads of theory first - I think that there are a lot of good resources out there that introduce you to F# and functional concepts at a more abstract level but I&#39;m going to take the approach that we want to tackle something specific and we&#39;ll discuss new F# concepts only when we encounter them while trying to get this work done!)&lt;/em&gt;&lt;/p&gt;

&lt;h3&gt;Translating the code into F#&lt;/h3&gt;

&lt;p&gt;Visual Studio 2017 includes support for F# without having to install anything extra. To get started, create a new project of type Visual F# / Console Application. This will generate a Program.fs file that will let you build and run (it won&#39;t be anything very interesting if you run it but that doesn&#39;t matter because we&#39;re going rewrite the file from scratch!).&lt;/p&gt;

&lt;p&gt;In the C# code from last time, the core logic was contained within a static method called &quot;Go&quot; within a static class. To set up the scaffolding for something similar in F# we&#39;ll use the following code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;open System;

let private Go (r: Random) =
    &quot;TODO: Implement this&quot;

[&amp;lt;EntryPoint&amp;gt;]
let private Main _ = 
    Go (new Random(0)) |&amp;gt; ignore
    0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In F#, functions are declared by the keyword &quot;let&quot; optionally followed by an accessibility modifier (eg. &quot;private&quot;) followed by their name followed by their arguments followed by &quot;=&quot; followed by the function body.&lt;/p&gt;

&lt;p&gt;The last line of the function body will be a value that is returned by the function (unlike C#, there is no need for an explicit &quot;return&quot; keyword).&lt;/p&gt;

&lt;p&gt;Above, there is a function &quot;Go&quot; that takes a &lt;strong&gt;Random&lt;/strong&gt; argument named &quot;r&quot; and that returns a string. The return type is not explicitly declared anywhere but F# relies upon type inference a lot of the time to make reduce the &quot;syntactic noise&quot; around declaring types where the compiler can work them out on its own. If you wanted reassurance that the type inference has worked as you expect then you can hover over the word &quot;Go&quot; and you&#39;ll see the following signature for the function -&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;val private Go : r:Random -&gt; string&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This confirms that the function &quot;Go&quot; takes an argument named &quot;r&quot; of type &lt;strong&gt;random&lt;/strong&gt; and that it returns a &lt;strong&gt;string&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;If we changed the &quot;Go&quot; function to this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;let private Go r =
    &quot;TODO: Implement this&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. and then hovered over the word &quot;Go&quot; then we&#39;d see the following signature:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;val private Go : r:&#39;a -&gt; string&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This essentially means that the type &quot;r&quot; is not fixed and that it may be any type because there is no way for the compiler to apply any restrictions to it based upon the code that it has access to. When comparing to C#, you might imagine that this would be equivalent to this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;private string Go(object r)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. but it would actually be more accurate to think about it like a generic method - eg.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;private string Go&amp;lt;T&amp;gt;(T r)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The difference isn&#39;t important right now it&#39;s worth bearing in mind.&lt;/p&gt;

&lt;p&gt;There&#39;s also a function &quot;Main&quot; that takes a single string argument argument named &quot;_&quot; and that returns an int. Just looking at this code, you may imagine that &quot;_&quot; would also be of an unknown / generic type but if you hover to the word &quot;Main&quot; then you&#39;ll see this signature:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;val private main : string [] -&gt; int&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;F# has applied some extra logic here, based upon the fact that the function has been annotated with &lt;strong&gt;[&amp;lt;EntryPoint&amp;gt;]&lt;/strong&gt; - this requires that the function matches the particular signature of string-array-to-string and you will get a compile error if you try to declare a function signature that differs from this.&lt;/p&gt;

&lt;p&gt;The string array is a list of arguments passed to the compiled executable if called from the command line. This will never be of use in this program and so I&#39;ve named that argument &quot;_&quot; to tell F# that I will never want to access it. I do this because F# will warn you if you have any unused arguments because it suggests that you have forgotten something (why specify an argument if you don&#39;t need it??). If you really don&#39;t care about one, though (as is the case here), if you give it an underscore prefix (or call it simply &quot;_&quot;) then the compiler won&#39;t warn you about it.&lt;/p&gt;

&lt;p&gt;In a similar vein, F# will warn you if you call a function and ignore its return value. If the idea is that all functions be pure (and so have no side effects) then a function is useless if you ignore its return value. In the scaffolding above, though, we just want to call &quot;Go&quot; (which will do some calculations and write a summary to the console) - we don&#39;t really care about its return value. To tell the compiler this, we use a special function called &quot;ignore&quot; that we pass the return value of the &quot;Go&quot; function to. The C# way to do this might look something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ignore(Go(new Random(0)))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is valid F# but it&#39;s criticised as having to be read &quot;inside out&quot;. It&#39;s more common in F# to see it like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Go (new Random(0)) |&amp;gt; ignore
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &quot;pipe forward&quot; operator (|&amp;gt;) effectively means take the value on the left and use it as the last argument in the function on the right. Since &quot;ignore&quot; only takes one argument, the two versions above are equivalent.&lt;/p&gt;

&lt;p&gt;If a function has more than one argument then the pipe operator only provides the last one. To illustrate this, consider the method &quot;List.map&quot; that takes two arguments; a &quot;mapping&quot; delegate and a list of items. It&#39;s very similar to LINQ&#39;s &quot;Select&quot; method. You could call it like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;let numbers = [1;2;3]
let squares = List.map (fun x -&amp;gt; x * x) numbers
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I&#39;ll breeze through some of the syntax above in a moment but the important point here is that there is a method that takes two arguments where the second is a list.&lt;/p&gt;

&lt;p&gt;It could be argued that this syntax is back-to-front because you may describe this in English as:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;given a list of values, perform an operation on each item (and return a new list containing the transformed - or &quot;mapped&quot; - values)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;.. but the code puts things in the opposite order (&quot;list of values&quot; is mentioned last instead of first).&lt;/p&gt;

&lt;p&gt;However, the pipe operator changes that -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;let numbers = [1;2;3]
let squares = numbers |&amp;gt; List.map (fun x -&amp;gt; x * x)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The code now is able to say &quot;here is the list, perform this operation on each value to provide me with a new list&quot;.&lt;/p&gt;

&lt;p&gt;Because the pipe operator passes the value on the left as the &lt;em&gt;last&lt;/em&gt; argument to the function on the right, F# often has list-based functions where the list is the last argument. This is often the opposite order to C# functions, where the &quot;subject&quot; of the operation is the commonly first argument.&lt;/p&gt;

&lt;p&gt;Now, as promised, a quick rundown of F# syntax introduced above. The &quot;let&quot; keyword is very similar to C#&#39;s &quot;var&quot; in that it use type inference to determine what type the specific reference should be. &lt;em&gt;Unlike&lt;/em&gt; &quot;var&quot;, though, you can&#39;t change the reference later on - eg.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;let numbers = [1;2;3]

// Invalid! This &quot;=&quot; operator is treated as a comparison whose return value is ignored
// rather than this statement being a reassignment - the &quot;numbers&quot; reference is still
// a list with the values 1, 2 and 3 (a compiler warning will be displayed)
numbers = [1;2;3;4]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Because F# &lt;em&gt;only&lt;/em&gt; allows you to set value in statements that include the &quot;let&quot; operator, it makes it easier for the F# compiler to know whether the code fragment:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;a = b
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;is an assignment or a comparison - if it follows a &quot;let&quot; then it&#39;s always an assignment but otherwise it&#39;s a comparison.&lt;/p&gt;

&lt;p&gt;This is unlike C# where the following is acceptable:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var numbers = new[] { 1, 2, 3 };

// This is allowed in C#
numbers = new[] { 1, 2, 3, 4 };
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. and this means that the C# compiler can&#39;t as easily tell whether the code fragment &quot;a = b&quot; is an assignment or a comparison and &lt;em&gt;that&lt;/em&gt; is why C# has the assignment operator &quot;=&quot; and a separate equality comparison operator &quot;==&quot; (and why F# can use &quot;=&quot; as both the assignment operator &lt;em&gt;and&lt;/em&gt; equality comparison operator).&lt;/p&gt;

&lt;p&gt;The next thing to talk about is that F# allows you to declare a list of values using square brackets and semi-colons as the separators. So the below are &lt;em&gt;similar&lt;/em&gt; (but not equivalent, as I&#39;ll explain) in C#
and F# -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var numbers = new List&amp;lt;int&amp;gt; { 1, 2, 3 }; // C#

let numbers = [1;2;3] // F#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The reason that they&#39;re similar and not identical is that the C# code uses the &lt;strong&gt;System.Collections.Generic.List&amp;lt;T&amp;gt;&lt;/strong&gt; type, which is mutable (you can add, remove and replace items within a list). In F#, the list is &lt;em&gt;immutable&lt;/em&gt; and so any operation (such as add, remove or replace) would return a new list reference, rather than mutating the existing list.&lt;/p&gt;

&lt;p&gt;You may have noticed that semi-colons are not present at the end of each F# line in the examples above. That&#39;s because they are not required. F# uses whitespace (such as line returns and indenting) to determine when statements terminate and when they continue and so semi-colons are not used to specify where statements finish (unlike in C#, where they are).&lt;/p&gt;

&lt;p&gt;Finally, there was a delegate shown in the above code -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(fun x -&amp;gt; x * x)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is an anonymous function. The F# code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;let numbers = [1;2;3]
let squares = numbers |&amp;gt; List.map (fun x -&amp;gt; x * x)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;is roughly the same as the following C# code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var numbers = new[] { 1, 2, 3 };
var squares = numbers.Select(x =&amp;gt; x * x);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It&#39;s not &lt;em&gt;precisely&lt;/em&gt; the same since &quot;numbers&quot; in the F# code is an immutable list reference and in C# it&#39;s an array but it&#39;s close enough. The point is that the &quot;fun&quot; keyword is used to declare an anonymous function and that brackets are required around that function declaration in order to segregate that code and make it clear to the compiler that the function declaration should be considered as a single value that is being passed to the &quot;List.map&quot; function.&lt;/p&gt;

&lt;h3&gt;Declaring training data&lt;/h3&gt;

&lt;p&gt;In the C# perceptron code from &lt;a href=&quot;http://www.productiverage.com/learning-f-sharp-via-some-machine-learning-the-single-layer-percepton&quot;&gt;last week&lt;/a&gt;, there was an array of &lt;strong&gt;Tuple&lt;/strong&gt; values that contained each pair of inputs and the expected result -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var trainingData = new[]
{
  Pattern(0.08, 0.94, true),

  // .. more patterns
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &quot;Pattern&quot; function was just this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;private static Tuple&amp;lt;double[], bool&amp;gt; Pattern(double x, double y, bool output)
{
  return Tuple.Create(new[] { x, y }, output);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;strong&gt;Tuple&lt;/strong&gt; class is a very convenient way to represent small groups of values (such as an input array and the expected boolean output) but one thing that I don&#39;t like is that the properties are named &quot;Item1&quot;, &quot;Item2&quot;, etc.. rather than it being possible to give them more descriptive labels.&lt;/p&gt;

&lt;p&gt;I could have defined a class to contain these values but that can involve a lot of boilerplate code (particularly if it&#39;s an immutable class, which would be my preference when creating classes that describe data that should be initialised once and never changed).&lt;/p&gt;

&lt;p&gt;Fortunately, F# has a convenient way to describe data like this called &quot;&lt;a href=&quot;https://docs.microsoft.com/en-us/dotnet/fsharp/language-reference/records&quot;&gt;Records&lt;/a&gt;&quot; - immutable types that may be defined using very little syntax, such as by pasting the following into the F# scaffolding from earlier, just after &quot;Open System&quot; -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type private Input = { Values: float list; Result: bool }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It is now possible to define an input list / output boolean object with properties name &quot;Values&quot; and &quot;Result&quot; like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;let x = { Values = [0.08; 0.94]; Result = true }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The type of &quot;x&quot; is not explicitly specified in the code but the compiler will be able to match it to the &lt;strong&gt;Input&lt;/strong&gt; type.&lt;/p&gt;

&lt;h3&gt;double vs float&lt;/h3&gt;

&lt;p&gt;Note that I&#39;ve defined the &quot;Values&quot; property to be of type &quot;float list&quot; (which is equivalent to &quot;list&amp;lt;float&amp;gt;&quot; - which is also valid syntax in F#) as opposed to &quot;double list&quot;. In C#, &lt;strong&gt;Double&lt;/strong&gt; and &lt;strong&gt;double&lt;/strong&gt; represent a &quot;double-precision floating point number&quot; while &lt;strong&gt;Single&lt;/strong&gt; and &lt;strong&gt;float&lt;/strong&gt; represent a &quot;single-precision floating point number&quot;. In F#, &lt;strong&gt;float&lt;/strong&gt; is a double-precision floating point number while &lt;strong&gt;float32&lt;/strong&gt; is a &lt;em&gt;single&lt;/em&gt;-precision floating point number. So &quot;float&quot; in F# is the same as &quot;double&quot; in C#. To make things more confusing, you can also specify the type &lt;strong&gt;double&lt;/strong&gt; in F# and it means the same as &lt;strong&gt;float&lt;/strong&gt; in F# - however, type signatures in the F# library specify &lt;strong&gt;float&lt;/strong&gt; when a double-precision floating point number is returned and so I&#39;m specifying &lt;strong&gt;float&lt;/strong&gt; to try to be consistent with other F# code. For example, the library function &quot;Double.Parse&quot; returns &lt;strong&gt;float&lt;/strong&gt; according to intellisense.&lt;/p&gt;

&lt;p&gt;This seems quite confusing to me (coming at this from C#) but I decided to try to be as F#-like as possible and so &quot;float list&quot; is what I&#39;m using.&lt;/p&gt;

&lt;h3&gt;Back to declaring training data..&lt;/h3&gt;

&lt;p&gt;To declare all of the training data in F#, we want a list of patterns -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;let trainingData = [
    { Values = [0.08; 0.94]; Result = true }
    { Values = [0.13; 0.95]; Result = true }

    // .. more patterns
]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When declaring a list (items within square brackets), they may either be separated by semi-colons or by line returns. So, above, each pattern is on its own line and so no semi-colons need to separate them while the individual numbers within the &quot;Values&quot; lists &lt;em&gt;do&lt;/em&gt; need semi-colon separators since there are no line returns to break them up.&lt;/p&gt;

&lt;p&gt;In the C# code, the &quot;Pattern&quot; function specifically took two &quot;x&quot; and &quot;y&quot; arguments and so each &lt;strong&gt;Tuple&lt;/strong&gt; had an &quot;Item1&quot; property which was an array with two elements. In the above code, there would be no compiler warning if I accidentally included a line where the pattern had more &quot;Values&quot; entries than the others. As a sanity check, we can include the following code after the &quot;trainingData&quot; list is declared -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;let inputLengths =
    trainingData
    |&amp;gt; List.map (fun input -&amp;gt; input.Values.Length)
    |&amp;gt; List.distinct
    |&amp;gt; List.length
if (inputLengths &amp;gt; 1) then raise (Exception &quot;Inconsistent pattern input lengths!&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There&#39;s a lot of piping here, which seems to be quite common in F# code. Hopefully, though, it illustrates how using the pipe operator allows code to be written in a more logical order. Here, we&#39;re saying:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Take the trainingData list&lt;/li&gt;
&lt;li&gt;Construct a list where each entry in the new list corresponds to the number of inputs in a trainingData entry&lt;/li&gt;
&lt;li&gt;Build a new list from &lt;em&gt;this&lt;/em&gt; by taking this list-of-input-lengths and excluding any duplicates&lt;/li&gt;
&lt;li&gt;If the list from step 3 has more than one entry then the trainingData must not have entries that all have the same number of inputs&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If the &quot;trainingData&quot; has patterns which all have the same number of inputs then there should only be one unique input input-list-length. If some patterns had two inputs and some patterns had three inputs then we would get more than one &lt;em&gt;unique&lt;/em&gt; input-list-length and that would not be good.&lt;/p&gt;

&lt;p&gt;Since F# has a concept of &quot;significant whitespace&quot; (meaning that it uses line returns and indentation to indicate where expressions start and end, which is why semi-colons are not required to terminate lines), sometimes it can get a bit demanding about what it thinks it ok and what isn&#39;t. In the code above, if you tried to put the &quot;trainingData&quot; on the same line as the &quot;let inputLengths =&quot; and then have the pipe operator lines start underneath it then you will get cryptic errors such as &quot;The block following this &#39;let&#39; is unfinished&quot;. Using the format above not only means that your code will be more consistent with other F# &quot;in the wild&quot; but it also means that the compiler will understand it!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// The F# compiler is happy with this..
let inputLengths =
    trainingData
    |&amp;gt; List.map (fun input -&amp;gt; input.Values.Length)
    |&amp;gt; List.distinct
    |&amp;gt; List.length

// .. it is NOT happy with this..
let inputLengths = trainingData
    |&amp;gt; List.map (fun input -&amp;gt; input.Values.Length)
    |&amp;gt; List.distinct
    |&amp;gt; List.length
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(I would not have thought that putting &quot;trainingData&quot; on the same line as &quot;let inputLengths =&quot; would introduce any ambiguity but presumably doing similar things must do in some situations).&lt;/p&gt;

&lt;h3&gt;Translating the network-training code&lt;/h3&gt;

&lt;p&gt;The c# code that we ended up with &lt;a href=&quot;http://www.productiverage.com/learning-f-sharp-via-some-machine-learning-the-single-layer-percepton&quot;&gt;last time&lt;/a&gt; for training a network looked like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;const double learningRate = 0.1;

var finalResult = Enumerable.Range(0, int.MaxValue)
  .Scan(
    seed: new
    {
      Weights = new[] { r.NextDouble(), r.NextDouble() },
      Bias = 0d,
      GlobalError = double.MaxValue
    },
    func: (previousState, iteration) =&amp;gt;
    {
      var resultForIteration = trainingData.Aggregate(
        seed: new { Weights = previousState.Weights, Bias = previousState.Bias, GlobalError = 0d },
        func: (stateSoFar, pattern) =&amp;gt;
        {
          var output = Output(stateSoFar.Weights, stateSoFar.Bias, pattern.Item1) ? 1 : -1;
          var localError = (pattern.Item2 ? 1 : -1) - output;
          return new
          {
            Weights = UpdateWeights(stateSoFar.Weights, learningRate, localError, pattern.Item1),
            Bias = stateSoFar.Bias + (learningRate * localError),
            GlobalError = stateSoFar.GlobalError + Math.Abs(localError)
          };
        }
      );
      Console.WriteLine(&quot;Iteration {0}\tError {1}&quot;, iteration, resultForIteration.GlobalError);
      return resultForIteration;
    }
  )
  .First(state =&amp;gt; state.GlobalError &amp;lt;= 0);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. and relied upon the following two functions:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;private static bool Output(double[] weights, double bias, double[] inputs)
{
  var sum = inputs.Zip(weights, (input, weight) =&amp;gt; input * weight).Sum() + bias;
  return (sum &amp;gt;= 0);
}

private static double[] UpdateWeights(double[] weights, double learningRate, double localError, double[] inputs)
{
  if (localError == 0)
    return weights;

  return weights
    .Zip(inputs, (weight, input) =&amp;gt; weight + (learningRate * localError * input))
    .ToArray();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I&#39;m going to start with translating the &quot;Output&quot; function first because it will be relatively straight forward but it will also demonstrate some interesting abilities of the F# compiler&#39;s type inference abilities.&lt;/p&gt;

&lt;p&gt;Type inference means that there are a &lt;em&gt;lot&lt;/em&gt; of types that you don&#39;t have to specify in F# code because the compiler will be able to work out what they are. But this can be confusing sometimes if you don&#39;t have a strong enough grasp on &lt;em&gt;how&lt;/em&gt; the compiler does this.&lt;/p&gt;

&lt;p&gt;Because I&#39;m still an F# noob, I like to specify function arguments types to begin with and then remove them afterwards once I can see that the compiler is happy without them &lt;em&gt;and when I understand how the compiler knows&lt;/em&gt;. So I&#39;ll start with this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;let Output (weights: float list) (bias: float) (inputs: float list) =
    let sum = (List.zip weights inputs |&amp;gt; List.map (fun (weight, input) -&amp;gt; weight * input) |&amp;gt; List.sum) + bias
    sum &amp;gt;= float 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The brackets around the arguments are required to &quot;group&quot; the argument name and its type into one value. When we remove the type annotations shortly, the argument brackets will no longer be necessary.&lt;/p&gt;

&lt;p&gt;The &quot;List.zip&quot; function is very similar to LINQ&#39;s &quot;Zip&quot; function except that it has no facility to take a delegate to combine the two values, instead it always returns a tuple for each pair of values that it combines.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(I didn&#39;t use the pipe operator with the &quot;List.zip&quot; call above because I think that it read more naturally without it in this case - I think of this as &quot;zipping the weights and inputs lists together&quot; and that is what the code says)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;F# has nice support for tuples that allows us to avoid having to rely upon &quot;Item1&quot; and &quot;Item2&quot; accesses. The lambda above that performs multiplication would have to look something like this in C# if the input was a tuple:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;weightAndInput =&amp;gt; weightAndInput.Item1 * weightAndInput.Item2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. but F# allows us to &quot;deconstruct&quot; the tuple by providing names for the tuple properties - ie.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fun (weight, input) -&amp;gt; weight * input
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is still a function that takes a single argument, it&#39;s just that that single argument is a two-item tuple and we&#39;re accessing its two items through named references &quot;weight&quot; and &quot;input&quot;.&lt;/p&gt;

&lt;p&gt;Hopefully the rest of the code is easy to understand, &quot;List.zip&quot; is like LINQ&#39;s &quot;Zip&quot; and &quot;List.map&quot; is like LINQ&#39;s &quot;Select&quot; and &quot;List.sum&quot; is like LINQ&#39;s &quot;Sum&quot;.&lt;/p&gt;

&lt;p&gt;The second line &quot;sum &amp;gt;= float 0&quot; is the return value for the function - either true or false. The expression &quot;float 0&quot; is important because the &quot;sum&quot; value will be a &lt;strong&gt;float&lt;/strong&gt; and F# will not attempt &lt;em&gt;any&lt;/em&gt; type coercion when comparing values. In C#, if you have two numeric types then you can compare them - eg.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Valid in C#
double x1 = 0; // double
int x2 = 0;    // int
var isMatch = (x1 == x2);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. but in F# this is not allowed. If you tried to write the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Not allowed in F#
let x1 = float 0 // float
let x2 = 0       // int
let isMatch = (x1 = x2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. then you would get the following error:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;This expression was expected to have type &#39;float&#39; but here has type &#39;int&#39;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Now that we&#39;re happy with the function implementation, we can remove the type annotations and reduce it to this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;let Output weights bias inputs =
    let sum = (List.zip weights inputs |&amp;gt; List.map (fun (weight, input) -&amp;gt; weight * input) |&amp;gt; List.sum) + bias
    sum &amp;gt;= float 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The compiler is able to infer all of those types. Some of the inference is quite simple - for example, both &quot;weights&quot; and &quot;inputs&quot; must be lists of some type because they are passed to &quot;List.zip&quot;.&lt;/p&gt;

&lt;p&gt;Some of the inference is more complicated, though..&lt;/p&gt;

&lt;p&gt;Firstly, the &quot;weights&quot; and &quot;inputs&quot; list must have element types that support a &quot;*&quot; operator (in F#, this means any of the numeric types or any type that has got a custom &quot;*&quot; overload implemented on it).&lt;/p&gt;

&lt;p&gt;Secondly, when elements are combined from &quot;weight&quot; and &quot;inputs&quot; using &quot;*&quot;, it must be possible to use the &quot;+&quot; operator on the result because &quot;List.sum&quot; requires it (the internal implementation of &quot;List.sum&quot; is to combine all of the values passed to it using &quot;+&quot;).&lt;/p&gt;

&lt;p&gt;Thirdly, the result from &quot;List.sum&quot; must also support the &quot;+&quot; operator in conjunction with whatever type that &quot;bias&quot; is.&lt;/p&gt;

&lt;p&gt;Fourthly, &lt;em&gt;this&lt;/em&gt; result must support the &quot;&amp;gt;=&quot; operator in conjunction with &quot;float 0&quot;.&lt;/p&gt;

&lt;p&gt;Working backwards, because F# does not support any type coercion when comparing numeric values, the type of &quot;sum&quot; must be &lt;strong&gt;float&lt;/strong&gt; in order for it to be compared to &quot;float 0&quot;. This means that the result of &quot;List.sum&quot; must be &lt;strong&gt;float&lt;/strong&gt; and so &quot;bias&quot; must be &lt;strong&gt;float&lt;/strong&gt;. This means that the &quot;weights&quot; and &quot;inputs&quot; must be lists of &lt;strong&gt;float&lt;/strong&gt;. (The return type of the function is &lt;strong&gt;boolean&lt;/strong&gt; because the return value is always true or false as it is the result of an &quot;&amp;gt;=&quot; comparison).&lt;/p&gt;

&lt;p&gt;This type inference is very powerful and can lead to clean and succint code. However, it can also lead to confusion if you haven&#39;t perfectly internalised its workings or if you&#39;re dealing with incomplete code. It&#39;s for both of those reasons that I prefer to start with more argument type annotations than necessary and then remove them later, when I&#39;m happy with what I&#39;ve written.&lt;/p&gt;

&lt;p&gt;The &quot;UpdateWeights&quot; function may be translated in a similar manner -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;let UpdateWeights weights localError inputs =
    if (localError = float 0)
        then weights
        else
            List.zip weights inputs
            |&amp;gt; List.map (fun (weight, input) -&amp;gt; weight + (learningRate * localError * input))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In F#, if / then / else is a bit different to C#. In F#, it is an expression that returns a value, so you could write something like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Valid F#
let x = if something then 1 else 2

// Not valid C#
var x = if something then 1 else 2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So, in the F# &quot;UpdateWeights&quot; function, the &quot;if&quot; expression returns either the original &quot;weights&quot; reference or the updated list.&lt;/p&gt;

&lt;p&gt;We&#39;ve actually seen quite a lot of F# syntax, just in the code above - variable and function definitions, type annotations (and discussed how they are optional in many cases), anonymous functions (with the &quot;fun&quot; keyword), the pipe forward operator, record types, tuple deconstruction. Let&#39;s throw in another one; nested functions. The two functions shown above (&quot;Output&quot; and &quot;UpdateWeights&quot;) will only be called from within the &quot;Go&quot; function that was part of the initial scaffolding code. We &lt;em&gt;could&lt;/em&gt; make these private functions at the same level as &quot;Go&quot;.. &lt;em&gt;or&lt;/em&gt; we can make them &lt;em&gt;nested&lt;/em&gt; functions &lt;em&gt;within&lt;/em&gt; &quot;Go&quot; so that their scope is as restrictive as possible (which is a good thing in my book) -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;let private Go (r: Random) =

    let Output weights bias inputs =
        let sum = (List.zip weights inputs |&amp;gt; List.map (fun (weight, input) -&amp;gt; weight * input) |&amp;gt; List.sum) + bias
        sum &amp;gt;= float 0

    let UpdateWeights weights localError inputs =
        if (localError = float 0)
            then weights
            else
                List.zip weights inputs
                |&amp;gt; List.map (fun (weight, input) -&amp;gt; weight + (learningRate * localError * input))

    &quot;TODO: Implement this&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;Sidebar: The influence of F# on C#&lt;/h3&gt;

&lt;p&gt;It seems that quite a lot of features from F# are coming over to C# from C# 7 onwards. For example, nested functions are already available (they weren&#39;t in C# 6 but they &lt;em&gt;are&lt;/em&gt; in C# 7) - eg.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public static void Go()
{
  int GetNumber()
  {
    return 123;
  }

  Console.WriteLine(GetNumber());
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Similarly, Tuple deconstruction is also now available -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public static void Go()
{
  var (inputs, output) = GetPattern();
  Console.WriteLine(string.Join(&quot;, &quot;, inputs));
  Console.WriteLine(output);
}

// Note: We&#39;re not returning a &quot;Tuple&amp;lt;double[], bool&amp;gt;&quot; here, it&#39;s a different type (and it requires
// the &quot;System.ValueType&quot; package to be added to the project
private static (double[] inputs, bool output) GetPattern()
{
  return (new[] { 0.5, 0.6 }, true);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Coming at some point (looks like it will be C# 8), there will be support for defining record types -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// This syntax is not yet available (as of January 2018)
public class Point(int X, int Y);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;strong&gt;Point&lt;/strong&gt; class will have X and Y properties that are set through a constructor call. It will have an &quot;Equals&quot; implementation that will return true for two &lt;strong&gt;Point&lt;/strong&gt; references that have the same X and Y values (and probably have == and != operator overloads that do the same thing) &lt;em&gt;and&lt;/em&gt; it will have a &quot;With&quot; method that allows you to take an instance of a &lt;strong&gt;Point&lt;/strong&gt; and create a new instance that has a new value for either X or Y - eg.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var p1 = new Point(1, 2);
var p2 = new Point(1, 2);
Console.WriteLine(p1 == p2); // True!

p2 = p2.With(X: 7);
Console.WriteLine(p1 == p2); // False
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(For more details about C# record types, see the &lt;a href=&quot;https://github.com/dotnet/csharplang/blob/master/proposals/records.md&quot;&gt;records proposal&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;It&#39;s interesting to see these features working their way into C# and hopefully it will make it easier for someone in the future to try F# if they already know C#. (Some may argue that it could make F# less appealing with more of its features being added to C# but I think that it will still have enough differences to stand apart - having immutability and non-nulls &lt;em&gt;by default&lt;/em&gt; is not something that is likely to be incorporated into C# because it would require enormous changes).&lt;/p&gt;

&lt;h3&gt;Back to translating the network-training code..&lt;/h3&gt;

&lt;p&gt;Now that the supporting functions (&quot;Output&quot; and &quot;UpdateWeights&quot;) have been translated, we need to look back at the main training code. This time I&#39;m going to go &quot;outside in&quot; and translate this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;const double learningRate = 0.1;

var finalResult = Enumerable.Range(0, int.MaxValue)
  .Scan(
    seed: new
    {
      Weights = new[] { r.NextDouble(), r.NextDouble() },
      Bias = 0d,
      GlobalError = double.MaxValue
    },
    func: (previousState, iteration) =&amp;gt;
    {
      // Do work here..
    }
  )
  .First(state =&amp;gt; state.GlobalError &amp;lt;= 0);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &quot;Enumerable.Range(0, int.MaxValue)&quot; line was basically a way to say &quot;keep enumerating for ever&quot; (int.MaxValue isn&#39;t technically the same as &quot;forever&quot; but in this context it&#39;s good enough because we&#39;ll die of boredom waiting for the code to perform two billion iterations).&lt;/p&gt;

&lt;p&gt;In F# there is a function that seems closer to what we want called &quot;Seq.initInfinite&quot; - this takes a single argument that is a delegate that takes an int and returns a value in the generated sequence based upon that int. It could be implemented in C# like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public static IEnumerable&amp;lt;T&amp;gt; InitInfinite&amp;lt;T&amp;gt;(Func&amp;lt;int, T&amp;gt; initialiser)
{
  return Enumerable.Range(0, int.MaxValue).Select(initialiser);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is &lt;em&gt;also&lt;/em&gt; limited to int.MaxValue iterations since the delegate argument is an &lt;strong&gt;int&lt;/strong&gt; but we&#39;re &lt;em&gt;still&lt;/em&gt; not going to worry too much about whether it&#39;s &lt;em&gt;really&lt;/em&gt; infinite or not.&lt;/p&gt;

&lt;p&gt;From my last post, we know that &quot;Scan&quot; is already an F# concept and so that should be easy to translate.&lt;/p&gt;

&lt;p&gt;The last function to translate is &quot;First&quot; and this has a corresponding function in F#; &quot;Seq.find&quot;.&lt;/p&gt;

&lt;p&gt;The only issue that we have to tackle now is that F# does not support anonymous types and so we&#39;ll need to declare another record type that I&#39;ll call &quot;CalculationState&quot;. &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type private CalculationState = {
    Weights: List&amp;lt;float&amp;gt;
    Bias: float
    GlobalError: float
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When I defined the &quot;Input&quot; record earlier, I used a single line definition and so each property had to be separated by semi-colons. Above, each property is on its line and so semi-colon delimiters are not required.&lt;/p&gt;

&lt;p&gt;Now we can translate the above C# into this F#:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;let finalResult =
    Seq.initInfinite (fun i -&amp;gt; i)
    |&amp;gt; Seq.scan
        (fun previousState iteration -&amp;gt;
            // Do work here..
        )
        { Weights = [r.NextDouble(); r.NextDouble()]; Bias = float 0; GlobalError = Double.MaxValue }
    |&amp;gt; Seq.find (fun state -&amp;gt; state.GlobalError = float 0)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &quot;// Do work here..&quot; code looks like this in C# -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var resultForIteration = trainingData.Aggregate(
  seed: new { Weights = previousState.Weights, Bias = previousState.Bias, GlobalError = 0d },
  func: (stateSoFar, pattern) =&amp;gt;
  {
    var output = Output(stateSoFar.Weights, stateSoFar.Bias, pattern.Item1) ? 1 : -1;
    var localError = (pattern.Item2 ? 1 : -1) - output;
    return new
    {
      Weights = UpdateWeights(stateSoFar.Weights, learningRate, localError, pattern.Item1),
      Bias = stateSoFar.Bias + (learningRate * localError),
      GlobalError = stateSoFar.GlobalError + Math.Abs(localError)
    };
  }
);
Console.WriteLine(&quot;Iteration {0}\tError {1}&quot;, iteration, resultForIteration.GlobalError);
return resultForIteration;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I&#39;m going to break this out into a separate function in the F# code because I want to avoid the final code being too &quot;dense&quot; (particularly while I&#39;m still getting used to reading F# syntax and common structures / flow) so I&#39;ll change the F# outer code to this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;let finalResult =
    Seq.initInfinite (fun i -&amp;gt; i)
    |&amp;gt; Seq.scan
        CalculateNextState
        { Weights = [r.NextDouble(); r.NextDouble()]; Bias = float 0; GlobalError = Double.MaxValue }
    |&amp;gt; Seq.find (fun state -&amp;gt; state.GlobalError = float 0)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. and then define this nested function:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;let CalculateNextState (state: CalculationState) (iteration: int) =
    // Do work here..
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(Again, I&#39;ve started by including explicit type annotations for the arguments but I&#39;ll be able to remove them later).&lt;/p&gt;

&lt;p&gt;The C# code used the &quot;Aggregate&quot; function which corresponds to &quot;List.fold&quot; in F# and &quot;Console.WriteLine&quot; which corresponds to &quot;printfn&quot;. With everything that we&#39;ve covered already, it shouldn&#39;t be a big leap to see that the complete implementation of the &quot;CalculateNextState&quot; function will be as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;let CalculateNextState (state: CalculationState) (iteration: int) =
    let resultForIteration =
        List.fold
            (fun stateSoFar input -&amp;gt;
                let output = if (Output stateSoFar.Weights stateSoFar.Bias input.Values) then 1 else -1
                let localError = float ((if input.Result then 1 else -1) - output)
                {
                    Weights =
                        if (localError = float 0)
                        then stateSoFar.Weights
                        else UpdateWeights stateSoFar.Weights localError input.Values
                    Bias =
                        if (localError = float 0)
                        then stateSoFar.Bias
                        else stateSoFar.Bias + (learningRate * localError)
                    GlobalError = stateSoFar.GlobalError + Math.Abs(localError)
                }
            )
            { Weights = state.Weights; Bias = state.Bias; GlobalError = float 0 }
            trainingData
    printfn &quot;Iteration %i\tError %i&quot; iteration (int resultForIteration.GlobalError)
    resultForIteration
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It&#39;s still taking me a little while to get used to there being no &quot;return&quot; keyword and so I sometimes have to remind myself that the anonymous function passed to &quot;List.fold&quot; returns the { Weights, Bias, GlobalError } value and that the &quot;CalculateNextState&quot; function returns the &quot;resultForIteration&quot; that is on its last line.&lt;/p&gt;

&lt;p&gt;Now that the function is fully defined, the type annotations can be removed from the &quot;state&quot; and &quot;iteration&quot; arguments. The &quot;state&quot; type is inferred because &quot;List.fold&quot; takes an initial value that has the properties Weights (float list) / Bias (float) / GlobalError (float) and the anonymous function also returns a value of that type and the only record type that matches those properties is &quot;CalculationState&quot;. The &quot;iteration&quot; argument is inferred because it is used as an argument in the &quot;printfn&quot; call to populate a &quot;%i&quot; placeholder and &quot;%i&quot; placeholder values have to be integers.&lt;/p&gt;

&lt;h3&gt;Writing to console using printfn and &quot;string interpolation&quot;&lt;/h3&gt;

&lt;p&gt;You might have noticed that in the code above, the C# write-info-to-console line:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Console.WriteLine(&quot;Iteration {0}\tError {1}&quot;, iteration, resultForIteration.GlobalError);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;was translated into this in F#:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;printfn &quot;Iteration %i\tError %i&quot; iteration (int resultForIteration.GlobalError)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In principle, it&#39;s very similar; there are placeholders in the format string (which is what the &quot;%i&quot; values are in the F# code above) that will be populated with arguments passed to Console.WriteLine / printfn but there are a couple of key differences. The first is that the &quot;%i% placeholder requires that the value used to populate it is an integer (alternatives are &quot;%s&quot; for strings, &quot;%f&quot; for floats and &quot;%b&quot; for booleans) but the second is much more exciting - the format string and the provided arguments are verified &lt;em&gt;at compile time&lt;/em&gt; in the F# code whereas the C# code is only verified at &lt;em&gt;run time&lt;/em&gt;. To make it really crystal clear what I mean by this, the following C# code will compile but fail when it&#39;s run -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// This will fail at runtime with &quot;System.FormatException: &#39;Index (zero based) must be greater
// than or equal to zero and less than the size of the argument list.&#39;&quot; because there are two
// placeholders in the format string but only one value provided
Console.WriteLine(&quot;Hello {0}, {1}&quot;, &quot;test&quot;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;On the other hand, the following F# won&#39;t even &lt;em&gt;compile&lt;/em&gt; -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Will refuse to compile: &quot;This expression is a function value, i.e. is missing arguments.&quot;
printfn &quot;Hello %s, %s&quot; &quot;test&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This makes me happy because I&#39;m all about making the compiler catch simple mistakes instead of allowing them to surface at runtime.&lt;/p&gt;

&lt;p&gt;Now, I will admit that I was using a somewhat old school method of writing messages there in C#. C#6 introduced its own interpretation of &quot;string interpolation&quot; that allows us to combine the &quot;template string&quot; with the placeholder values so that we don&#39;t accidentally include too many or too few placeholder value arguments. Instead of writing this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Old style
Console.WriteLine(&quot;Iteration {0}\tError {1}&quot;, iteration, resultForIteration.GlobalError);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. we could write this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// C# 6 string interpolation
Console.WriteLine($&quot;Iteration {iteration}\tError {resultForIteration.GlobalError});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I would argue that this is even better again than the F# approach and it&#39;s unfortunate that F# doesn&#39;t currently have anything quite like this. That is one of the downsides to F# pioneering and pushing a lot of useful techniques that were later incorporated in to C#, I suppose!&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(There is a proposal to add something similar to F# but it doesn&#39;t exist yet and I don&#39;t think that there is any suggestions that it will become available any time soon - see &lt;a href=&quot;https://github.com/fsharp/fslang-design/blob/master/RFCs/FS-1001-StringInterpolation.md&quot;&gt;F# RFC FS-1001 - String Interpolation&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;h3&gt;Sidebar: Selecting F# BCL functions&lt;/h3&gt;

&lt;p&gt;A little earlier, I nonchalantly said that&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The C# code used the &quot;Aggregate&quot; function which corresponds to &quot;List.fold&quot; in F#&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;.. and you may (quite reasonably) have wondered how you or I were supposed to know that &quot;Aggregate&quot; in C# is equivalent to &quot;fold&quot; in F#.&lt;/p&gt;

&lt;p&gt;You may also have picked up on the fact that sometimes I&#39;m using &quot;Seq&quot; functions (such as &quot;Seq.initInfinite&quot;) and sometimes I&#39;m using &quot;List&quot; functions (such as &quot;List.fold&quot;) and be wondering how I&#39;m deciding which to use.&lt;/p&gt;

&lt;p&gt;I&#39;ll address the second point first. As I do so, it&#39;s worth bearing in mind that I&#39;m going to explain how &lt;em&gt;I&lt;/em&gt; have been deciding up to this point and &lt;em&gt;hopefully&lt;/em&gt; it&#39;s a sensible approach but there&#39;s always a chance that someone who knows better (maybe me in six months!) will have a slightly different take on things..&lt;/p&gt;

&lt;p&gt;In a nutshell, I&#39;m going to use &quot;List&quot; if I&#39;m certain that I want to fully evaluate the set of items. In the &quot;CalculateNextState&quot; function, I want to take all of the weights in the current state and generate a completely updated set of weights to use in the next iteration - in that next iteration, I will be using &lt;em&gt;all&lt;/em&gt; of the just-calculated weights to generate &lt;em&gt;another&lt;/em&gt; completely updated set of weights. Every time, I will be considering &lt;em&gt;every&lt;/em&gt; weight value and there would be no benefit to lazily evaluating the data and I think that lazy evaluation is one of the main benefits to using &quot;Seq&quot;. When I don&#39;t know how many iterations will be required, I start by lazily evaluating an infinite set of items by calling &quot;Seq.initInfinite&quot; and then terminating enumeration when I get a state with a sufficiently low GlobalError. This approach &lt;em&gt;only&lt;/em&gt; works because the sequence is evaluated &quot;lazily&quot; - it would make no sense for there to be a &quot;List.initInfinite&quot; because that list&#39;s contents would have to be fully populated at runtime and you&#39;d run out of memory!&lt;/p&gt;

&lt;p&gt;I suspect that a case &lt;em&gt;could&lt;/em&gt; be made for always using &quot;Seq&quot; unless you find a compelling reason not to.. where a compelling case is that you need pattern matching* or if you&#39;re sure that using &quot;Seq&quot; is resulting in expensive operations being repeated because you are enumerating over a sequence multiple times and the operations in each enumeration are complex / expensive (if you used &quot;List&quot; then you would be sure that the work to build the list would only be done once, no matter how many times you enumerated over it).&lt;/p&gt;

&lt;p&gt;* &lt;em&gt;(which we haven&#39;t encountered yet but which is fairly common in F# code and which only works with instances of &lt;strong&gt;list&lt;/strong&gt; and not of &lt;strong&gt;seq&lt;/strong&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;F# also supports arrays but these tend to used in fairly niche situations - such as when interoperating with other .NET code that requires an array or when you&#39;ve found a performance bottleneck in your code relating to indexed access into your set of items (for both a &lt;strong&gt;seq&lt;/strong&gt; and a &lt;strong&gt;list&lt;/strong&gt; it&#39;s relatively slow to jump straight to the nth item because you have to start at the beginning and walk that many items into the list, whereas with an array you can jump straight there).. but arrays have their disadvantages, such as being mutable (bleurgh, filthy!) and having no cheap way to create a new version with a single new item (which also applies to &lt;strong&gt;seq&lt;/strong&gt; but which is something that &lt;strong&gt;list&lt;/strong&gt; can do well).&lt;/p&gt;

&lt;p&gt;So (for now?) I&#39;ll be using a &lt;strong&gt;list&lt;/strong&gt; if I have a known set of items and will be performing an operation on every item each iteration and a &lt;strong&gt;seq&lt;/strong&gt; otherwise.. unless I encounter a really exciting reason to do otherwise*.&lt;/p&gt;

&lt;p&gt;* &lt;em&gt;(Spoiler alert: in a future post in the series, I will find a case where there is a huge difference in memory usage between &lt;strong&gt;list&lt;/strong&gt; and &lt;strong&gt;array&lt;/strong&gt; when loading data from disk - brace yourself for that thrill!)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;To return to my first point in relation to &quot;Selecting F# BCL functions&quot; - how did I know that &quot;List.fold&quot; is equivalent to &quot;Aggregate&quot;? The simple answer is by looking through the docs.. the MSDN pages are pretty good (here is the one for &lt;a href=&quot;https://msdn.microsoft.com/en-us/visualfsharpdocs/conceptual/list.fold%5B&amp;#39;t,&amp;#39;state%5D-function-%5Bfsharp%5D&quot;&gt;List.fold&lt;/a&gt;) and the number of base library functions is not that large. You can often guess what many of them do (such as &quot;List.average&quot; and &quot;List.distinct&quot;) but you might need to read the documentation for others (either on MSDN or just via the intellisense tooltips) for others. If you are familiar with LINQ then it shouldn&#39;t take you too long to learn the names of the F# equivalents of many of your old favourites!&lt;/p&gt;

&lt;h3&gt;Demonstrating the network&#39;s abilities&lt;/h3&gt;

&lt;p&gt;Before I went on a couple of tangents about writing to the console and learning the F# BCL, we had actually finished translating the code that trained the network (it may be an &lt;em&gt;extremely&lt;/em&gt; simple one but it &lt;em&gt;is&lt;/em&gt; still technically a network!). Now the only C# that remains to be translated is the code that passes pairs of inputs through the network to see what output it generates for each pair - just to ensure that it matches our expectations. This is how we left it last time:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;const double startAt = 0;
const double endAt = 1;
const double increment = 0.25;
var range = Enumerable.Range(0, (int)((endAt - startAt) / increment) + 1).Select(value =&amp;gt; value * increment);
var xyPairs = range.SelectMany(value =&amp;gt; range, (x, y) =&amp;gt; new[] { x, y });
Console.WriteLine(string.Join(
  Environment.NewLine,
  xyPairs.Select(inputs =&amp;gt; $&quot;{string.Join(&quot;\t&quot;, inputs)}\t{(Output(finalResult.Weights, finalResult.Bias, inputs) ? &quot;Yes&quot; : &quot;No&quot;)}&quot;)
));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The first thing that will be nice about translating this into F# is that it has better support for defining ranges. In C#, we used &quot;Enumerable.Range&quot; but that only works with integers and so we then had to do some division. In F#, we&#39;re able to say &quot;define a range by starting at x and incrementing by y until you get to z&quot;. So we could replace this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;const double startAt = 0;
const double endAt = 1;
const double increment = 0.25;
var range = Enumerable.Range(0, (int)((endAt - startAt) / increment) + 1).Select(value =&amp;gt; value * increment);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. with this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;let range = { float 0 .. float 0.25 .. float 1 }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We could then translate the rest of the C# shown above in a like-for-like fashion into F# &lt;em&gt;or&lt;/em&gt; we could get a tiny bit fancier with some code that I found on Stack Overflow that takes one set of values and transforms it by combining every value with other value (so if your input set was the numbers 1 and 2 then the output would be {1,1} and {1,2} and {2,1} and {2,2}). This is sometimes referred to as taking the &quot;cross product&quot; and is the same concept as doing a &quot;cross join&quot; in SQL. &lt;/p&gt;

&lt;p&gt;The code to do it is as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Inspired by https://stackoverflow.com/a/482922/3813189
let crossproductWithSelf xs = seq { for x1 in xs do for x2 in xs do yield x1, x2 }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Using this means that our &quot;Display network generalisation&quot; summary code looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;let crossproductWithSelf xs = seq { for x1 in xs do for x2 in xs do yield x1, x2 }
let calculatedResults =
    { float 0 .. float 0.25 .. float 1 }
    |&amp;gt; crossproductWithSelf
    |&amp;gt; Seq.map (fun (x, y) -&amp;gt;
        x.ToString() + &quot;,\t&quot; +
        y.ToString() + &quot;,\t&quot; +
        (if (Output finalResult.Weights finalResult.Bias [x; y]) then &quot;Yes&quot; else &quot;No&quot;)
    )
printfn &quot;&quot;
printfn &quot;X,\tY,\tOutput&quot;
printfn &quot;%s&quot; (String.concat Environment.NewLine calculatedResults)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Pretty neat and tidy, I think!&lt;/p&gt;

&lt;h3&gt;Done! What&#39;s next?&lt;/h3&gt;

&lt;p&gt;Phew! Well that felt like quite a lot of work. Getting to grips with a new language can be mentally taxing, particularly when it involves a new paradigm (like making the leap from OOP to functional programming) and I think that that&#39;s why it&#39;s taken me several attempts at getting started with F# to even get this far.&lt;/p&gt;

&lt;p&gt;And although this is a good start, the &quot;machine learning&quot; aspect of the Single Layer Perceptron is very basic and it should be fun to try to dig a little deeper and attempt something more complicated. To that end, I have a few more posts that I&#39;d like to write that will explain how to train a neural network (that has more layers than just the input and output layers) using the Backpropagation Algorithm and then use this to recognise handwritten digits from the famous &lt;a href=&quot;http://yann.lecun.com/exdb/mnist/&quot;&gt;MNIST&lt;/a&gt; image database.&lt;/p&gt;

&lt;p&gt;As with the code here, I will be starting with C# from the &lt;a href=&quot;http://www.robosoup.com/blog&quot;&gt;Robosoup blog&lt;/a&gt; and translating it into a functional style before rewriting it as F#. I think that it&#39;s exciting stuff!&lt;/p&gt;

&lt;p&gt;One more thing - in case you&#39;re curious to see the complete F# code that was scattered through this post, here it is:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;open System

type private Input = { Values: list&amp;lt;float&amp;gt;; Result: bool }

type private CalculationState = {
    Weights: List&amp;lt;float&amp;gt;
    Bias: float
    GlobalError: float
}

let Go (r: Random) =
    let trainingData = [
        { Values = [0.08; 0.94]; Result = true }; { Values = [0.13; 0.95]; Result = true };
        { Values = [0.28; 0.66]; Result = true }; { Values = [0.3; 0.59]; Result = true };
        { Values = [0.31; 0.51]; Result = true }; { Values = [0.34; 0.67]; Result = true };
        { Values = [0.34; 0.63]; Result = true }; { Values = [0.36; 0.55]; Result = true };
        { Values = [0.38; 0.67]; Result = true }; { Values = [0.4; 0.59]; Result = true };
        { Values = [0.4; 0.68]; Result = true }; { Values = [0.41; 0.5]; Result = true };
        { Values = [0.42; 0.53]; Result = true }; { Values = [0.43; 0.65]; Result = true };
        { Values = [0.44; 0.56]; Result = true }; { Values = [0.47; 0.61]; Result = true };
        { Values = [0.47; 0.5]; Result = true }; { Values = [0.48; 0.66]; Result = true };
        { Values = [0.52; 0.53]; Result = true }; { Values = [0.53; 0.58]; Result = true };
        { Values = [0.55; 0.6]; Result = true }; { Values = [0.56; 0.44]; Result = true };
        { Values = [0.58; 0.63]; Result = true }; { Values = [0.62; 0.57]; Result = true };
        { Values = [0.68; 0.42]; Result = true }; { Values = [0.69; 0.21]; Result = true }
        { Values = [0.7; 0.31]; Result = true }; { Values = [0.73; 0.48]; Result = true };
        { Values = [0.74; 0.47]; Result = true }; { Values = [0.74; 0.42]; Result = true };
        { Values = [0.76; 0.34]; Result = true }; { Values = [0.78; 0.5]; Result = true };
        { Values = [0.78; 0.26]; Result = true }; { Values = [0.81; 0.48]; Result = true };
        { Values = [0.83; 0.32]; Result = true }; { Values = [0.83; 0.28]; Result = true };
        { Values = [0.85; 0.07]; Result = true }; { Values = [0.85; 0.45]; Result = true };
        { Values = [0.88; 0.4]; Result = true }; { Values = [0.89; 0.92]; Result = true };
        { Values = [0.9; 0.33]; Result = true }; { Values = [0.91; 0.05]; Result = true };
        { Values = [0.92; 0.44]; Result = true }; { Values = [0.95; 0.94]; Result = true };
        { Values = [0.96; 0.08]; Result = true };

        { Values = [0.02; 0.76]; Result = false }; { Values = [0.06; 0.22]; Result = false };
        { Values = [0.07; 0.16]; Result = false }; { Values = [0.09; 0.43]; Result = false }; 
        { Values = [0.1; 0.08]; Result = false }; { Values = [0.14; 0.07]; Result = false };
        { Values = [0.15; 0.23]; Result = false }; { Values = [0.17; 0.18]; Result = false };
        { Values = [0.17; 0.11]; Result = false }; { Values = [0.21; 0.28]; Result = false };
        { Values = [0.22; 0.17]; Result = false }; { Values = [0.25; 0.09]; Result = false };
        { Values = [0.28; 0.28]; Result = false }; { Values = [0.28; 0.27]; Result = false };
        { Values = [0.29; 0.22]; Result = false }; { Values = [0.29; 0.29]; Result = false };
        { Values = [0.3; 0.29]; Result = false }; { Values = [0.31; 0.14]; Result = false };
        { Values = [0.33; 0.19]; Result = false }; { Values = [0.33; 0.06]; Result = false };
        { Values = [0.39; 0.15]; Result = false }; { Values = [0.52; 0.1]; Result = false };
        { Values = [0.65; 0.07]; Result = false }; { Values = [0.71; 0.1]; Result = false };
        { Values = [0.74; 0.05]; Result = false }
    ]

    let inputLengths =
        trainingData
        |&amp;gt; List.map (fun input -&amp;gt; input.Values.Length)
        |&amp;gt; List.distinct
        |&amp;gt; List.length
    if (inputLengths &amp;gt; 1) then raise (Exception &quot;Inconsistent pattern input lengths!&quot;)

    let learningRate = 0.1

    let Output weights bias inputs =
        let sum = (List.zip weights inputs |&amp;gt; List.map (fun (weight, input) -&amp;gt; weight * input) |&amp;gt; List.sum) + bias
        sum &amp;gt;= float 0

    let UpdateWeights weights localError inputs =
        if (localError = float 0)
            then weights
            else
                List.zip weights inputs
                |&amp;gt; List.map (fun (weight, input) -&amp;gt; weight + (learningRate * localError * input))

    let CalculateNextState state iteration =
        let resultForIteration =
            List.fold
                (fun stateSoFar input -&amp;gt;
                    let output = if (Output stateSoFar.Weights stateSoFar.Bias input.Values) then 1 else -1
                    let localError = float ((if input.Result then 1 else -1) - output)
                    {
                        Weights =
                            if (localError = float 0)
                            then stateSoFar.Weights
                            else UpdateWeights stateSoFar.Weights localError input.Values
                        Bias =
                            if (localError = float 0)
                            then stateSoFar.Bias
                            else stateSoFar.Bias + (learningRate * localError)
                        GlobalError = stateSoFar.GlobalError + Math.Abs(localError)
                    }
                )
                { Weights = state.Weights; Bias = state.Bias; GlobalError = float 0 }
                trainingData
        printfn &quot;Iteration %i\tError %i&quot; iteration (int resultForIteration.GlobalError)
        resultForIteration

    let finalResult =
        Seq.initInfinite (fun i -&amp;gt; i)
        |&amp;gt; Seq.scan
            CalculateNextState
            { Weights = [r.NextDouble(); r.NextDouble()]; Bias = float 0; GlobalError = Double.MaxValue }
        |&amp;gt; Seq.find (fun state -&amp;gt; state.GlobalError = float 0)

    let crossproductWithSelf xs = seq { for x1 in xs do for x2 in xs do yield x1, x2 }
    let calculatedResults =
        { float 0 .. float 0.25 .. float 1 }
        |&amp;gt; crossproductWithSelf
        |&amp;gt; Seq.map (fun (x, y) -&amp;gt;
            x.ToString() + &quot;,\t&quot; +
            y.ToString() + &quot;,\t&quot; +
            (if (Output finalResult.Weights finalResult.Bias [x; y]) then &quot;Yes&quot; else &quot;No&quot;)
        )
    printfn &quot;&quot;
    printfn &quot;X,\tY,\tOutput&quot;
    printfn &quot;%s&quot; (String.concat Environment.NewLine calculatedResults)

Go (new Random(0))
&lt;/code&gt;&lt;/pre&gt;</description>
			<pubDate>Wed, 04 Apr 2018 23:24:00 GMT</pubDate>
		</item>
		<item>
			<title>Learning F# via some Machine Learning: The Single Layer Perceptron</title>
            <link>http://www.productiverage.com/learning-f-sharp-via-some-machine-learning-the-single-layer-percepton</link>
			<guid>http://www.productiverage.com/learning-f-sharp-via-some-machine-learning-the-single-layer-percepton</guid>
			<description>&lt;h3&gt;TL;DR&lt;/h3&gt;

&lt;p&gt;I know C# well and I want to learn F#. I want to wrap my head about some of the underlying algorithms that enable the machine learning that seems so prevalent in the world today (voice recognition, computer vision, sales prediction, semantic analysis, translation). I&#39;m going to try to do both together and prove to myself that I have a good understanding of them both by writing about it.&lt;/p&gt;

&lt;h3&gt;The lure of F#&lt;/h3&gt;

&lt;p&gt;For a few years now, I&#39;ve been wanting to have a proper crack at learning F#. There&#39;s a lot about it that sounds very appealing - immutability-by-default and better representation / handling of null values while still being able to use Visual Studio and use the .NET framework library (as well as other .NET assemblies). I&#39;ve tried a couple of times in the past but without any concrete project to work on, I find that I struggle to motivate myself without a target to work towards that is more tangible than &quot;feel like I&#39;ve learned a bit of a new language&quot;.&lt;/p&gt;

&lt;p&gt;To address this, I&#39;ve decided to combine learning-some-F# with learning-some-machine-learning-basics so that I have a more solid goal. As I go, I thought that I&#39;d write a few posts about the process for two reasons; firstly, being able to explain something clearly is a good indicator that you understand it yourself and, secondly, there is a chance (admittedly slim!) that this might be useful to someone else in a similar position to me, who is familiar with C# and wants to get to grips with F# - I wouldn&#39;t even consider myself intermediately competent yet and so I&#39;m still encountering the pain points of being an F# beginner and seeing how I deal with them might be helpful to others.&lt;/p&gt;

&lt;p&gt;Last year, I wrote &lt;a href=&quot;http://www.productiverage.com/face-or-no-face-finding-faces-in-photos-using-c-sharp-and-accordnet&quot;&gt;Face or no face (finding faces in photos using C# and Accord.NET)&lt;/a&gt;, which classified image regions using a linear support vector machine. This was technically a machine learning solution but it&#39;s only one particular algorithm and there are limitations to the sorts of problem that it can tackle. I want to work up to implementing a Back-Propagation Neural Network that will categorise hand written digits (0-9) but I&#39;m going to start a little simpler.&lt;/p&gt;

&lt;p&gt;While trying to decide how I was going to get started, I read (or scan-read, in many cases, if I&#39;m being honest) what felt like hundreds of articles about neural networks. One of the issues with trying to learn something like this through the research of others is that the people writing about it already have a level of knowledge far above my own on the matter and it feels like there is a lot of knowledge that it assumed that the reader will have. Another issue is that there is often maths involved that can seem sufficiently complicated that it is off-putting. In my posts, I&#39;m going to try to keep things as simple as possible (which may well mean brushing some &quot;whys&quot; under the carpet - leaving it as an exercise to the reader to find out more from other people, once the basics are understood). One series of posts that I &lt;em&gt;did&lt;/em&gt; find very approachable, though, was on a site &quot;&lt;a href=&quot;http://www.robosoup.com/&quot;&gt;Robosoup&lt;/a&gt;&quot; - which is for a consultancy based in London that specialise in machine learning. The first post in the series is &quot;&lt;a href=&quot;http://www.robosoup.com/2008/09/the-single-layer-perceptron-c.html&quot;&gt;The Single Layer Perceptron C#&lt;/a&gt;&quot; and I&#39;m actually going to start with some of the code there and the example data. I&#39;m going to try to explain things my own way but much of the content here will owe a debt to that Robosoup article (I got in touch with John Wakefield at Robosoup and he said that he was happy for me share his code - rest assured that I&#39;m not just stealing it without asking for permission first!).&lt;/p&gt;

&lt;h3&gt;The Single Layer Perceptron&lt;/h3&gt;

&lt;p&gt;The concept of an &quot;artificial neural network&quot; is, essentially, that there is a system of neurons that are connected together and that have a series of inputs that send signals into the system and which eventually get (somehow) transformed into a series of outputs. Each output represents a particular result. If, for example, the neural network was intended to categorise images then the inputs will all be derived from the image data in some way and there may be an output for &quot;dog&quot; and an output for &quot;cat&quot; and these output will end up with a stronger or weaker signal depending upon the input data. The connections between the neurons will have different &quot;weights&quot; and so different input values will somehow result in different outputs. These different weights will have to calculated as part of the network&#39;s &quot;training&quot;. This sort of description is often accompanied by complicated-looking diagrams such as this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.productiverage.com/Content/Images/Posts/NeuralNetwork.jpg&quot; alt=&quot;Neural Network&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(Taken from &lt;a href=&quot;https://www.flickr.com/photos/worldworldworld/7880912598/in/photostream/&quot;&gt;Cesar Harada&#39;s Flickr&lt;/a&gt; under &lt;a href=&quot;https://creativecommons.org/licenses/by-nc-sa/2.0/&quot;&gt;license conditions&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This raises a lot of questions and feels like far too complicated a place to start! (Though, in later posts, I &lt;em&gt;will&lt;/em&gt; be talking about multi-layered multi-output neural networks similar to what is shown above).&lt;/p&gt;

&lt;p&gt;The &quot;Single Layer Perceptron&quot; is simpler - it only has one input &quot;layer&quot; and one output &quot;layer&quot;, where a layer is a list of &quot;neurons&quot;. A neuron is something that takes an input value (a value from -1 to 1), multiplies it by a &quot;weight&quot; (also a value from -1 to 1) and then passes that value onto every node that it is connected to in the layer ahead of it. Pretty much the simplest possible network imaginable that would fit this description would be just two neurons in the input layer and a single neuron in the output layer. Like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.productiverage.com/Content/Images/Posts/SingleLayerPerceptron.png&quot; alt=&quot;Simple Single Layer Perceptron&quot;&gt;&lt;/p&gt;

&lt;p&gt;Now this might almost seem &lt;em&gt;too&lt;/em&gt; simple! Can this really do anything useful? Well, actually, it&#39;s entirely possible to configure a network like this to act as a classifier for any data that is &quot;linearly separable&quot; in as many dimensions as there are inputs. This is already sounding like mumbo jumbo, so I&#39;ll go over those terms..&lt;/p&gt;

&lt;p&gt;A &quot;classifier&quot; will look at its inputs and give a yes/no answer (for example, an &quot;is this a cat?&quot; classifier might look at a photograph and report whether there appears to be a cat in it or not).&lt;/p&gt;

&lt;p&gt;&quot;Linearly separable&quot; is simplest to understand in 2D - if the data is plotted on a graph then it&#39;s &quot;linearly separable&quot; if it&#39;s possible to draw a straight line across the graph that puts all of the values for &quot;yes&quot; lie on one side and all inputs for &quot;no&quot; lie on the other side. When I wrote about linear support vector machines, I talked about a fictional decision history for a  manager, where they would give the go-ahead (or not) to a new feature based upon how much of it could be billed to Clients and what strategic value it had to the company.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.productiverage.com/Content/Images/Posts/ManagerDecisionHistory-Predictions.jpg&quot; alt=&quot;Manager Decision History&quot;&gt;&lt;/p&gt;

&lt;p&gt;This data &lt;em&gt;is&lt;/em&gt; linearly separable because it&#39;s possible to draw a line on the graph where all of the data above gets a &quot;yes&quot; response and all of the data below gets a &quot;no&quot;.&lt;/p&gt;

&lt;p&gt;Some data sets will &lt;em&gt;not&lt;/em&gt; fit this model and so are &lt;em&gt;not&lt;/em&gt; linearly separable. That won&#39;t make it impossible to classify using a neural network but it &lt;em&gt;will&lt;/em&gt; make it impossible for a perceptron to classify (without some form of processing of the data before classification - which is out of the scope of what I want to cover today).&lt;/p&gt;

&lt;p&gt;2D data like this would involve a perceptron with two inputs. 3D data that is linearly separable would have all of its data points seperable by a plane in the 3D space - all points on one side would be &quot;yes&quot; and all points on the other would be &quot;no&quot;; this data would involve a perceptron with three inputs.&lt;/p&gt;

&lt;p&gt;While it&#39;s not as easy to envisage this in more dimensions, the same principle holds. For this sort of multi-dimensional data, the additional dimensions tend to be additional measurable factors that are thought to have affected the outcome (for example, maybe the manager in the example above is predictably less likely to give the go-ahead to features on Monday and Tuesday because they&#39;re always snowed under with emails for those first two days of the week &lt;em&gt;and&lt;/em&gt; they&#39;re less likely to sign off on things when they&#39;re hungry; that would mean that there would be four dimensions to consider, which would be &quot;amount of cost that can be put onto Clients&quot;, &quot;strategic value of the work&quot;, &quot;day of the week&quot; and &quot;time since last ate&quot; - these four dimensions would require a perceptron with four inputs to model the data).&lt;/p&gt;

&lt;h3&gt;Training a perceptron&lt;/h3&gt;

&lt;p&gt;I said above that it&#39;s possible to &lt;em&gt;configure&lt;/em&gt; a network to act as a classifier for linearly separable data. All that is required to configure the network is to assign the weight0 and weight1 values (at least, that is the case for 2D data - since each input has its own weight value then 2D data requires two inputs but if the input is three dimensional then there will be three weight values that must be set and if there were four dimensions then there would be four weight values, etc..). When it is correctly configured, it will be possible to apply any values to the input neurons and to get a single output value. If this output value is higher than a particular threshold then the output will be considered a positive response and otherwise it will be considered a negative response.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.productiverage.com/Content/Images/Posts/SingleLayerPerceptron-ManagerDecision.png&quot; alt=&quot;Simple Single Layer Perceptron for predicting Manager Decisions&quot;&gt;&lt;/p&gt;

&lt;p&gt;Returning to the Manager Decision data, one of the inputs will be for the &quot;amount of cost that can be put onto Clients&quot; while the other will be the &quot;strategic value of the work&quot;. For the data and the code that I&#39;m going to look at further down, all inputs and outputs are in the 0-1 range (this is convenient enough for &quot;amount of cost that can be put onto Clients&quot; but it may be more difficult in the real world to fit all features into a 0-1 range for &quot;strategic value of the work&quot; - but since that data is just a fictional example, we don&#39;t need to worry about that too much).&lt;/p&gt;

&lt;p&gt;The question, then, is &lt;em&gt;how&lt;/em&gt; should we determine the weights for each neuron? This is where the &quot;machine learning&quot; part comes into it. What we&#39;re going to do is &quot;train&quot; a network by using historical data.&lt;/p&gt;

&lt;p&gt;At its essence, the way that a trained network like this is produced is by -&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Setting all of the weights to be random values between 0 and 1&lt;/li&gt;
&lt;li&gt;Passing all of the historical data (aka &quot;training data&quot;) through it and, for each &quot;pattern&quot; (which is the name given to a series of inputs)

&lt;ul&gt;
&lt;li&gt;Calculating the &quot;local error&quot; (the error for that particular pattern)&lt;/li&gt;
&lt;li&gt;Adjusting the weights based upon this local error&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Taking the &quot;total error&quot; or &quot;global error&quot; (the sum of all of the local errors from the training data) and either finding that it is less than a predetermined threshold (in which case the network training is considered complete - hurrah!) or going back to step 2&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;There are a lot of things to consider there - what &lt;em&gt;precisely&lt;/em&gt; are the &quot;local errors&quot;, how are the weights adjusted each iteration and what threshold should we stop at? Let&#39;s work through each of those in order..&lt;/p&gt;

&lt;p&gt;The local error for a particular pattern is how far away the output of the network is from the expected result. Since all of the input data has a yes/no expected output, we&#39;ll translate &quot;yes&quot; into 1 and &quot;no&quot; into 0. For each pattern, we take its inputs and set the input neurons with those values. Then we calculate the output for the network (by multiplying the first input by the first weight and the second input by the second weight and then adding those two values together). Then we compare this output value to the expected 1 or 0 - so, if we get 0.3 as the output value for the first pattern and we expected 1 then that&#39;s an error of 0.7 (since 0.3 is 0.7 away from the expected value of 1). If we get 0.6 for the output value for the second pattern and we expected 0 then that&#39;s an error of 0.6 (since 0.6 is 0.6 away from the expected value of 0).&lt;/p&gt;

&lt;p&gt;In order to adjust the weights after each pattern has been run through the network, a fairly simple equation is used - each new weight is calculated using:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;weight[i] = weight[i] + (learningRate * localError * patternInput[i])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For this network, there are only two inputs and so there will only be two values for &quot;i&quot;.&lt;/p&gt;

&lt;p&gt;The &quot;learning rate&quot; is a value between 0 and 1 that determines how quickly the weights change as the network is trained. Clearly, a value of 0 would mean that the weights &lt;em&gt;don&#39;t&lt;/em&gt; change each iteration, which would be useless. The larger the value of the learning rate, the more that the weights will be adjusted each iteration - however, larger is not always better because the adjustments may swing too far each time and, instead of slowing homing in on a well-trained network, the weights may alternate back and forth and never significantly improve. In the example code that I&#39;m going to look at, I&#39;m using a learning rate of 0.1* but this is a value that you may wish to try playing with when experimenting with training - there seem to be many guidelines when it comes to machine learning and many sensible approaches to classes or problem but there aren&#39;t always hard and fast rules for all of the variables and there are often things to tweak here and there that may affect how quickly you get a result (or if you get one at all).&lt;/p&gt;

&lt;p&gt;* &lt;em&gt;(To be honest, I&#39;ve &quot;decided&quot; to use a learning rate of 0.1 because much of the initial C# code below comes from the &lt;a href=&quot;http://www.robosoup.com/2008/09/the-single-layer-perceptron-c.html&quot;&gt;Robosoup article&lt;/a&gt; that I mentioned earlier and a 0.1 learning rate is used there!)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The acceptable &quot;global error&quot; is another &quot;tunable parameter&quot; in that a higher acceptable threshold should mean that training will complete more quickly but also that the resulting network will be less accurate. On the other hand, it may be impossible to train a network (particularly so simple a network as a single perceptron) to match all of the training data perfectly and so a reasonable threshold must be accepted. In the example code below, a network that perfectly matches the training data &lt;em&gt;is&lt;/em&gt; possible and won&#39;t take long to train and so we&#39;ll aim for a zero global error.&lt;/p&gt;

&lt;p&gt;I&#39;m not going to go into any more detail about how you may set these tunable parameters (learning rate and global error threshold) because there&#39;s a &lt;em&gt;lot&lt;/em&gt; of material to cover and I want to try to stick to practical concepts and code (&lt;em&gt;and&lt;/em&gt; because I&#39;m still not very confident that I&#39;ve got a great system for deciding them!).&lt;/p&gt;

&lt;h3&gt;Input bias&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;http://www.productiverage.com/Content/Images/Posts/SingleLayerPerceptron-ManagerDecision-With-Bias.png&quot; alt=&quot;Simple Single Layer Perceptron for predicting Manager Decisions (with bias node)&quot;&gt;&lt;/p&gt;

&lt;p&gt;Using the training method described above, you will always get a line that cuts through the data at the point (0, 0). This would not work for the &quot;Manager Decision History&quot; graph because there is no way that a line starting at the bottom left of the graph could correctly cut through the data with all of the red points on one side and all of the green points on the other (on that graph all values are 0-1 and so the bottom left is the 0, 0 point).&lt;/p&gt;

&lt;p&gt;A way to address this is to introduce an additional &quot;bias&quot; value. This is effectively like adding an additional neuron whose input value is always one and that has its own weight, just like every other input. Every time that a pattern is passed through the system while it is being trained, when the weights are adjusted, the bias should also be adjusted using the following formula:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bias = bias + (learningRate * localError)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;(The formula is basically the same as the weight-adjusting formula except that the &quot;patternInput[i]&quot; value is removed because the bias neuron&#39;s input value is always 1)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This bias value means that the line that separates the yes/no values no longer has to go through (0, 0) but it has no other effect on training process, other than there being more slightly more work to do (although, without it, we wouldn&#39;t be able to get an answer for many sets of data - so it&#39;s not really &lt;em&gt;more&lt;/em&gt; work at all!).&lt;/p&gt;

&lt;p&gt;I&#39;ve just said that it would not be possible to train a simple network of this form for &lt;em&gt;some&lt;/em&gt; data sets without a bias.. which begs the question &quot;for what data sets &lt;em&gt;should&lt;/em&gt; a bias node be introduced?&quot; - I think that it makes sense to &lt;em&gt;always&lt;/em&gt; include one since, presumably, you don&#39;t know what solution the neural net should produce and so you don&#39;t know whether or not it would strictly be necessary to have a bias. So it&#39;s better to err on the safe side. If the data does &lt;em&gt;not&lt;/em&gt; require a bias then the trained network should end up with a small (ie. close to zero) bias value and it will have little impact.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(This &quot;input bias&quot; is very different to moral biases that can creep into machine learning predictions due to biases, that are often unintentionally included, in the training data - see &quot;&lt;a href=&quot;https://www.technologyreview.com/s/608986/forget-killer-robotsbias-is-the-real-ai-danger/&quot;&gt;Forget Killer Robots—Bias Is the Real AI Danger&lt;/a&gt;&quot;)&lt;/em&gt;&lt;/p&gt;

&lt;h3&gt;From C# to F#..&lt;/h3&gt;

&lt;p&gt;The format that I intend to follow for these posts is roughly as follows:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Talk about the theory (we&#39;ve already done that today!)&lt;/li&gt;
&lt;li&gt;Look at some fairly standard C# code&lt;/li&gt;
&lt;li&gt;Look at making the C# code more functional by removing variable mutations (including loops)&lt;/li&gt;
&lt;li&gt;Rewrite the &quot;functional C#&quot; in F#&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;As an F# beginner, this is the approach that I&#39;ve been using for trying to learn it - until I&#39;ve internalised it further, it still feels like a big ask to take regular C# and rewrite it into idiomatic F# and so the &quot;functional C#&quot; stage helps me a lot. The syntax of F# is not that big of a deal but &lt;em&gt;thinking&lt;/em&gt; in (functional) F# is still something that I&#39;m working towards.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(It&#39;s worth noting that, for me, getting my head around F# and functional programming is the priority. Much of the C# that we&#39;ll be looking will be doing in-place mutations - which, arguably, is a good model for doing the processing that we&#39;ll be looking at when it&#39;s done on a single thread - and since we&#39;ll be moving to using immutable structures then there is a good chance that the performance will be worse in the final F# code. If that turns out to be the case, though, then I&#39;m not going to worry about it. I think that performance concerns are for when you have a better grasp of the technology that you&#39;re working with and I&#39;m not there yet with F# - so I don&#39;t mind if I end up with worse-performing code in the context of this post so long as I&#39;ve learned a lot from writing it!)&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Code slightly modified from that at
// http://www.robosoup.com/2008/09/the-single-layer-perceptron-c.html
public static class Perceptron
{
  public static void Go(Random r)
  {
    // Load sample input patterns and expected outputs
    var trainingData = new[]
    {
      Pattern(0.08, 0.94, true), Pattern(0.13, 0.95, true), Pattern(0.28, 0.66, true),
      Pattern(0.3, 0.59, true), Pattern(0.31, 0.51, true), Pattern(0.34, 0.67, true),
      Pattern(0.34, 0.63, true), Pattern(0.36, 0.55, true), Pattern(0.38, 0.67, true),
      Pattern(0.4, 0.59, true), Pattern(0.4, 0.68, true), Pattern(0.41, 0.5, true),
      Pattern(0.42, 0.53, true),  Pattern(0.43, 0.65, true), Pattern(0.44, 0.56, true),
      Pattern(0.47, 0.61, true), Pattern(0.47, 0.5, true), Pattern(0.48, 0.66, true),
      Pattern(0.52, 0.53, true), Pattern(0.53, 0.58, true), Pattern(0.55, 0.6, true),
      Pattern(0.56, 0.44, true), Pattern(0.58, 0.63, true), Pattern(0.62, 0.57, true),
      Pattern(0.68, 0.42, true), Pattern(0.69, 0.21, true), Pattern(0.7, 0.31, true),
      Pattern(0.73, 0.48, true), Pattern(0.74, 0.47, true), Pattern(0.74, 0.42, true),
      Pattern(0.76, 0.34, true), Pattern(0.78, 0.5, true), Pattern(0.78, 0.26, true),
      Pattern(0.81, 0.48, true), Pattern(0.83, 0.32, true), Pattern(0.83, 0.28, true),
      Pattern(0.85, 0.07, true), Pattern(0.85, 0.45, true), Pattern(0.88, 0.4, true),
      Pattern(0.89, 0.92, true), Pattern(0.9, 0.33, true), Pattern(0.91, 0.05, true),
      Pattern(0.92, 0.44, true), Pattern(0.95, 0.94, true), Pattern(0.96, 0.08, true),

      Pattern(0.02, 0.76, false), Pattern(0.06, 0.22, false), Pattern(0.07, 0.16, false),
      Pattern(0.09, 0.43, false), Pattern(0.1, 0.08, false), Pattern(0.14, 0.07, false),
      Pattern(0.15, 0.23, false), Pattern(0.17, 0.18, false), Pattern(0.17, 0.11, false),
      Pattern(0.21, 0.28, false), Pattern(0.22, 0.17, false), Pattern(0.25, 0.09, false),
      Pattern(0.28, 0.28, false), Pattern(0.28, 0.27, false), Pattern(0.29, 0.22, false),
      Pattern(0.29, 0.29, false), Pattern(0.3, 0.29, false), Pattern(0.31, 0.14, false),
      Pattern(0.33, 0.19, false), Pattern(0.33, 0.06, false), Pattern(0.39, 0.15, false),
      Pattern(0.52, 0.1, false), Pattern(0.65, 0.07, false), Pattern(0.71, 0.1, false),
      Pattern(0.74, 0.05, false)
    };

    // Randomise weights
    var weights = new[] { r.NextDouble(), r.NextDouble() };
    var bias = 0d;

    // Set learning rate
    var learningRate = 0.1;
    var iteration = 0;
    double globalError;
    do
    {
      globalError = 0;
      for (var p = 0; p &amp;lt; trainingData.Length; p++)
      {
        // Calculate output
        var inputs = trainingData[p].Item1;
        var output = Output(weights, bias, inputs[0], inputs[1]) ? 1 : -1;

        // Calculate error
        var expected = trainingData[p].Item2;
        var localError = (expected ? 1 : -1) - output;
        if (localError != 0)
        {
          // Update weights
          for (var i = 0; i &amp;lt; 2; i++)
          {
            weights[i] += learningRate * localError * inputs[i];
          }
          bias += learningRate * localError;
        }

        // Convert error to absolute value
        globalError += Math.Abs(localError);
      }
      Console.WriteLine(&quot;Iteration {0}\tError {1}&quot;, iteration, globalError);
      iteration++;
    } while (globalError != 0);

    Console.WriteLine();
    Console.WriteLine(
      $&quot;Final weights: {weights[0]}, {weights[1]}, Bias: {bias} =&amp;gt; Error: {globalError}&quot;
    );

    // Display network generalisation (note: the &quot;Manager Decision&quot; data has input values that
    // are all in the range 0-1 in both dimensions and so we will only look at values in this
    // range in this preview here)
    Console.WriteLine();
    Console.WriteLine(&quot;X,\tY,\tOutput&quot;);
    for (double x = 0; x &amp;lt;= 1; x += .25)
    {
      for (double y = 0; y &amp;lt;= 1; y += .25)
      {
        var output = Output(weights, bias, x, y);
        Console.WriteLine(&quot;{0},\t{1},\t{2}&quot;, x, y, output ? &quot;Yes&quot; : &quot;No&quot;);
      }
    }
    Console.WriteLine();
  }

  private static bool Output(double[] weights, double bias, double x, double y)
  {
    var sum = (x * weights[0]) + (y * weights[1]) + bias;
    return (sum &amp;gt;= 0);
  }

  /// &amp;lt;summary&amp;gt;Helper for initialising training data&amp;lt;/summary&amp;gt;
  private static Tuple&amp;lt;double[], bool&amp;gt; Pattern(double x, double y, bool output)
  {
    return Tuple.Create(new[] { x, y }, output);
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This code is fairly straightforward and it goes through the steps that I described before:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Set weights to be random values and the bias to be zero&lt;/li&gt;
&lt;li&gt;Take each training data entry&#39;s input and calculate the output using the current weights (and bias), adjusting the weights (and bias) if the calculated output did not match the expected output&lt;/li&gt;
&lt;li&gt;Compare the total error against a threshold (of zero) and go back to step 2 if it&#39;s too high&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The way that I&#39;m going to change this code from &quot;regular&quot; (I would call it &quot;object oriented&quot; C# but the code shown here is probably closer to being &quot;procedural&quot;) to &quot;functional*&quot; C# is by looking for things that would seem out of place in functional code and replacing them.&lt;/p&gt;

&lt;p&gt;* &lt;em&gt;(&quot;functional&quot; is often interpreted as meaning that you avoid side effects and avoid mutation - we can argue about that definition another day if you like but it&#39;s a good enough place to start for now!)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Immediately, the following things jump out at me:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Variables whose values are explicitly changed during processing (eg. &quot;iteration&quot; and &quot;globalError&quot;)&lt;/li&gt;
&lt;li&gt;Variables whose values change as part of looping constructs (eg. &quot;i&quot;, &quot;x&quot; and &quot;y&quot;)&lt;/li&gt;
&lt;li&gt;The do..while loop will not be useful if values are not to be mutated with it and so that will need to be replaced with something else&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I suppose the question, then, is &lt;em&gt;how&lt;/em&gt; can we possibly write code like this &lt;em&gt;without&lt;/em&gt; changing / mutating / updating values?&lt;/p&gt;

&lt;p&gt;The first thing to recognise is that LINQ made a more functional style of processing much more mainstream within C# and seem less alien. Before LINQ, if you had an array of values and you wanted an array containing the squares of these values (contrived example, I know, but bear with me) then you may well have achieved this in a fairly procedural manner - eg.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var values = new[] { 1, 2, 3 };
var squaredValues = new int[values.Length];
for (var i = 0; i &amp;lt; values.Length; i++)
  squaredValues[i] = values[i] * values[i];
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Each time the loop is executed, the value of &quot;i&quot; changes and the &quot;squareValues&quot; array is updated.&lt;/p&gt;

&lt;p&gt;Until the for loop has been fully executed, the &quot;squaredValues&quot; array is only partially initialised.&lt;/p&gt;

&lt;p&gt;Within the loop, it&#39;s technically possible to change the value of &quot;i&quot; and move it backwards or forwards (such as by throwing in a bonus &quot;i++&quot; to keep future code maintainers on their toes) and this can be the cause of potential coding errors in loops more complicated than the one shown here.&lt;/p&gt;

&lt;p&gt;Since all we want to do is transform every single value in one array and create a new array from the results, it would be nice if we could be more descriptive in what we are trying to do and to remove some &quot;book keeping&quot; (such as tracking the &quot;i&quot; value using the for loop). This is what would happen if LINQ was used to perform the same work -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var values = new[] { 1, 2, 3 };
var squaredValues = values
  .Select(value =&amp;gt; value * value)
  .ToArray();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that there is no mutation occurring here. Each time that the lambda that is passed to the &quot;Select&quot; method is called, a new &quot;value&quot; reference is created (unlike &quot;i&quot;, which was a single variable shared across each iteration of the loop).&lt;/p&gt;

&lt;p&gt;This is one technique that will be useful to remove mutation from code.&lt;/p&gt;

&lt;p&gt;Another is the &quot;Aggregate&quot; method for enumerating a list of items and reducing it to a single reference. To try to illustrate; if I had a collection of words and I wanted to get the &lt;em&gt;total number of words&lt;/em&gt; and the &lt;em&gt;total number of letters&lt;/em&gt; then I might write procedural code like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;static void ShowLetterAndWordCount(IEnumerable&amp;lt;string&amp;gt; words)
{
  var numberOfLetters = 0;
  var numberOfWords = 0;
  foreach (var word in words)
  {
    numberOfLetters += word.Length;
    numberOfWords++;
  }
  Console.WriteLine(&quot;Total number of letters: &quot; + numberOfLetters);
  Console.WriteLine(&quot;Total number of words: &quot; + numberOfWords);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. or I could achieve the same thing without any mutating variables by using the following code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;static void ShowLetterAndWordCount(IEnumerable&amp;lt;string&amp;gt; words)
{
  var summary = words.Aggregate(
    seed: new { NumberOfLetters = 0, NumberOfWords = 0 },
    func: (valueSoFar, nextWord) =&amp;gt; new
    {
      NumberOfLetters = valueSoFar.NumberOfLetters + nextWord.Length,
      NumberOfWords = valueSoFar.NumberOfWords + 1
    }
  );
  Console.WriteLine(&quot;Total number of letters: &quot; + summary.NumberOfLetters);
  Console.WriteLine(&quot;Total number of words: &quot; + summary.NumberOfWords);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;What &quot;Aggregate&quot; does is it takes a &quot;seed&quot; value and the first value of the list of items and combines them using the &quot;func&quot; lambda. It then takes this result and combines it with the &lt;em&gt;second&lt;/em&gt; value of the list, also using the &quot;func&quot; lambda. It will then take &lt;em&gt;this&lt;/em&gt; result and combines it with the &lt;em&gt;third&lt;/em&gt; value of the list, etc.. until one final combined value is returned. In the code above, I&#39;ve used an anonymous type for the seed (and so the final &quot;summary&quot; reference will also be an instance of that anonymous type and so have &quot;NumberOfLetters&quot; and &quot;NumberOfWords&quot; properties) but the seed can be a class or a primitive or any type that you need.&lt;/p&gt;

&lt;p&gt;All of the &quot;book keeping&quot; required by the Aggregate method is handled by the method itself - there is no loop variable to worry about and there are no variables outside of the loop (such as &quot;numberOfLetters&quot; and &quot;numberOfWords&quot;) that must be tracked. You need only to tell it what the initial &quot;seed&quot; value should be and how it should combine the &quot;value so far&quot; with a single item from the input list.&lt;/p&gt;

&lt;p&gt;This is the advantage that it has over the procedural version (which may initially appear &quot;less complicated&quot;) - you only need to consider what actually happens within a single operation and you don&#39;t have to look after any variables that must be maintained across the entire loop (which was the case with &quot;numberOfLetters&quot; and &quot;numberOfWords&quot; in the first version).&lt;/p&gt;

&lt;p&gt;At its core, this means that the scope of variables is reduced and when they don&#39;t change (ie. they are immutable) there are less moving parts for you to mentally consider when trying to reason about any particular line of code.&lt;/p&gt;

&lt;p&gt;I&#39;m finding that the F# version of Aggregate (called &quot;fold&quot;) is a very powerful and useful technique and so having a good grasp on how it works is very useful. Just to make it &lt;em&gt;extra&lt;/em&gt; clear (apologies if this is belabouring the point but Aggregate doesn&#39;t, in my experience, tend to be commonly used in C# and so it may not be familiar to some), here&#39;s another example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var values = new[] { 1, 2, 3, 4, 5 };
var sumOfValues = words.Aggregate(
  seed: 0,
  func: (valueSoFar, value) =&amp;gt; valueSoFar + value
);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will return 15 because it will just add all of the values together. It begins with a seed value of 0 and adds it to the first value (which is 1) to get 1. It then adds this &quot;value so far&quot; to the second value (which is 2) to get 3. It adds this to the third value (which is 3) to get 6 and adds this to the fourth value (which is 4) to get 10 and adds this to the fifth value (which is 5) to get 15.&lt;/p&gt;

&lt;p&gt;Not a particularly useful piece of code - and one that could have been written more clearly as:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var values = new[] { 1, 2, 3, 4, 5 };
var sumOfValues = words.Sum();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. but hopefully it reinforces how the Aggregate method operates on data. And hopefully it makes it clear how powerful Aggregate can be because so many other operations may be built on top of it, such as Min or Max -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;static int? Min(IEnumerable&amp;lt;int&amp;gt; values)
{
  return values.Aggregate(
    seed: (int?)null,
    func: (valueSoFar, nextValue) =&amp;gt; (valueSoFar.HasValue &amp;amp;&amp;amp; valueSoFar &amp;lt; nextValue)
      ? valueSoFar
      : nextValue
  );
}

static int? Max(IEnumerable&amp;lt;int&amp;gt; values)
{
  return values.Aggregate(
    seed: (int?)null,
    func: (valueSoFar, nextValue) =&amp;gt; (valueSoFar.HasValue &amp;amp;&amp;amp; valueSoFar &amp;gt; nextValue)
      ? valueSoFar
      : nextValue
  );
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;To functional code.. one step at a time&lt;/h3&gt;

&lt;p&gt;Back to the Single Layer Perceptron code.. The way that I&#39;m approaching this is to take one logical section of code and replace the procedural style of code with functional constructs.&lt;/p&gt;

&lt;p&gt;The first that I&#39;ll tackle is the do..while loop and the mutation of the outer &quot;iteration&quot;, &quot;weights&quot;, &quot;bias&quot; and &quot;globalError&quot; variables.&lt;/p&gt;

&lt;p&gt;This will be straightforward if we use the Aggregate method where the &quot;value so far&quot; contains a &quot;Weights&quot; array, a &quot;Bias&quot; value and a &quot;GlobalError&quot; value that will be re-calculated each iteration.&lt;/p&gt;

&lt;p&gt;The input list passed to Aggregate will be an incrementing list of integers representing the current iteration number. The &quot;func&quot; lambda will take the previous Weights / Bias / GlobalError state and calculate the &lt;em&gt;next&lt;/em&gt; Weight / Bias / GlobalError state. If the &quot;previousState&quot; already has a low enough GlobalError then the &quot;func&quot; lambda won&#39;t have to do any more calculating and can just return the previousState reference immediately (meaning that we don&#39;t have to do any more work and we can just let Aggregate finish as many iterations as it is configured to do so until the Aggregate call completes - if that sounds a bit unclear then hopefully it will make more sense after you see the code and I talk more about it below).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;const double learningRate = 0.1;
const int maxNumberOfIterationsToPerform = 100; // See notes below code

var finalResult = Enumerable.Range(0, maxNumberOfIterationsToPerform)
  .Aggregate(
    seed: new
    {
      Weights = new[] { r.NextDouble(), r.NextDouble() },
      Bias = 0d,
      GlobalError = double.MaxValue
    },
    func: (previousState, iteration) =&amp;gt;
    {
      // The network is already trained - no more calculations required
      if (previousState.GlobalError == 0)
        return previousState;

      var weights = previousState.Weights;
      var bias = previousState.Bias;
      var globalError = 0d;
      for (var p = 0; p &amp;lt; trainingData.Length; p++)
      {
        // Calculate output
        var inputs = trainingData[p].Item1;
        var output = Output(weights, bias, inputs[0], inputs[1]) ? 1 : -1;

        // Calculate error
        var expected = trainingData[p].Item2;
        var localError = (expected ? 1 : -1) - output;
        if (localError != 0)
        {
          // Update weights (taking a copy of the weights array rather than altering its values)
          weights = weights.ToArray();
          for (var i = 0; i &amp;lt; 2; i++)
          {
            weights[i] += learningRate * localError * inputs[i];
          }
          bias += learningRate * localError;
        }

        // Convert error to absolute value
        globalError += Math.Abs(localError);
      }
      Console.WriteLine(&quot;Iteration {0}\tError {1}&quot;, iteration, globalError);
      return new { Weights = weights, Bias = bias, GlobalError = globalError };
    }
  );
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(You may notice that I also changing &quot;learningRate&quot; from being a variable to be a const - since this will never change, it makes sense).&lt;/p&gt;

&lt;p&gt;I&#39;ve had to make a compromise in how I&#39;ve written this code - I&#39;ve had to specify a &quot;maxNumberOfIterationsToPerform&quot; value because the Aggregate method has no way to say &quot;stop processing now, we have an answer that we&#39;re happy with&quot;. This is why there is the check at the top of the &quot;func&quot; lambda that says &quot;if previousState&#39;s GlobalError is low enough then do no more calculation&quot; - the Aggregate method will keep running through &lt;em&gt;every single value&lt;/em&gt; in the input list. But how do we know that 100 iterations will be enough to get a zero Global Error? We don&#39;t!&lt;/p&gt;

&lt;p&gt;What would be really helpful would be if we could have a variation of Aggregate that returns an &lt;strong&gt;IEnumerable&lt;/strong&gt; of all of the intermediate calculation states (all of the &quot;previousState&quot; values) so that we could stop enumerating as soon as one of them has a GlobalError of zero - that way we wouldn&#39;t have to limit ourselves to a low maxNumberOfIterationsToPerform value. Something that would let us write code like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;const double learningRate = 0.1;

var finalResult = Enumerable.Range(0, int.MaxValue)
  .AggregateAndReturnIntermediateStates(
    seed: new
    {
      // Same as in earlier code sample..
    },
    func: (previousState, iteration) =&amp;gt;
    {
      // Same as in earlier code sample but without the need to check GlobalError..
    }
  )
  .First(state =&amp;gt; state.GlobalError == 0);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I searched through the LINQ and the F# library documentation and I couldn&#39;t find anything in LINQ that I could use to do this but I &lt;em&gt;did&lt;/em&gt; find something in F# called &quot;scan&quot;. To implement it as a LINQ-esque C# extension method, though, is simple. If we start by considering what an implementation of Aggregate would look like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public static TAccumulate Aggregate&amp;lt;TSource, TAccumulate&amp;gt;(
  this IEnumerable&amp;lt;TSource&amp;gt; source,
  TAccumulate seed,
  Func&amp;lt;TAccumulate, TSource, TAccumulate&amp;gt; func)
{
  var valueSoFar = seed;
  foreach (var value in source)
    valueSoFar = func(valueSoFar, value);
  return valueSoFar;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. we need only to change the return type from &lt;strong&gt;TAccumulate&lt;/strong&gt; to &lt;strong&gt;IEnumerable&amp;lt;TAccumulate&amp;gt;&lt;/strong&gt; and to throw in some &quot;yield return&quot; magic to produce &quot;Scan&quot;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public static IEnumerable&amp;lt;TAccumulate&amp;gt; Scan&amp;lt;TSource, TAccumulate&amp;gt;(
  this IEnumerable&amp;lt;TSource&amp;gt; source,
  TAccumulate seed,
  Func&amp;lt;TAccumulate, TSource, TAccumulate&amp;gt; func)
{
  yield return seed;

  var valueSoFar = seed;
  foreach (var value in source)
  {
    valueSoFar = func(valueSoFar, value);
    yield return valueSoFar;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This means that I can now write:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;const double learningRate = 0.1;

var finalResult = Enumerable.Range(0, int.MaxValue)
  .Scan(
    seed: new
    {
      // Same as in earlier code sample..
    },
    func: (previousState, iteration) =&amp;gt;
    {
      // Same as in earlier code sample (but still without the need to check GlobalError)..
    }
  )
  .First(state =&amp;gt; state.GlobalError == 0);    
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Hurrah! That&#39;s a good step forward! &lt;/p&gt;

&lt;p&gt;Now I need to tackle the inner section:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var weights = previousState.Weights;
var bias = previousState.Bias;
var globalError = 0d;
for (var p = 0; p &amp;lt; trainingData.Length; p++)
{
  // Calculate output
  var inputs = trainingData[p].Item1;
  var output = Output(weights, bias, inputs[0], inputs[1]) ? 1 : -1;

  // Calculate error
  var expected = trainingData[p].Item2;
  var localError = (expected ? 1 : -1) - output;
  if (localError != 0)
  {
    // Update weights (taking a copy of the weights array rather than altering its values)
    weights = weights.ToArray();
    for (var i = 0; i &amp;lt; 2; i++)
    {
      weights[i] += learningRate * localError * inputs[i];
    }
    bias += learningRate * localError;
  }

  // Convert error to absolute value
  globalError += Math.Abs(localError);
}

Console.WriteLine(&quot;Iteration {0}\tError {1}&quot;, iteration, globalError);
return new { Weights = weights, Bias = bias, GlobalError = globalError };
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I&#39;m going to start from the inside and work outward this time. The first thing that I want to get rid of is the loop that is used to update weights. What this loop is effectively doing is walking through two arrays (&quot;weights&quot; and &quot;inputs&quot;) and performing an operation on a single pair of items from each (each loop iteration, we do something with one weight value and one input value).&lt;/p&gt;

&lt;p&gt;This is just what the &quot;&lt;a href=&quot;https://msdn.microsoft.com/en-us/library/dd267698(v=vs.110).aspx&quot;&gt;Zip&lt;/a&gt;&quot; LINQ function does and so we can use that here. We&#39;ll replace:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Update weights (taking a copy of the weights array rather than altering its values)
weights = weights.ToArray();
for (var i = 0; i &amp;lt; 2; i++)
{
  weights[i] += learningRate * localError * inputs[i];
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. with this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;weights
  .Zip(inputs, (weight, input) =&amp;gt; weight + (learningRate * localError * input))
  .ToArray();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To maker the &quot;inner section&quot; simpler, I&#39;m going to hide that logic into a function:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;private static double[] UpdateWeights(double[] weights, double learningRate, double localError, double[] inputs)
{
  if (localError == 0)
    return weights;

  return weights
    .Zip(inputs, (weight, input) =&amp;gt; weight + (learningRate * localError * input))
    .ToArray();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I&#39;ve also pulled the &quot;is localError zero&quot; check into the method. It feels a little unnecessary when there are only two weights and two inputs but this new version of the weight-updating code may be called with any number of inputs and so it may make sense to avoid looping through them all when the localError is zero (because we won&#39;t be making any changes to the weights in that case).&lt;/p&gt;

&lt;p&gt;The next thing to do is to get rid of the other for-loop and the values that it mutates on each iteration. This part:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var weights = previousState.Weights;
var bias = previousState.Bias;
var globalError = 0d;
for (var p = 0; p &amp;lt; trainingData.Length; p++)
{
  // Apply current pattern and alter weights, bias and globalError accordingly..
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If we group the &quot;weights / bias / globalError&quot; values into a single value then we can replace this with an Aggregate call, like we saw earlier:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var resultForIteration = trainingData.Aggregate(
  seed: new { Weights = previousState.Weights, Bias = previousState.Bias, GlobalError = 0d },
  func: (stateSoFar, pattern) =&amp;gt;
  {
    // Apply current pattern and calculate new weights, bias and globalError values..

    // .. and return new object wrapping these values
    return new { Weights = newWeights, Bias = newBias, GlobalError = newGlobalError },
  }
);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Before I pull it all together, I want to make a small change to the &quot;Output&quot; function - the current version only works if there are precisely two inputs and two weights but the &quot;UpdateWeights&quot; function from a moment ago works with any number of inputs and so I think that &quot;Output&quot; should too. So we&#39;ll replace this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;private static bool Output(double[] weights, double bias, double x, double y)
{
  var sum = (x * weights[0]) + (y * weights[1]) + bias;
  return (sum &amp;gt;= 0);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. with this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;private static bool Output(double[] weights, double bias, double[] inputs)
{
  var sum = inputs.Zip(weights, (input, weight) =&amp;gt; input * weight).Sum() + bias;
  return (sum &amp;gt;= 0);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;(Note that using &quot;Zip&quot; again means that we don&#39;t have to resort to any for loops)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Combining all of this, the network-training code becomes the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;const double learningRate = 0.1;

var finalResult = Enumerable.Range(0, int.MaxValue)
  .Scan(
    seed: new
    {
      Weights = new[] { r.NextDouble(), r.NextDouble() },
      Bias = 0d,
      GlobalError = double.MaxValue
    },
    func: (previousState, iteration) =&amp;gt;
    {
      var resultForIteration = trainingData.Aggregate(
        seed: new { Weights = previousState.Weights, Bias = previousState.Bias, GlobalError = 0d },
        func: (stateSoFar, pattern) =&amp;gt;
        {
          var output = Output(stateSoFar.Weights, stateSoFar.Bias, pattern.Item1) ? 1 : -1;
          var localError = (pattern.Item2 ? 1 : -1) - output;
          return new
          {
            Weights = UpdateWeights(stateSoFar.Weights, learningRate, localError, pattern.Item1),
            Bias = stateSoFar.Bias + (learningRate * localError),
            GlobalError = stateSoFar.GlobalError + Math.Abs(localError)
          };
        }
      );
      Console.WriteLine(&quot;Iteration {0}\tError {1}&quot;, iteration, resultForIteration.GlobalError);
      return resultForIteration;
    }
  )
  .First(state =&amp;gt; state.GlobalError &amp;lt;= 0);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The final piece of the puzzle is to change the &quot;Display network generalisation&quot; code to remove the for loops from there too -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for (double x = 0; x &amp;lt;= 1; x += .25)
{
  for (double y = 0; y &amp;lt;= 1; y += .25)
  {
    var output = Output(weights, bias, new[] { x, y });
    Console.WriteLine(&quot;{0},\t{1},\t{2}&quot;, x, y, output ? &quot;Yes&quot; : &quot;No&quot;);
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The natural thing would seem to be to replace those loops with Enumerable.Range calls.. however, &quot;Range&quot; only works &lt;strong&gt;int&lt;/strong&gt; values and we need to use &lt;strong&gt;double&lt;/strong&gt; in order to increment by 0.25 each time. We could write a new &quot;Range&quot; extension method that would take &lt;strong&gt;double&lt;/strong&gt; values or we could just workaround the limitation. If we want the values 0, 0.25, 0.5, 0.75, 1 then that&#39;s five distinct values. The number of items may be calculated by taking the end value, subtracting the start value, dividing by the increment and then adding one (to ensure that we get the start value &lt;em&gt;and&lt;/em&gt; the end value).&lt;/p&gt;

&lt;p&gt;In this case, that would be ((1 - 0) / 0.25) + 1 = 4 + 1 = 5.&lt;/p&gt;

&lt;p&gt;We can do that in code like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;const double startAt = 0;
const double endAt = 1;
const double increment = 0.25;
var range = Enumerable.Range(0, (int)((endAt - startAt) / increment) + 1)
  .Select(value =&amp;gt; value * increment);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We then want to &quot;cross join&quot; range with itself so that we loop through every (x, y) combination. We can do that with creative use of &quot;SelectMany&quot; -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var xyPairs = range.SelectMany(value =&amp;gt; range, (x, y) =&amp;gt; new[] { x, y });
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And now that nested for-loop may be replaced by this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;const double startAt = 0;
const double endAt = 1;
const double increment = 0.25;
var range = Enumerable.Range(0, (int)((endAt - startAt) / increment) + 1)
  .Select(value =&amp;gt; value * increment);
var xyPairs = range.SelectMany(value =&amp;gt; range, (x, y) =&amp;gt; new[] { x, y });
Console.WriteLine(string.Join(
  Environment.NewLine,
  xyPairs.Select(inputs =&amp;gt; $&quot;{string.Join(&quot;\t&quot;, inputs)}\t{(Output(finalResult.Weights, finalResult.Bias, inputs) ? &quot;Yes&quot; : &quot;No&quot;)}&quot;)
));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That&#39;s the final piece of the convert-to-functional-code puzzle. Now we just need to translate it into F#!&lt;/p&gt;

&lt;h3&gt;Sidebar: &quot;Function&quot; vs &quot;Method&quot;&lt;/h3&gt;

&lt;p&gt;I find that in languages that are thought to be object oriented, the words &quot;function&quot; and &quot;method&quot; are commonly used interchangeably. Since beginning to become interested in so-called &quot;functional programming&quot;, I&#39;ve tried to find out whether there is a definitive or accepted difference between the two (after all, it&#39;s called &lt;em&gt;functional&lt;/em&gt; programming rather than &lt;em&gt;methodical&lt;/em&gt; programming, so surely &lt;em&gt;someone&lt;/em&gt; thought that there was a difference!).&lt;/p&gt;

&lt;p&gt;A few times, I&#39;ve heard that the difference is that a &quot;function&quot; should not have any side effects and so should always return the same value given the same inputs. On the other hand, a &quot;method&quot; &lt;em&gt;may&lt;/em&gt; cause side effects or rely upon ambient references - if the code writes to disk or reads &lt;strong&gt;DateTime.Now&lt;/strong&gt; then it&#39;s not &quot;pure&quot; (where &quot;pure&quot; means that it relies only upon its arguments and does not produce any side effects - it &lt;em&gt;only&lt;/em&gt; produces a return value and does not manipulate anything else) and so should be described as being part of a &lt;em&gt;method&lt;/em&gt; rather than part of a &lt;em&gt;function&lt;/em&gt;. Most recently I&#39;ve seen it described in &lt;a href=&quot;https://softwareengineering.stackexchange.com/a/222378/204464&quot;&gt;this Software Engineering Stack Exchange answer&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I try to use the word &quot;function&quot; only when it is known to be a pure function and a &quot;method&quot; otherwise (when it either &lt;em&gt;definitely&lt;/em&gt; causes / relies upon side effects or if it&#39;s not clear). I still get it wrong from time to time (for example, I&#39;ve been referring to LINQ &quot;methods&quot; in this post and we can probably presume that they are pure functions in most cases) but I&#39;m still in the process of trying to internalise this terminology while I&#39;m trying to internalise writing a more &quot;functional&quot; style of code for writing F#.&lt;/p&gt;

&lt;h3&gt;Writing F# code&lt;/h3&gt;

&lt;p&gt;If you&#39;ve read this far then you may be detecting an unexpectedly abrupt end to the post judging by your browser&#39;s scrollbar!&lt;/p&gt;

&lt;p&gt;Originally, I had intended to include all of the above content &lt;em&gt;and&lt;/em&gt; go into how precisely to translate the functional C# code into F# but it quickly became clear that the post would be insanely large (I&#39;ve written my fair share of monster posts in the past and I think that the time has come to put an end to them - this one&#39;s already pretty hefty).&lt;/p&gt;

&lt;p&gt;Cliffhanger!&lt;/p&gt;

&lt;p&gt;Sorry.&lt;/p&gt;

&lt;p&gt;My next post will jump straight into F#. I will assume zero prior knowledge of the language itself but I also want to proceed at a decent rate. Hopefully this will mean that you won&#39;t get bored if you already have a little exposure to F# (or maybe it will be the worst of both worlds and be too slow for F# novices but too fast for those who&#39;ve never seen it before). Let&#39;s wait and see*!&lt;/p&gt;

&lt;p&gt;* &lt;em&gt;(Should you be desperately excited and dying for part two, rest assured that it&#39;s already written and just needs a thorough proof-read - so it should be published early next week at the latest)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I&#39;m not sure how many posts there will be in the series in total but the Single Layer Perceptron is just the first model that I want to cover before moving onto the Back Propagation Neural Network model and then onto the Multi-Output variation (which will be necessary in order to classify hand written digits from 0-9 as opposed to being a simple yes/no classifier). Although I said that performance is not my primary concern for this playing-with-F# process, there are a couple of interesting things that I&#39;d like to talk about on that front. So there should be a lot to come over the next few months!&lt;/p&gt;&lt;div class=&quot;Related&quot;&gt;&lt;h3&gt;You may also be interested in&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;http://www.productiverage.com/face-or-no-face-finding-faces-in-photos-using-c-sharp-and-accordnet&quot;&gt;Face or no face (finding faces in photos using C# and Accord.NET)&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;</description>
			<pubDate>Tue, 27 Mar 2018 22:18:00 GMT</pubDate>
		</item>
		<item>
			<title>Trying to set a readonly auto-property value externally (plus, a little BenchmarkDotNet)</title>
            <link>http://www.productiverage.com/trying-to-set-a-readonly-autoproperty-value-externally-plus-a-little-benchmarkdotnet</link>
			<guid>http://www.productiverage.com/trying-to-set-a-readonly-autoproperty-value-externally-plus-a-little-benchmarkdotnet</guid>
			<description>&lt;p&gt;If you&#39;re thinking that you should try changing a readonly property.. well, in short, you almost certainly shouldn&#39;t try.&lt;/p&gt;

&lt;p&gt;For example, the following class has a property that should &lt;em&gt;only&lt;/em&gt; be set in its constructor and then never mutated again -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public sealed class Example
{
  public Example(int id)
  {
    Id = id;
  }
  public int Id { get; }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;And it is a good thing&lt;/em&gt; that we are able to write code so easily that communicates precisely when a property may (and may not) change.&lt;/p&gt;

&lt;p&gt;However..&lt;/p&gt;

&lt;p&gt;You might have some very particular scenario in mind where you really &lt;em&gt;do&lt;/em&gt; want to try to write to a readonly auto-property&#39;s value for an instance that has already been created. It&#39;s possible that you are writing some interesting deserialisation code, I suppose. For something that I was looking at, I was curious to look into how feasible it is (or isn&#39;t) and I came up with the following three basic approaches.&lt;/p&gt;

&lt;p&gt;I think that each approach demonstrates something a little off the beaten track of .NET - granted, there&#39;s absolutely nothing here that&#39;s never been done before.. but sometimes it&#39;s fun to be reminded of how flexible .NET &lt;em&gt;can&lt;/em&gt; be, if only to appreciate how hard it works to keep everything reliable and consistent.&lt;/p&gt;

&lt;h3&gt;TL;DR&lt;/h3&gt;

&lt;p&gt;I&#39;ll show three approaches, in decreasing order of ease of writing. They all depend upon a particular naming conventions in .NET&#39;s internals that is not documented and should not be considered reliable (ie. a future version of C# and/or the compiler could break it). Even if you ignore this potential time bomb, only the first of the three methods will actually work. Like I said at the start, this is something that you almost certainly shouldn&#39;t be attempting anyway!&lt;/p&gt;

&lt;h3&gt;Approach 1: Reflection (with some guesswork)&lt;/h3&gt;

&lt;p&gt;C# 6 introduced read-only auto-properties. Before thoses were available, you had two options to do something similar. You could use a private setter -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public sealed class Example
{
  public Example(int id)
  {
    Id = id;
  }
  public int Id { get; private set; }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. or you could manually create a private readonly backing field for the property -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public sealed class Example
{
  private readonly int _id;
  public Example(int id)
  {
    _id = id;
  }
  public int Id { get { return _id; } }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The first approach requires less code but the guarantees that it claims to make are less strict. When a field is readonly then it may &lt;em&gt;only&lt;/em&gt; be set within a constructor but when it has a private setter then it could feasibly change &lt;em&gt;at any point&lt;/em&gt; in the lifetime of the instance. In the class above, it&#39;s clear to see that it &lt;em&gt;is&lt;/em&gt; only set in the constructor but there are no compiler assurances that someone won&#39;t come along and add a method to the &lt;strong&gt;Example&lt;/strong&gt; class that mutates the private-setter &quot;Id&quot; property. If you have a readonly &quot;_id&quot; backing field then it would not be possible to write a method to mutate the value*.&lt;/p&gt;

&lt;p&gt;* &lt;em&gt;(Without resorting to the sort of shenanigans that we are going to look at here)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;So the second class is more reliable and more accurately conveys the author&#39;s intentions for the code (that the &quot;Id&quot; property of an &lt;strong&gt;Example&lt;/strong&gt; instance will never change during its lifetime). The disadvantage is that there is more code to write.&lt;/p&gt;

&lt;p&gt;The C# 6 syntax is the best of both worlds - as short (shorter, in fact, since there is &lt;em&gt;no&lt;/em&gt; setter defined) as the first version but with the stronger guarantees of the second version.&lt;/p&gt;

&lt;p&gt;Interestingly, the compiler generates IL that is essentially identical to that which result from the C# 5 syntax where you manually define a property that backs onto a readonly field. The only real difference relates to the fact that it wants to be sure that it can inject a readonly backing field whose name won&#39;t clash with any other field that the human code writer may have added to the class. To do this, it uses characters in the generated field names that are not valid to appear in C#, such as &quot;&amp;lt;Id&amp;gt;k__BackingField&quot;. The triangle brackets may not be used in C# code but they may be used in the IL code that the compiler generates. And, just to make things extra clear, it adds a &lt;strong&gt;[CompilerGenerated]&lt;/strong&gt; attribute to the backing field.&lt;/p&gt;

&lt;p&gt;This is sufficient information for us to try to identify the compiler-generated backing field using reflection. Going back to this version of the class:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public sealed class Example
{
  public Example(int id)
  {
    Id = id;
  }
  public int Id { get; }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. we can identify the backing field for the &quot;Id&quot; property with the following code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var type = typeof(Example);
var property = type.GetProperty(&quot;Id&quot;);

var backingField = type
  .GetFields(BindingFlags.NonPublic | BindingFlags.Instance | BindingFlags.Static)
  .FirstOrDefault(field =&amp;gt;
    field.Attributes.HasFlag(FieldAttributes.Private) &amp;amp;&amp;amp;
    field.Attributes.HasFlag(FieldAttributes.InitOnly) &amp;amp;&amp;amp;
    field.CustomAttributes.Any(attr =&amp;gt; attr.AttributeType == typeof(CompilerGeneratedAttribute)) &amp;amp;&amp;amp;
    (field.DeclaringType == property.DeclaringType) &amp;amp;&amp;amp;
    field.FieldType.IsAssignableFrom(property.PropertyType) &amp;amp;&amp;amp;
    field.Name.StartsWith(&quot;&amp;lt;&quot; + property.Name + &quot;&amp;gt;&quot;)
  );    
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With this backingField reference, we can start doing devious things. Like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Create an instance with a readonly auto-property
var x = new Example(123);
Console.WriteLine(x.Id); // Prints &quot;123&quot;

// Now change the value of that readonly auto-property!
backingField.SetValue(x, 456);
Console.WriteLine(x.Id); // Prints &quot;456&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We took an instance of a class that has a readonly property (meaning that it should never change after the instance has been constructed) and we &lt;em&gt;changed&lt;/em&gt; that property. Evil.&lt;/p&gt;

&lt;p&gt;One more time, though: this relies upon the current convention that the compiler-generated backing fields follow a particular naming convention. If that changes one day then this code will fail.&lt;/p&gt;

&lt;p&gt;Enough with the boring warnings, though - let&#39;s get to the real nub of the matter; reflection is slooooooooow, isn&#39;t it? Surely we should never resort to such a clunky technology??&lt;/p&gt;

&lt;h3&gt;Approach 2: Using LINQ Expressions to generate fast code to set the field&lt;/h3&gt;

&lt;p&gt;If &lt;strong&gt;Example&lt;/strong&gt; had a regular private field that we wanted to set - eg.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public sealed class Example
{
  private int _somethingElse;
  public Example(int id, int somethingElse)
  {
    Id = id;
    _somethingElse = somethingElse;
  }

  public int Id { get; }

  public int GetSomethingElse()
  {
    return _somethingElse;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then we could use reflection to get a reference to that field once and build a delegate using LINQ Expressions that would allow us to update that field value using something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var field = typeof(Example).GetField(&quot;_somethingElse&quot;, BindingFlags.Instance | BindingFlags.NonPublic);

var sourceParameter = Expression.Parameter(typeof(Example), &quot;source&quot;);
var valueParameter = Expression.Parameter(field.FieldType, &quot;value&quot;);
var fieldSetter =
  Expression.Lambda&amp;lt;Action&amp;lt;Example, int&amp;gt;&amp;gt;(
    Expression.Assign(
      Expression.MakeMemberAccess(sourceParameter, field),
      valueParameter
    ),
    sourceParameter,
    valueParameter
  )
  .Compile();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We could then cache that &quot;fieldSetter&quot; delegate and call it any time that we wanted to update the private &quot;_somethingElse&quot; field on an &lt;strong&gt;Example&lt;/strong&gt; instance. There would be a one-off cost to the reflection that identifies the field and a one-off cost to generating that delegate initially but any subsequent call should be comparably quick to hand-written field-updating code (obviously it&#39;s not possible to hand-write code to update a private field from outside the class.. but you get the point).&lt;/p&gt;

&lt;p&gt;There&#39;s one big problem with this approach, though; it doesn&#39;t work for readonly fields. The &quot;Expression.Assign&quot; call will throw an &lt;strong&gt;ArgumentException&lt;/strong&gt; if the specified member is readonly:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Expression must be writeable&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;SAD FACE.&lt;/p&gt;

&lt;p&gt;This is quite unfortunate. It had been a little while since I&#39;d played around with LINQ Expressions and I was feeling quite proud of myself getting the code to work.. only to fall at the last hurdle.&lt;/p&gt;

&lt;p&gt;Never mind.&lt;/p&gt;

&lt;p&gt;One bright side is that I also tried out this code in a .NET Core application and it worked to the same extent as the &quot;full fat&quot; .NET Framework - ie. I was able to generate a delegate using LINQ Expressions that would set a non-readonly private field on an instance. Considering that reflection capabilities were limited in the early days of .NET Standard, I found it a nice surprise that support seems so mature now.&lt;/p&gt;

&lt;h3&gt;Approach 3: Emitting IL&lt;/h3&gt;

&lt;p&gt;Time to bring out the big guns!&lt;/p&gt;

&lt;p&gt;If the friendlier way of writing code that dynamically compiles other .NET code (ie. LINQ Expressions) wouldn&#39;t cut it, surely the old fashioned (and frankly intimidating) route of writing code to directly emit IL would do the job?&lt;/p&gt;

&lt;p&gt;It&#39;s been a long time since I&#39;ve written any IL-generating code, so let&#39;s take it slow. If we&#39;re starting with the case that worked with LINQ Expressions then we want to create a method that will take an &lt;strong&gt;Example&lt;/strong&gt; instance and an &lt;strong&gt;int&lt;/strong&gt; value in order to set the &quot;_somethingElse&quot; field on the &lt;strong&gt;Example&lt;/strong&gt; instance to that new number.&lt;/p&gt;

&lt;p&gt;The first thing to do is to create some scaffolding. The following code is almost enough to create a new method of type &lt;strong&gt;Action&amp;lt;Example, int&amp;gt;&lt;/strong&gt; -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Set restrictedSkipVisibility to true to avoid any pesky &quot;visibility&quot; checks being made (in other
// words, let the IL in the generated method access any private types or members that it tries to)
var method = new DynamicMethod(
  name: &quot;SetSomethingElseField&quot;,
  returnType: null,
  parameterTypes: new[] { typeof(Example), typeof(int) },
  restrictedSkipVisibility: true
);

var gen = method.GetILGenerator();

// TODO: Emit require IL op codes here..

var fieldSetter = (Action&amp;lt;Example, int&amp;gt;)method.CreateDelegate(typeof(Action&amp;lt;Example, int&amp;gt;));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The only problem is that &quot;TODO&quot; section.. the bit where we have to know what IL to generate.&lt;/p&gt;

&lt;p&gt;There are basically two ways you can go about working out what to write here. You can learn enough about IL (and remember it again years after you learn some!) that you can just start hammering away at the keyboard.. or you can write some C# that basically does what you want, compile that using Visual Studio and then use a disassembler to see what IL is produced. I&#39;m going for plan b. Handily, if you use Visual Studio then you probably already have a disassembler installed! It&#39;s called ildasm.exe and I found it on my computer in &quot;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.6.1 Tools&quot; after reading this: &quot;&lt;a href=&quot;https://blogs.msdn.microsoft.com/lucian/2008/11/14/where-are-the-sdk-tools-where-is-ildasm/&quot;&gt;Where are the SDK tools? Where is ildasm?&lt;/a&gt;&quot;.&lt;/p&gt;

&lt;p&gt;To make things as simple as possible, I created a new class in a C# project -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class SomethingWithPublicField
{
  public int Id;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and then created a static method that I would want to look at the disassembly of:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;static void MethodToCopy(SomethingWithPublicField source, int value)
{
  source.Id = value;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I compiled the console app, opened the exe in ildasm and located the method. Double-clicking it revealed this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;.method private hidebysig static void  MethodToCopy(class Test.Program/SomethingWithPublicField source,
                                                    int32 &#39;value&#39;) cil managed
{
  // Code size         9 (0x9)
  .maxstack  8
  IL_0000:  nop
  IL_0001:  ldarg.0
  IL_0002:  ldarg.1
  IL_0003:  stfld      int32 Test.Program/SomethingWithPublicField::Id
  IL_0008:  ret
} // end of method Program::MethodToCopyTyped
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ok. That actually couldn&#39;t be much simpler. The &quot;ldarg.0&quot; code means &quot;load argument 0 onto the stack&quot;, &quot;ldarg.1&quot; means &quot;load argument 1 onto the stack&quot; and &quot;stfld&quot; means take the instance of the first object on the stack and set the specified field to be the second object on the stack. &quot;ret&quot; just means exit method (returning any value, if there is one - which there isn&#39;t in this case).&lt;/p&gt;

&lt;p&gt;This means that the &quot;TODO&quot; comment in my scaffolding code may be replaced with real content, resulting in the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var field = typeof(Example).GetField(&quot;_somethingElse&quot;, BindingFlags.Instance | BindingFlags.NonPublic);

var method = new DynamicMethod(
  name: &quot;SetSomethingElseField&quot;,
  returnType: null,
  parameterTypes: new[] { typeof(Example), typeof(int) },
  restrictedSkipVisibility: true
);
var gen = method.GetILGenerator();
gen.Emit(OpCodes.Ldarg_0);
gen.Emit(OpCodes.Ldarg_1);
gen.Emit(OpCodes.Stfld, field);
gen.Emit(OpCodes.Ret);

var fieldSetter = (Action&amp;lt;Example, int&amp;gt;)method.CreateDelegate(typeof(Action&amp;lt;Example, int&amp;gt;));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That&#39;s it! We now have a delegate that is a compiled method for writing a new &lt;strong&gt;int&lt;/strong&gt; into the private field &quot;_somethingElse&quot; for any given instance of &lt;strong&gt;Example&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Unfortunately, things go wrong at exactly the same point as they did with LINQ Expressions. The above code works fine for setting a regular private field but if we tried to set a &lt;em&gt;readonly&lt;/em&gt; field using the same approach then we&#39;d be rewarded with an error:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;System.Security.VerificationException&lt;/strong&gt;: &#39;Operation could destabilize the runtime.&#39;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Another disappointment!*&lt;/p&gt;

&lt;p&gt;* &lt;em&gt;(Though hopefully not a surprise if you&#39;re reading this article since I said right at the top that only the first of these three approaches would work!)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;But, again, to try to find a silver lining, I also tried the non-readonly-private-field-setting-via-emitted-IL code in a .NET Core application and I was pleasantly surprised to find that it worked. It required the packages &quot;System.Reflection.Emit.ILGeneration&quot; and &quot;System.Reflection.Emit.Lightweight&quot; to be added through NuGet but nothing more difficult than that.&lt;/p&gt;

&lt;p&gt;Although I decided last month that &lt;a href=&quot;http://www.productiverage.com/revisiting-net-core-tooling-visual-studio-2017&quot;&gt;I&#39;m still not convinced that .NET Core is ready for me to use in work&lt;/a&gt;, I &lt;em&gt;am&lt;/em&gt; impressed by how much &lt;em&gt;does&lt;/em&gt; work with it.&lt;/p&gt;

&lt;h3&gt;Performance comparison&lt;/h3&gt;

&lt;p&gt;So we&#39;ve ascertained that there is only one way* to set a readonly field on an existing instance and, regrettably, it&#39;s also the slowest. I guess that a pertinent question to ask, though, is just &lt;em&gt;how much&lt;/em&gt; slower is the slowest?&lt;/p&gt;

&lt;p&gt;* &lt;em&gt;(As further evidence that there isn&#39;t another way around this, I&#39;ve found an issue from EntityFramework&#39;s GitHub repo: &quot;&lt;a href=&quot;https://github.com/aspnet/EntityFramework/issues/6202&quot;&gt;Support readonly fields&lt;/a&gt;&quot; which says that it&#39;s possible to set a readonly property with reflection but that the issue-raiser encountered the same two failures that I&#39;ve demonstrated above when he tried alternatives and no-one has proposed any other ways to tackle it)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Obviously we can&#39;t compare the readonly-field-setting performance of the three approaches above because only one of them is actually capable of doing that. But we &lt;em&gt;can&lt;/em&gt; compare the performance of something similar; setting a private (but not readonly) field, since &lt;em&gt;all three&lt;/em&gt; are able to achieve that.&lt;/p&gt;

&lt;p&gt;Ordinarily at this point, I would write some test methods and run them in a loop and time the loop and divide by the number of runs and then maybe repeat a few times for good measure and come up with a conclusion. Today, though, I thought that I might try something a bit different because I recently heard again about something called &quot;&lt;a href=&quot;https://github.com/dotnet/BenchmarkDotNet&quot;&gt;BenchmarkDotNet&lt;/a&gt;&quot;. It claims that:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Benchmarking is really hard (especially microbenchmarking), you can easily make a mistake during performance measurements. BenchmarkDotNet will protect you from the common pitfalls (even for experienced developers) because it does all the dirty work for you: it generates an isolated project per each benchmark method, does several launches of this project, run multiple iterations of the method (include warm-up), and so on. Usually, you even shouldn&#39;t care about a number of iterations because BenchmarkDotNet chooses it automatically to achieve the requested level of precision.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This sounds ideal for my purposes!&lt;/p&gt;

&lt;p&gt;What I&#39;m most interesting in is how reflection compares to compiled LINQ expressions and to emitted IL when it comes to setting a private field. If this is of any importance whatsoever then presumably the code will be run over and over again and so it should be the execution time of the compiled property-setting code that is of interest - the time taken to actually compile the LINQ expressions / emitted IL can probably be ignored as it should disappear into insignificance when the delegates are called enough times. But, for a sense of thoroughness (and because BenchmarkDotNet makes it so easy), I&#39;ll &lt;em&gt;also&lt;/em&gt; measure the time that it takes to do the delegate compilation as well.&lt;/p&gt;

&lt;p&gt;To do this, I created a .NET Core Console application in VS2017, added the BenchmarkDotNet NuGet package and changed the .csproj file by hand to build for both .NET Core and .NET Framework 4.6.1 by changing&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;TargetFramework&amp;gt;netcoreapp1.1&amp;lt;/TargetFramework&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;to&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;TargetFrameworks&amp;gt;netcoreapp1.1;net461&amp;lt;/TargetFrameworks&amp;gt;
&amp;lt;PlatformTarget&amp;gt;AnyCPU&amp;lt;/PlatformTarget&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(as described in the &lt;a href=&quot;https://github.com/dotnet/BenchmarkDotNet/blob/master/docs/guide/FAQ.md&quot;&gt;BenchmarkDotNet FAQ&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Then I put the following together. There are six benchmarks in total; three to measure the creation of the different types of property-setting delegates and three to then measure the execution time of those delegates -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class Program
{
    static void Main(string[] args)
    {
        BenchmarkRunner.Run&amp;lt;TimedSetter&amp;gt;();
        Console.ReadLine();
    }
}

[CoreJob, ClrJob]
public class TimedSetter
{
    private SomethingWithPrivateField _target;
    private FieldInfo _field;
    private Action&amp;lt;SomethingWithPrivateField, int&amp;gt;
        _reflectionSetter,
        _linqExpressionSetter,
        _emittedILSetter;

    [GlobalSetup]
    public void GlobalSetup()
    {
        _target = new SomethingWithPrivateField();

        _field = typeof(SomethingWithPrivateField)
            .GetFields(BindingFlags.NonPublic | BindingFlags.Instance)
            .FirstOrDefault(f =&amp;gt; f.Name == &quot;_id&quot;);

        _reflectionSetter = ConstructReflectionSetter();
        _linqExpressionSetter = ConstructLinqExpressionSetter();
        _emittedILSetter = ConstructEmittedILSetter();
    }

    [Benchmark]
    public Action&amp;lt;SomethingWithPrivateField, int&amp;gt; ConstructReflectionSetter()
    {
        return (source, value) =&amp;gt; _field.SetValue(source, value);
    }

    [Benchmark]
    public Action&amp;lt;SomethingWithPrivateField, int&amp;gt; ConstructLinqExpressionSetter()
    {
        var sourceParameter = Expression.Parameter(typeof(SomethingWithPrivateField), &quot;source&quot;);
        var valueParameter = Expression.Parameter(_field.FieldType, &quot;value&quot;);
        var fail = Expression.Assign(
            Expression.MakeMemberAccess(sourceParameter, _field),
            valueParameter
        );
        return Expression.Lambda&amp;lt;Action&amp;lt;SomethingWithPrivateField, int&amp;gt;&amp;gt;(
                Expression.Assign(
                    Expression.MakeMemberAccess(sourceParameter, _field),
                    valueParameter
                ),
                sourceParameter,
                valueParameter
            )
            .Compile();
    }

    [Benchmark]
    public Action&amp;lt;SomethingWithPrivateField, int&amp;gt; ConstructEmittedILSetter()
    {
        var method = new DynamicMethod(
            name: &quot;SetField&quot;,
            returnType: null,
            parameterTypes: new[] { typeof(SomethingWithPrivateField), typeof(int) },
            restrictedSkipVisibility: true
        );
        var gen = method.GetILGenerator();
        gen.Emit(OpCodes.Ldarg_0);
        gen.Emit(OpCodes.Ldarg_1);
        gen.Emit(OpCodes.Stfld, _field);
        gen.Emit(OpCodes.Ret);
        return (Action&amp;lt;SomethingWithPrivateField, int&amp;gt;)method.CreateDelegate(
          typeof(Action&amp;lt;SomethingWithPrivateField, int&amp;gt;)
        );
    }

    [Benchmark]
    public void SetUsingReflection()
    {
        _reflectionSetter(_target, 1);
    }

    [Benchmark]
    public void SetUsingLinqExpressions()
    {
        _linqExpressionSetter(_target, 1);
    }

    [Benchmark]
    public void SetUsingEmittedIL()
    {
        _emittedILSetter(_target, 1);
    }
}

public class SomethingWithPrivateField
{
    private int _id;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &quot;GlobalSetup&quot; method will be run once and will construct the delegates for delegate-executing benchmark methods (&quot;SetUsingReflection&quot;, &quot;SetUsingLinqExpressions&quot; and &quot;SetUsingEmittedIL&quot;). The time that it takes to execute the [GlobalSetup] method does not contribute to any of the benchmark method times - the benchmark methods will record &lt;em&gt;only&lt;/em&gt; their own execution time.&lt;/p&gt;

&lt;p&gt;However, having delegate-creation benchmark methods (&quot;ConstructReflectionSetter&quot;, &quot;ConstructLinqExpressionSetter&quot; and &quot;ConstructEmittedILSetter&quot;) means that I&#39;ll have an idea how large the initial cost to construct each delegate is (or isn&#39;t), separate to the cost of executing each type of delegate.&lt;/p&gt;

&lt;p&gt;BenchmarkDotNet has capabilities beyond what I&#39;ve taken advantage of. For example, it can also build for Mono (though I don&#39;t have Mono installed on my computer, so I didn&#39;t try this) and it can test 32-bit vs 64-bit builds.&lt;/p&gt;

&lt;p&gt;Aside from testing .NET Core 1.1 and .NET Framework 4.6.1, I&#39;ve kept things fairly simple.&lt;/p&gt;

&lt;p&gt;After it has run, it emits the following summary about my computer:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;BenchmarkDotNet=v0.10.8, OS=Windows 8.1 (6.3.9600)&lt;/p&gt;
  
  &lt;p&gt;Processor=AMD FX(tm)-8350 Eight-Core Processor, ProcessorCount=8&lt;/p&gt;
  
  &lt;p&gt;Frequency=14318180 Hz, Resolution=69.8413 ns, Timer=HPET&lt;/p&gt;
  
  &lt;p&gt;dotnet cli version=1.0.4&lt;/p&gt;
  
  &lt;p&gt;[Host] : .NET Core 4.6.25211.01, 64bit RyuJIT [AttachedDebugger]&lt;/p&gt;
  
  &lt;p&gt;Clr    : Clr 4.0.30319.42000, 64bit RyuJIT-v4.6.1087.0&lt;/p&gt;
  
  &lt;p&gt;Core   : .NET Core 4.6.25211.01, 64bit RyuJIT&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;And produces the following table:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;                        Method |  Job | Runtime |           Mean |         Error |        StdDev |
------------------------------ |----- |-------- |----------------|---------------|---------------|
     ConstructReflectionSetter |  Clr |     Clr |       9.980 ns |     0.2930 ns |     0.4895 ns |
 ConstructLinqExpressionSetter |  Clr |     Clr | 149,552.853 ns | 1,752.4151 ns | 1,639.2100 ns |
      ConstructEmittedILSetter |  Clr |     Clr | 126,454.797 ns | 1,143.9593 ns | 1,014.0900 ns |
            SetUsingReflection |  Clr |     Clr |     158.784 ns |     3.1892 ns |     3.6727 ns |
       SetUsingLinqExpressions |  Clr |     Clr |       1.139 ns |     0.0542 ns |     0.0742 ns |
             SetUsingEmittedIL |  Clr |     Clr |       1.832 ns |     0.0689 ns |     0.1132 ns |
                               |      |         |                |               |               |
     ConstructReflectionSetter | Core |    Core |       9.465 ns |     0.1083 ns |     0.0904 ns |
 ConstructLinqExpressionSetter | Core |    Core |  66,430.408 ns | 1,303.5243 ns | 2,104.9488 ns |
      ConstructEmittedILSetter | Core |    Core |  38,483.764 ns |   605.3819 ns |   536.6553 ns |
            SetUsingReflection | Core |    Core |   2,626.527 ns |    24.1110 ns |    22.5534 ns |
       SetUsingLinqExpressions | Core |    Core |       1.063 ns |     0.0516 ns |     0.0688 ns |
             SetUsingEmittedIL | Core |    Core |       1.718 ns |     0.0599 ns |     0.0560 ns |
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The easiest thing to interpret is the &quot;Mean&quot; - BenchmarkDotNet did a few &quot;pilot runs&quot; to try to see how long the benchmark methods would take and then tries to decide what is an appropriate number of runs to do for real in order to get reliable results.&lt;/p&gt;

&lt;p&gt;The short version is that when delegates are compiled using LINQ Expressions and emitted-IL that they both execute &lt;em&gt;a lot&lt;/em&gt; faster than reflection; over 85x faster for .NET Framework 4.6.1 and 1,500x faster for .NET Core 1.1!&lt;/p&gt;

&lt;p&gt;The huge difference between reflection and the other two approaches, though, may slightly overshadow the fact that the LINQ Expression delegates are actually about 1.6x faster than the emitted-IL delegates. I hadn&#39;t expected this at all, I would have thought that they would be almost identical - in fact, I&#39;m still surprised and don&#39;t currently have any explanation for it.&lt;/p&gt;

&lt;p&gt;The mean value doesn&#39;t usually tell the whole story, though. When looking at the mean, it&#39;s also useful to look at the Standard Deviation (&quot;StdDev&quot; in the table above). The mean might be within a small spread of values or a very large spread of values. A small spread is better because it suggests that the single mean value that we&#39;re looking at is representative of behaviour in the real world and that values aren&#39;t likely to vary too wildly - a large standard deviation means that there was much more variation in the recorded values and so the times could be all over the place in the real world. (Along similar lines, the &quot;Error&quot; value is described as being &quot;Half of 99.9% confidence interval&quot; - again, the gist is that smaller values suggest that the mean is a more useful indicator of what we would see in the real world for any given request).&lt;/p&gt;

&lt;p&gt;What I&#39;ve ignored until this point are the &quot;ConstructReflectionSetter&quot; / &quot;ConstructLinqExpressionSetter&quot; / &quot;ConstructEmittedILSetter&quot; methods. If we first look at the generation of the LINQ Expression delegate on .NET 4.6.1, we can see that the mean time to generate that delegate was around 150ms - compared to approx 10ms for the reflection delegate. Each time the LINQ Expressions delegate is used to set the field instead of the reflection delegate we save around 0.16ms. That means that we need to call the delegate around 950 times in order to pay of the cost of constructing it!&lt;/p&gt;

&lt;p&gt;As I suggested earlier, it would only make sense to investigate these sort of optimisations if you expect to execute the code over and over and over again (otherwise, why not just keep it simple and stick to using plain old reflection).. but it&#39;s still useful to have the information about just how much &quot;upfront cost&quot; there is to things like this, compared to how much you hope to save in the long run.&lt;/p&gt;

&lt;p&gt;It&#39;s also interesting to see the discrepancies between .NET Framework 4.6.1 and .NET Core 1.1 - the times to compile LINQ Expressions and emitted-IL delegates are noticeably shorter and the time to set the private field by reflection noticeably longer. In fact, these differences mean that you only need to set the field 25 times before you start to offset the cost of creating the LINQ Expressions delegate (when you compare it to updating the field using reflection) and only 14 times to offset the cost of creating the emitted-IL delegate!&lt;/p&gt;

&lt;h3&gt;BenchmarkDotNet is fun&lt;/h3&gt;

&lt;p&gt;I&#39;m really happy with how easy BenchmarkDotNet makes it to measure these sorts of very short operations. Whenever I&#39;ve tried to do something similar in the past, I&#39;ve felt niggling doubts that maybe I&#39;m not running it enough times or maybe there are some factors that I should try to average out. Even when I get a result, I&#39;ve sometimes just looked at the single average (ie. the mean) time taken, which is a bit sloppy since the spread of results can be of vital importance as well. That BenchmarkDotNet presents the final data in such a useful way and with so few decisions on my part is fantastic.&lt;/p&gt;

&lt;h3&gt;Benchmarking across multiple frameworks&lt;/h3&gt;

&lt;p&gt;I forget each time that I start a new project how the running-benchmarks-against-multiple-frameworks functionality works, so I&#39;ll add a note here for anyone else that gets confused (and, likely, for me in the future!) - the first thing to do is to manually edit the .csproj file of the benchmark project so that it includes the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;TargetFrameworks&amp;gt;netcoreapp2.0;net461&amp;lt;/TargetFrameworks&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It&#39;s not currently possible to specify multiple projects using the VS GUI, so your .csproj file will normally have a line like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;TargetFramework&amp;gt;netcoreapp2.0&amp;lt;/TargetFramework&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(Not only is there only a single framework specified but the node is called &quot;TargetFramework&quot; - without an &quot;s&quot; - as opposed to &quot;TargetFrameworks&quot; &lt;em&gt;with&lt;/em&gt; an &quot;s&quot;)&lt;/p&gt;

&lt;p&gt;After you&#39;ve done this, you need to run the benchmark project from the command line (if you try to run it from within VS, even in Release configuation, you will get a warning that the results may be inaccurate as a debugger is attached). You do that with a command like this (it may vary if you&#39;re using a different version of .NET Core) -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dotnet run --framework netcoreapp2.0 --configuration release
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You have to specify a framework to run the project as but (and this is the important part) that does &lt;em&gt;not&lt;/em&gt; mean that the benchmarks will only be run against the framework. What happens when you run this command is that multiple executables are built and then executed, which run the tests in each of the frameworks that you specified in the benchmark attributes and in the &quot;TargetFrameworks&quot; node in the .csproj file. The results of these multiple executables are aggregated to give you the final benchmark output.&lt;/p&gt;

&lt;h3&gt;.NET Core upsets me again&lt;/h3&gt;

&lt;p&gt;On the other hand, unfortunately .NET Core has been hard work for me again when it came to BenchmarkDotNet. I made it sound very easy earlier to get everything up and running because I didn&#39;t want dilute my enthusiasm for the benchmarking. However, I did have a myriad of problems before everything started working properly.&lt;/p&gt;

&lt;p&gt;When I was hand-editing the .csproj file to target multiple frameworks (I still don&#39;t know why this isn&#39;t possible within VS when editing project properties), Visual Studio would only seem to intermittently acknowledge that I&#39;d changed it and offer to reload. This wasn&#39;t super-critical but it also didn&#39;t fill me with confidence.&lt;/p&gt;

&lt;p&gt;When it &lt;em&gt;was&lt;/em&gt; ready to build and target both .NET Framework 4.6.1 and .NET Core 1.1, I got a cryptic warning:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Detected package downgrade: Microsoft.NETCore.App from 1.1.2 to 1.1.1 &lt;/p&gt;
  
  &lt;p&gt;CoreExeTest (&gt;= 1.0.0) -&gt; BenchmarkDotNet (&gt;= 0.10.8) -&gt; Microsoft.NETCore.App (&gt;= 1.1.2) 
   CoreExeTest (&gt;= 1.0.0) -&gt; Microsoft.NETCore.App (&gt;= 1.1.1)               &lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Everything seemed to build alright but I didn&#39;t know if this was something to worry about or not (I like my projects to be zero-warning). It suggested to me that I was targeting .NET Core 1.1 and BenchmarkDotNet was expecting .NET Core 1.1.2 - sounds simple enough, surely I can upgrade? I first tried changing the .csproj to target &quot;netcoreapp1.1.2&quot; but that didn&#39;t work. In fact, it &quot;didnt work&quot; in a very unhelpful way; when I ran the project it would open in a window and immediately close, with no way to break and catch the exception in the debugger. I used &quot;dotnet run&quot;* on the command line to try to see more information and was then able to see the error message:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The specified framework &#39;Microsoft.NETCore.App&#39;, version &#39;1.1.2&#39; was not found.&lt;/p&gt;
  
  &lt;ul&gt;
  &lt;li&gt;&lt;p&gt;Check application dependencies and target a framework version installed at:&lt;/p&gt;
  
  &lt;p&gt;C:\Program Files\dotnet\shared\Microsoft.NETCore.App&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;The following versions are installed:&lt;/p&gt;
  
  &lt;p&gt;1.0.1&lt;/p&gt;
  
  &lt;p&gt;1.0.4&lt;/p&gt;
  
  &lt;p&gt;1.1.1&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;Alternatively, install the framework version &#39;1.1.2&#39;.&lt;/p&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;* &lt;em&gt;(Before being able to use &quot;dotnet run&quot; I had to manually edit the .csproj file to only target .NET Core - if you target multiple frameworks and try to use &quot;dotnet run&quot; then you get an error &quot;Unable to run your project. Please ensure you have a runnable project type and ensure &#39;dotnet run&#39; supports this project&quot;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I changed the .csproj file back from &quot;netcoreapp1.1.2&quot; to &quot;netcoreapp1.1&quot; and went to the NuGet UI to see if I could upgrade the &quot;Microsoft.NETCore.App&quot; package.. but the version dropdown wouldn&#39;t let me change it (stating that the other versions that it was aware of were &quot;Blocked by project&quot;).&lt;/p&gt;

&lt;p&gt;I tried searching online for a way to download and install 1.1.2 but got nowhere.&lt;/p&gt;

&lt;p&gt;Finally, I saw that VS 2017 had an update pending entitled &quot;Visual Studio 15.2 (26430.16)&quot;. The &quot;15.2&quot; caught me out for a minute because I initially presumed it was an update for VS &lt;em&gt;2015&lt;/em&gt;. The update includes .NET Core 1.1.2 (see &lt;a href=&quot;https://github.com/dotnet/core/issues/622&quot;&gt;this dotnet GitHub issue&lt;/a&gt;) and, when I loaded my solution again, the warning above had gone. Looking at the installed packages for my project, I saw that &quot;Microsoft.NETCore.App&quot; was now on version 1.1.2 and that all other versions were &quot;Blocked by project&quot;. This does not feel friendly and makes me worry about sharing code with others - if they don&#39;t have the latest version of Visual Studio then the code may cause them warnings like the above that don&#39;t happen on my PC. Yuck.&lt;/p&gt;

&lt;p&gt;After all this, I got the project compiling (without warnings) and running, only for it to intermittently fail as soon as it started:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Access to the path &#39;BDN.Generated.dll&#39; is defined&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This relates to an output folder created by BenchmarkDotNet. Sometimes this folder would be locked and it would not be possible to overwrite the files on the next run. Windows wouldn&#39;t let me delete the folder directly but I could trick it by renaming the folder and &lt;em&gt;then&lt;/em&gt; deleting it. I didn&#39;t encounter this problem if I created an old-style .NET Framework project and used BenchmarkDotNet there - this would prevent me from running tests against multiple frameworks but it might have also prevented me from teetering over the brink of insanity.&lt;/p&gt;

&lt;p&gt;This is not how I would expect mature tooling to behave. For now, I continue to consider .NET Core as the Top Gear boys (when they still &lt;em&gt;were&lt;/em&gt; the Top Gear boys) described old Alfa Romeos; &quot;you want to believe that it can be something wonderful but you couldn&#39;t, in all good conscience, recommend it to a friend&quot;.&lt;/p&gt;

&lt;h3&gt;Summary&lt;/h3&gt;

&lt;p&gt;I suspect that, to some, this may seem like one of my more pointless blog posts. I tried to do something that .NET really doesn&#39;t want you to do (and that whoever wrote the code containing the readonly auto-properties really doesn&#39;t expect you to do) and then tried to optimise that naughty behaviour - then spent a lot more time explaining how it wasn&#39;t posible to do so!&lt;/p&gt;

&lt;p&gt;However, along the way I discovered BenchmarkDotNet and I&#39;m counting that as a win - I&#39;ll be keeping that in my arsenal for future endeavours. And I also enjoyed revisiting what is and isn&#39;t possible with reflection and reminding myself of the ways that .NET allows you to write code that could make &lt;em&gt;my&lt;/em&gt; code appear to work in surprising ways.&lt;/p&gt;

&lt;p&gt;Finally, it was interesting to see how the .NET Framework compared to .NET Core in terms of performance for these benchmarks &lt;em&gt;and&lt;/em&gt; to see take another look at the question of how mature .NET Core and its tooling is (or isn&#39;t). And when you learn a few things, can it ever really count as a waste of time?&lt;/p&gt;

&lt;h3&gt;Minor follow-up (8th August 2017)&lt;/h3&gt;

&lt;p&gt;A comment on this post by &quot;ai_enabled&quot; asked about the use of the reflection method &quot;SetValueDirect&quot; instead of &quot;SetValue&quot;. I must admit that I was unaware of this method but it was an interesting question posed about its performance in comparison to &quot;SetValue&quot; &lt;em&gt;and&lt;/em&gt; there was a very important point made about the code that I&#39;d presented so far when it comes to structs; in particular, because structs are copied when they&#39;re passed around, the property-update mechanisms that I&#39;ve shown wouldn&#39;t have worked. I&#39;ll try to demonstrate this with some code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public static void Main()
{
    var field = typeof(SomeStructWithPrivateField)
      .GetFields(BindingFlags.NonPublic | BindingFlags.Instance)
      .FirstOrDefault(f =&amp;gt; f.Name == &quot;_id&quot;);

    // FAIL! &quot;target&quot; will still have an &quot;_id&quot; value of zero :(
    var target = new SomeStructWithPrivateField();
    field.SetValue(target, 123);
}

public struct SomeStructWithPrivateField
{
    private int _id;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Because the &quot;SetValue&quot; method&#39;s first parameter is of type &lt;strong&gt;object&lt;/strong&gt;, the &quot;target&quot; struct will get boxed - any time that a non-reference type is passed as an argument where a reference type is expected, it effectively gets &quot;wrapped up&quot; into an &lt;strong&gt;object&lt;/strong&gt;. I won&#39;t go into all of the details of boxing / unboxing here (if you&#39;re interested, though, then &quot;&lt;a href=&quot;https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/types/boxing-and-unboxing&quot;&gt;Boxing and Unboxing (C# Programming Guide)&lt;/a&gt;&quot; is a good starting point) but one important thing to note is that structs are copied as part of the boxing process. This means &quot;SetValue&quot; will be working on a copy of &quot;target&quot; and so the &quot;_id&quot; property of the &quot;target&quot; value will not be changed by the &quot;SetValue&quot; call!&lt;/p&gt;

&lt;p&gt;The way around this is to use &quot;SetValueDirect&quot;, which takes a special &lt;strong&gt;TypedReference&lt;/strong&gt; argument. The way in which this is done is via the little-known &quot;__makeref&quot; keyword (I wasn&#39;t aware of it before looking into &quot;SetValueDirect&quot;) -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public static void Main()
{
    var field = typeof(SomeStructWithPrivateField)
      .GetFields(BindingFlags.NonPublic | BindingFlags.Instance)
      .FirstOrDefault(f =&amp;gt; f.Name == &quot;_id&quot;);

    // SUCCESS! &quot;target&quot; will have its &quot;_id&quot; value updated!
    var target = new SomeStructWithPrivateField();
    field.SetValueDirect(__makeref(target), 123);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If we wanted to wrap this up into a delegate then we need to ensure that the target parameter is marked as being &quot;ref&quot;, otherwise we&#39;ll end up creating another place that the struct gets copied and the update lost. That means that we can no longer use something like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Action&amp;lt;SomeStructWithPrivateField, int&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In fact, we can&#39;t use the generic &lt;strong&gt;Action&lt;/strong&gt; class at all because it doesn&#39;t allow for &quot;ref&quot; parameters to be specified. Instead, we&#39;ll need to define a new delegate -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public delegate void Updater(ref SomeStructWithPrivateField target, int value);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Instances of this may be created like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var field = typeof(SomeStructWithPrivateField)
  .GetFields(BindingFlags.NonPublic | BindingFlags.Instance)
  .FirstOrDefault(f =&amp;gt; f.Name == &quot;_id&quot;);

Updater updater = (ref SomeStructWithPrivateField target, int value)
  =&amp;gt; field.SetValueDirect(__makeref(target), id);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If we want to do the same with the LINQ Expressions or generated IL approaches then we need only make some minor code tweaks to what we saw earlier. The first argument of the generated delegates must be changed to be a &quot;ref&quot; type and we need to generate a delegate of type &lt;strong&gt;Updater&lt;/strong&gt; instead of &lt;strong&gt;Action&amp;lt;SomeStructWithPrivateField, int&amp;gt;&lt;/strong&gt; -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Construct an &quot;Updater&quot; delegate using LINQ Expressions
var sourceParameter = Expression.Parameter(
  typeof(SomeStructWithPrivateField).MakeByRefType(),
  &quot;source&quot;
);
var valueParameter = Expression.Parameter(field.FieldType, &quot;value&quot;);
var fail = Expression.Assign(
  Expression.MakeMemberAccess(sourceParameter, field),
  valueParameter
);
var linqExpressionUpdater = Expression.Lambda&amp;lt;Updater&amp;gt;(
    Expression.Assign(
      Expression.MakeMemberAccess(sourceParameter, field),
      valueParameter
    ),
    sourceParameter,
    valueParameter
  )
  .Compile();

// Construct an &quot;Updater&quot; delegate by generating IL
var method = new DynamicMethod(
  name: &quot;SetField&quot;,
  returnType: null,
  parameterTypes: new[] { typeof(SomeStructWithPrivateField).MakeByRefType(), typeof(int) },
  restrictedSkipVisibility: true
);
var gen = method.GetILGenerator();
gen.Emit(OpCodes.Ldarg_0);
gen.Emit(OpCodes.Ldind_Ref);
gen.Emit(OpCodes.Ldarg_1);
gen.Emit(OpCodes.Stfld, field);
gen.Emit(OpCodes.Ret);
var emittedIlUpdater = (Updater)method.CreateDelegate(typeof(Updater));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;(Note: There is an additional &quot;Ldind_Ref&quot; instruction required for the IL to &quot;unwrap&quot; the ref argument but it&#39;s otherwise the same)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I used BenchmarkDotNet again to compare the performance of the two reflection methods (&quot;SetValue&quot; and &quot;SetValueDirect&quot;) against LINQ Expressions and emitted IL when setting a private field on an instance and found that having to call &quot;__makeRef&quot; and &quot;SetValueDirect&quot; was much slower on .NET 4.6.1 than just calling &quot;SetValue&quot; (about 17x slower) but actually marginally &lt;em&gt;faster&lt;/em&gt; on .NET Core.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;                                 Method | Runtime |           Mean |         Error |        StdDev |
--------------------------------------- |-------- |----------------|---------------|---------------|
              ConstructReflectionSetter |     Clr |      11.285 ns |     0.3082 ns |     0.8281 ns |
 ConstructReflectionWithSetDirectSetter |     Clr |      10.597 ns |     0.2845 ns |     0.4345 ns |
          ConstructLinqExpressionSetter |     Clr | 196,194.530 ns | 2,075.5246 ns | 1,839.8983 ns |
               ConstructEmittedILSetter |     Clr | 170,913.441 ns | 2,289.5219 ns | 2,141.6200 ns |
                     SetUsingReflection |     Clr |     142.976 ns |     2.8706 ns |     3.3058 ns |
         SetUsingReflectionAndSetDirect |     Clr |   2,444.816 ns |    40.9226 ns |    38.2790 ns |
                SetUsingLinqExpressions |     Clr |       2.370 ns |     0.0795 ns |     0.0744 ns |
                      SetUsingEmittedIL |     Clr |       2.616 ns |     0.0849 ns |     0.0834 ns |
              ConstructReflectionSetter |    Core |      10.595 ns |     0.2196 ns |     0.1946 ns |
 ConstructReflectionWithSetDirectSetter |    Core |      10.540 ns |     0.2838 ns |     0.3378 ns |
          ConstructLinqExpressionSetter |    Core | 117,697.478 ns |   758.9277 ns |   672.7696 ns |
               ConstructEmittedILSetter |    Core |  82,080.062 ns |   310.8230 ns |   275.5365 ns |
                     SetUsingReflection |    Core |   2,782.834 ns |    17.5705 ns |    16.4355 ns |
         SetUsingReflectionAndSetDirect |    Core |   2,541.563 ns |    21.8272 ns |    20.4172 ns |
                SetUsingLinqExpressions |    Core |       2.421 ns |     0.0227 ns |     0.0212 ns |
                      SetUsingEmittedIL |    Core |       2.655 ns |     0.0090 ns |     0.0080 ns |
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It&#39;s worth noting that the LINQ Expressions and emitted-IL approaches are slightly slower when working with a &quot;ref&quot; parameter than they were in the original version of the code. I suppose that this makes sense because there is an extra instruction explicitly required in the emitted-IL code and the LINQ-Expression-constructed delegate will have to deal with the added indirection under the hood (though this happens &quot;by magic&quot; and the way that LINQ Expressions code doesn&#39;t need to be changed to account for it).&lt;/p&gt;

&lt;p&gt;I guess that it&#39;s possible that &quot;SetValueDirect&quot; could be faster if you already have a &lt;strong&gt;TypedReference&lt;/strong&gt; (which is what &quot;__makeRef&quot; gives you) and you want to set multiple properties on it.. but that wasn&#39;t the use case that I had in mind when I looked into all of this and so I haven&#39;t tried to measured that.&lt;/p&gt;

&lt;p&gt;All in all, this was another fun diversion. It&#39;s curious that the performance between using &quot;SetValue&quot; and &quot;__makeRef&quot; / &quot;SetValueDirect&quot; is so pronounced in the &quot;classic&quot; .NET Framework but much less so in Core. On the other hand, if the target reference is a struct then the performance discrepancies are moot since trying to use &quot;SetValue&quot; won&#39;t work!&lt;/p&gt;

&lt;h4&gt;Some more notes about .NET Core&lt;/h4&gt;

&lt;p&gt;If you want to try to reproduce this for yourself in .NET Core then (accurate as of 8th August 2017) you need to install Visual Studio 2017 15.3 Preview 2 so that you can build .NET Core 2.0 projects and you&#39;ll then need to install the NuGet package &quot;System.Runtime.CompilerServices.Unsafe&quot;*. Without both of these, you won&#39;t be able to use &quot;__makeRef&quot;, you&#39;ll get a slightly cryptic error:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Predefined type &#39;System.TypedReference&#39; is not defined or imported&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;* &lt;em&gt;(I found this out via Ben Bowen&#39;s post &quot;&lt;a href=&quot;http://benbowen.blog/post/fun_with_makeref/&quot;&gt;Fun With __makeref&lt;/a&gt;&quot;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Once you have these bleeding edge bits, though, you can build a project with BenchmarkDotNet tests configured to run in both .NET Framework and .NET Core with a .csproj like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;Project Sdk=&quot;Microsoft.NET.Sdk&quot;&amp;gt;

  &amp;lt;PropertyGroup&amp;gt;
    &amp;lt;OutputType&amp;gt;Exe&amp;lt;/OutputType&amp;gt;
    &amp;lt;TargetFrameworks&amp;gt;netcoreapp2.0;net461&amp;lt;/TargetFrameworks&amp;gt;
    &amp;lt;PlatformTarget&amp;gt;AnyCPU&amp;lt;/PlatformTarget&amp;gt;
  &amp;lt;/PropertyGroup&amp;gt;

  &amp;lt;ItemGroup&amp;gt;
    &amp;lt;PackageReference Include=&quot;BenchmarkDotNet&quot; Version=&quot;0.10.8&quot; /&amp;gt;
  &amp;lt;/ItemGroup&amp;gt;

&amp;lt;/Project&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and you can run the tests in &lt;em&gt;both&lt;/em&gt; frameworks using this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dotnet run --framework netcoreapp2.0 --configuration release
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Don&#39;t be fooled by the fact that you have to specify a single framework - through BenchmarkDotNet magic, the tests &lt;em&gt;will&lt;/em&gt; be run for both frameworks (so long as you annotate the benchmark class with &quot;[CoreJob, ClrJob]&quot;) and the results will be displayed in one convenient combined table at the end.&lt;/p&gt;</description>
			<pubDate>Wed, 26 Jul 2017 20:31:00 GMT</pubDate>
		</item>
		<item>
			<title>Revisiting .NET Core tooling (Visual Studio 2017)</title>
            <link>http://www.productiverage.com/revisiting-net-core-tooling-visual-studio-2017</link>
			<guid>http://www.productiverage.com/revisiting-net-core-tooling-visual-studio-2017</guid>
			<description>&lt;p&gt;In November last year, I migrated a fairly small but non-trivial project to .NET Core to see what I thought about the process and whether I was happy to make .NET Core projects my default. At that point I wasn&#39;t happy to, there were too many rough edges.&lt;/p&gt;

&lt;p&gt;Since then, things have changed. The project.json format has been replaced with a .csproj format that is supported by the now-available Visual Studio 2017 and various other aspects of .NET Core development have had a chance to mature. So I thought that it was worth revisiting.&lt;/p&gt;

&lt;h3&gt;TL;DR&lt;/h3&gt;

&lt;p&gt;Things have come on a long way since the end of last year. But you don&#39;t get the level of consistency and stability with Visual Studio when you use it to develop .NET Core applications that you do when you use it to develop .NET Framework applications. To avoid frustration (and because I don&#39;t currently have use cases that would benefit from multi-platform support), I&#39;m still not going to jump to .NET Core for my day-to-day professional development tasks. I will probably dabble with it for personal projects.&lt;/p&gt;

&lt;h3&gt;The Good&lt;/h3&gt;

&lt;p&gt;First off, I&#39;m a huge fan of the new .csproj format. The &quot;legacy&quot; .csproj is huge and borderline-incomprehensible. Just what do the &quot;ProjectTypeGuids&quot; values mean and what are the acceptable choices? An incorrect one will mean that VS won&#39;t load the project and it won&#39;t readily give you information as to why. The new format is streamlined and beautiful. They took the best parts of project.json and made it into a format that would play better with MSBuild (and avoiding frightening developers who see &quot;project.json&quot; and get worried they&#39;re working on a frontend project that may have a terrifying Babel configuration hidden somewhere). I like that files in the folder structure are included by default, it makes sense (the legacy format required that every file explicitly be &quot;opted in&quot; to the project).&lt;/p&gt;

&lt;p&gt;Next big win: Last time I tried .NET Core, one of the things that I wanted to examine was how it easy it would be to migrate a solution with multiple projects. Could I change one project from being .NET Framework to .NET Core and then reference that project from the .NET Framework projects? It was possible but only with an ugly hack (where you had to edit the legacy .NET Framework .csproj files and manually create the references). That wasn&#39;t the end of it, though, since this hack confused VS and using &quot;Go To Definition&quot; on a reference that lead into a .NET Core dependency would take you to a &quot;from metadata&quot; view instead of the file in the Core project. Worse, the .NET Framework project wouldn&#39;t know that it had to be rebuilt if the Core project that it referenced was rebuilt. All very clumsy. The good news is that VS2017 makes this all work perfectly!&lt;/p&gt;

&lt;p&gt;Shared projects may also now be referenced from .NET Core projects. This didn&#39;t work in VS2015. There were workarounds but, again, they were a bit ugly (see the Stack Overflow question &lt;a href=&quot;https://stackoverflow.com/questions/38523457/how-do-i-reference-a-visual-studio-shared-project-in-a-net-core-class-library&quot;&gt;How do I reference a Visual Studio Shared Project in a .NET Core Class Library&lt;/a&gt;). With 2017, everything works as you would expect.&lt;/p&gt;

&lt;p&gt;The final positive isn&#39;t something that&#39;s changed since last year but I think that it&#39;s worth shouting out again - the command line experience with .NET Core is really good. Building projects, running tests and creating NuGet packages are all really simple. In many of my older projects, I&#39;ve had some sort of custom NuGet-package-creating scripts but any .NET Core projects going forward won&#39;t need them. (One thing that I particularly like is that if you have a unit test project that builds for multiple frameworks - eg. .NET Core 1.1 and .NET Framework 4.5.2 - then the tests will all be run against both frameworks when &quot;dotnet test&quot; is executed).&lt;/p&gt;

&lt;h3&gt;The Bad&lt;/h3&gt;

&lt;p&gt;Let&#39;s look at the not-so-good stuff. Firstly, I still find some of the terminology around .NET Core confusing. And, reading around, I&#39;m not the only one. When I create a new project, I can choose a &quot;.NET Core Class Library&quot; and I can also choose a &quot;.NET Standard Class Library&quot;. Now, as I understand it, the basic explanation is that .NET Standard is a baseline &quot;standard&quot; that may have multiple implementations - all of them have to provide the full API that .NET Standard specifies. And .NET Core is one of the implementations of .NET Standard, so that means that a .NET Core class library has access to everything that .NET Standard dictates must be available.. plus (potentially) a bit more. Now, what that &quot;bit more&quot; might entail isn&#39;t 100% clear to me. I guess that the short answer is the you would need to create a &quot;.NET Core Class Library&quot; if you want to reference something that uses APIs that only .NET Core (and not .NET Standard) surface.&lt;/p&gt;

&lt;p&gt;Another way to look at it is that it&#39;s best to start with a &quot;.NET Standard Class Library&quot; (rather than a &quot;.NET Core Class Library&quot;) unless you have a really compelling reason not to because more people / platforms / frameworks will be able to use the library that you use; .NET Standard assemblies may be referenced by .NET Core project and .NET Framework projects (and, if I have this right, Mono or Xamarin projects as well).&lt;/p&gt;

&lt;p&gt;I&#39;ve &lt;a href=&quot;http://quoteinvestigator.com/2013/03/06/artists-steal/&quot;&gt;stolen&lt;/a&gt; the following from an MSDN post by &lt;a href=&quot;https://social.msdn.microsoft.com/profile/Immo+Landwerth+[MSFT]&quot;&gt;Immo Landwerth&lt;/a&gt; that relates to this:&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;Unnecessary Optional instantiation&quot; src=&quot;http://www.productiverage.com/Content/Images/Posts/NETStandard.png&quot; class=&quot;NoBorder FullWidth&quot;&gt;&lt;/p&gt;

&lt;p&gt;However, this still leaves another problem. If you want access to more APIs then you might have to change from .NET Standard to .NET Core. &lt;em&gt;Or&lt;/em&gt; you might have be able to stick with .NET Standard but use a later version. If you create a .NET Standard Class Library then you can tell it what version of .NET Standard that you want to support by going to the project properties and changing the Target Framework. In general, if you&#39;re building a library for use by other people then you probably want to build it against the lowest version of .NET Standard possible. Maybe it&#39;s better to say the &quot;most accessible&quot; version of .NET Standard. If your library might be referenced by a project that targets .NET Standard 1.6 then it won&#39;t work if your library requires .NET Standard 2.0 (you&#39;ll force the consumer to require the later version or they&#39;ll decide not to use your library).&lt;/p&gt;

&lt;p&gt;Currently, .NET Core 1.1 and .NET Framework 4.6 implement .NET Standard 1.6 and so it&#39;s probably not the end of the world to take 1.6 as an acceptable minimum for .NET Standard libraries. But .NET Standard 2.0 &lt;a href=&quot;https://blogs.msdn.microsoft.com/dotnet/2017/05/10/announcing-net-core-2-0-preview-1/&quot;&gt;is in beta&lt;/a&gt; and I&#39;m not really sure what that will run on (will .NET Framework 4.6 be able to reference .NET Standard 2.0 or will we need 4.7?).. my point is that this is still quite confusing. That&#39;s not the fault of the tooling but it&#39;s still something you&#39;ll have to butt up against if you start going down the .NET Core / .NET Standard path.&lt;/p&gt;

&lt;p&gt;My final whinge about .NET Standard versions is that it&#39;s often hard to know &lt;em&gt;when&lt;/em&gt; to change version. While doing research for this post, I re-created one of my projects &lt;em&gt;again&lt;/em&gt; and tried to start with the minimum framework version each time. I had some reflection code that uses BindingFlags.GetField and it was refusing to compile. Because I was using .NET Standard 1.3. If I changed to .NET Standard 1.6 then it compiled fine. The problem is that it&#39;s hard to know what to do, it feels like a lot of guess work - do I need to change the .NET Standard version or do I need to switch to a .NET Core Class Library?&lt;/p&gt;

&lt;p&gt;Let me try and get more tightly focused on the tooling again. Earlier, I said that one of the plusses is that it&#39;s so easy to create NuGet packages with &quot;dotnet pack&quot;. One of the &lt;em&gt;problems&lt;/em&gt; (maybe &quot;mixed blessing&quot; would be more accurate) with this is that the packages are built entirely from metadata in the .csproj file. So you need to add any extra NuGet-specific information there. This actually works great - for example, here is one of my project files:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;Project Sdk=&quot;Microsoft.NET.Sdk&quot;&amp;gt;

  &amp;lt;PropertyGroup&amp;gt;
    &amp;lt;TargetFrameworks&amp;gt;netstandard1.6;net45&amp;lt;/TargetFrameworks&amp;gt;

    &amp;lt;PackageId&amp;gt;FullTextIndexer.Serialisation.Json&amp;lt;/PackageId&amp;gt;
    &amp;lt;PackageVersion&amp;gt;1.1.0&amp;lt;/PackageVersion&amp;gt;
    &amp;lt;Authors&amp;gt;ProductiveRage&amp;lt;/Authors&amp;gt;
    &amp;lt;Copyright&amp;gt;Copyright 2017 Productive Rage&amp;lt;/Copyright&amp;gt;
    &amp;lt;PackageTags&amp;gt;C# full text index search&amp;lt;/PackageTags&amp;gt;
    &amp;lt;PackageIconUrl&amp;gt;https://secure.gravatar.com/avatar/6a1f781d4d5e2d50dcff04f8f049767a?s=200&amp;lt;/PackageIconUrl&amp;gt;
    &amp;lt;PackageProjectUrl&amp;gt;https://bitbucket.org/DanRoberts/full-text-indexer&amp;lt;/PackageProjectUrl&amp;gt;
  &amp;lt;/PropertyGroup&amp;gt;

  &amp;lt;ItemGroup&amp;gt;
    &amp;lt;PackageReference Include=&quot;Newtonsoft.Json&quot; Version=&quot;10.0.2&quot; /&amp;gt;
  &amp;lt;/ItemGroup&amp;gt;

  &amp;lt;ItemGroup&amp;gt;
    &amp;lt;ProjectReference Include=&quot;..\FullTextIndexer.Common\FullTextIndexer.Common.csproj&quot; /&amp;gt;
    &amp;lt;ProjectReference Include=&quot;..\FullTextIndexer.Core\FullTextIndexer.Core.csproj&quot; /&amp;gt;
  &amp;lt;/ItemGroup&amp;gt;

  &amp;lt;ItemGroup Condition=&quot;&#39;$(TargetFramework)&#39; == &#39;netstandard1.6&#39;&quot;&amp;gt;
    &amp;lt;PackageReference Include=&quot;System.Reflection.TypeExtensions&quot;&amp;gt;
      &amp;lt;Version&amp;gt;4.3.0&amp;lt;/Version&amp;gt;
    &amp;lt;/PackageReference&amp;gt;
  &amp;lt;/ItemGroup&amp;gt;

&amp;lt;/Project&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I&#39;ve got everything I need; package id, author, copyright, icon, tags, .. My issue isn&#39;t how this works, it&#39;s that this doesn&#39;t seem to well documented. Searching on Google presents articles such as &lt;a href=&quot;https://docs.microsoft.com/en-us/nuget/guides/create-net-standard-packages-vs2017&quot;&gt;Create .NET standard packages with Visual Studio 2017&lt;/a&gt; which is very helpful but &lt;em&gt;doesn&#39;t&lt;/em&gt; link anywhere to a definitive list of what properties are and aren&#39;t supported. I came up with the above by hoping that it would work, calling &quot;dotnet pack&quot; and then examining the resulting .nupkg file in NuGet Package Explorer.&lt;/p&gt;

&lt;p&gt;My next beef is with unit testing. Earlier, I said that &quot;dotnet test&quot; is great because it executes the test against every framework that your project targets. And that &lt;em&gt;is&lt;/em&gt; great. But getting your unit test project to that point can be hard work. I like xUnit and they have a great article about &lt;a href=&quot;http://xunit.github.io/docs/getting-started-dotnet-core&quot;&gt;Getting started with xUnit.net (.NET Core / ASP.NET Core)&lt;/a&gt; but I dislike that there&#39;s copy-pasting into the .csproj file required to make it work, I wish that the GUI tooling was mature enough to be up to the job for people who wish to take that avenue. But it isn&#39;t. There is no way to do this without manually hacking about your .csproj file. I like that the command line interface is so solid but I&#39;m not sure that it&#39;s ok to &lt;em&gt;require&lt;/em&gt; that the CLI / manual-file-editing be used - .NET is such a well-established and well-used technology that not everyone wants to have to rely upon the CLI. I suspect that 90% of .NET users want Visual Studio to be able to everything for them because it has historically been able to - and I don&#39;t think that anyone should judge those people and tell them they&#39;re wrong and should embrace the CLI.&lt;/p&gt;

&lt;p&gt;To make things worse, in order to use xunit with .NET Core (or .NET Standard) you need to use pre-release versions of the libraries. Why? They&#39;ve been pre-release for a long time now, I find it hard to believe that they&#39;re not sufficiently stable / well-tested to make it to a &quot;real&quot; NuGet package release. Microsoft is suggesting that .NET Core is ready for mainstream use but other common dependencies aren&#39;t (this doesn&#39;t go for all major NuGet packages - AutoMapper, Json.NET and Dapper, for example, all work with .NET Standard without requiring pre-release versions).&lt;/p&gt;

&lt;p&gt;Oh, one more thing about unit tests (with xunit, at least) - after you follow the instructions and get the tests recognised in the VS Test Explorer, they only get run for one framework. I&#39;m not sure which, if you specify multiple. Which is disappointing. Since the CLI is so good and runs tests for all supported frameworks, I wish that the Test Explorer integration would as well.&lt;/p&gt;

&lt;p&gt;Last bugbear: When I create a .NET Framework Web Project and run it and see the result in the browser, so long as I have disabled &quot;Enable Edit and Continue&quot; in the Project Properties / Web pane then I can make changes, rebuild and then refresh in the browser without &quot;running&quot; (ie. attaching the debugger). This often shortens the edit-build-retry cycle (sometimes only slightly but sometimes by a few valuable seconds) but it&#39;s something I can&#39;t reproduce with .NET Core Web Projects; once the project is stopped, I can&#39;t refresh the page in the browser until I tell VS to run again. Why can&#39;t it leave the site running in IIS Express??&lt;/p&gt;

&lt;h3&gt;The Ugly&lt;/h3&gt;

&lt;p&gt;I&#39;ve been trying to find succinct examples of this problem while writing this article and I&#39;ve failed.. While looking into VS2017 tooling changes, I migrated my &quot;&lt;a href=&quot;https://bitbucket.org/DanRoberts/full-text-indexer&quot;&gt;Full Text Indexer&lt;/a&gt;&quot; code across. It&#39;s not a massive project by any means but it spans multiple projects within the same solution and builds NuGet packages for consumption by both .NET Standard and .NET Framework. Last year, I got it working with the VS2015 tooling and the project.json format. This year, I changed it to use the new .csproj format and got it building nicely in VS2017. One of the most annoying things that I found during this migration was that I would make change to projects (sometimes having to edit the project files directly) and the changes would refuse to apply themselves without me restarting VS (probably closing and re-opening the solution would have done it too). This was very frustrating. More frustrating at this very minute, frankly, is that I&#39;m unable to start a clean project and come up with an example of having to restart VS to get a change applied. But the feeling that I was left with was that the Visual Studio tooling was flakey. If I built everything using the CLI then it was fine - another case where I felt that if you don&#39;t mind manual editing and the command line then you&#39;ll be fine; but that&#39;s not, in my mind, a .NET release that is ready for &quot;prime time&quot;.&lt;/p&gt;

&lt;p&gt;Another flakey issue I had is that I have a &quot;FullTextIndexer&quot; project that doesn&#39;t have any code of its own, it only exists to generate a single NuGet package that pulls in the other five projects / packages in one umbrella add-this-and-you-get-everything package. When I first created the project and used &quot;dotnet pack&quot; then the resulting package only listed the five dependencies for .NET Standard and &lt;em&gt;not&lt;/em&gt; for .NET Framework. I couldn&#39;t work out what was causing the problem.. then it went away! I couldn&#39;t put my finger on anything that had changed but it started emitting correct packages (with correct dependencies) at some point. I had another problem building my unit test project because one of the referenced projects needed the &quot;System.Text.RegularExpressions&quot; package when built as .NET Standard and it complained that it couldn&#39;t load version 4.1.1.0. One of the projects reference 4.3.1.0 but I could never find where the 4.1.1.0 requirement came in and I couldn&#39;t find any information about assembly binding like I&#39;m used to in MVC projects (where the web.config will say &quot;for versions x through y, just load y&quot;). This problem, also, just disappeared and I couldn&#39;t work out what had happened to make it go away.&lt;/p&gt;

&lt;p&gt;In my multi-framework-targeting example solution, I have some conditional compilation statements. I use the Full Text Indexer to power the search on this blog and I serialise a search index using the BinaryFormatter. In order to do this, the search index classes have to have the [Serializable] attribute. But this attribute is not available in .NET Standard.. So the .NET Standard builds of the Full Text Indexer don&#39;t have [Serializable] attributes, while the .NET Framework builds &lt;em&gt;do&lt;/em&gt; have it. That way I can produce nice, clean, new .NET Standard libraries without breaking backwards compatibility for .NET Framework consumers (like my Blog). To do this end, I have code like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#if NET45
  [Serializable]
#endif
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I have two problems with this. Firstly, the conditional compile strings are a little bit &quot;magic&quot; and are not statically analysed. If, for example, you change &quot;#if NET45&quot; to &quot;#if net45&quot; then the code would not be included in .NET Framwork 4.5 builds. You wouldn&#39;t get any warning or indication of this, it would happen silently. Similarly, if your project builds for &quot;netstandard1.6&quot; and &quot;net45&quot; and you include a conditional &quot;#if NET452&quot; then that condition will never be met because you should have used &quot;NET45&quot; and not &quot;NET452&quot;. Considering the fact that I use languages like C# that are statically typed so that the compiler can identify silly mistakes like this that I might make, this is frustrating when I get it wrong. The second issue I have is that the conditional statement highlighting is misleading when the debugger steps through code. If I have a project that has target frameworks &quot;netstandard1.6;net45&quot; and I reference this from a .NET Framework Console Application and I step through into the library code, any &quot;#if NET45&quot; code will appear &quot;disabled&quot; in the IDE when, really, that code is in play. That&#39;s misleading and makes me sad.&lt;/p&gt;

&lt;h3&gt;To summarise..?&lt;/h3&gt;

&lt;p&gt;I&#39;m really impressed with how much better the experience has been in writing .NET Core / .NET Standard projects (or projects that build for .NET Core / Standard &lt;em&gt;and&lt;/em&gt; &quot;full fat&quot; Framework). However.. I&#39;m just still not that confident that the technology is mature yet. I&#39;ve encountered too many things that work ok only 95% of the time - and this makes me think that if I tried to encourage everyone at work to adopt .NET Core / Standard today then I&#39;d regret it. There would just be too many occurrences where someone would hit a &quot;weird issue that may or may not go away.. and if it does then we&#39;re not sure why&quot; problems.&lt;/p&gt;

&lt;p&gt;I think that the future is bright for .NET Core.. but it seems like the last two years have permanently had us feeling that &quot;in just six months or so, .NET Core will be ready to rock&#39;n&#39;roll&quot;. And things like &quot;&lt;a href=&quot;https://www.theregister.co.uk/2017/05/09/dot_net_compatibility/&quot;&gt;ASP.NET Core 2.0 won&#39;t be supported on .NET Framework&lt;/a&gt;&quot; mix-ups don&#39;t help (TL;DR: It was said that &quot;ASP.NET Core&quot; wouldn&#39;t work on the &quot;full fat&quot; .NET Framework and that that was by design - but then it turned out that this was a miscommunication and that it would only be a temporary sitation and ASP.NET Core &lt;em&gt;would&lt;/em&gt; work within .NET Framework as well).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;To sumarise the summary:&lt;/strong&gt; I hope to move to .NET Core in the foreseeable future. But, professionally, I&#39;m not going to today (personal projects maybe, but not at work).&lt;/p&gt;</description>
			<pubDate>Mon, 19 Jun 2017 19:55:00 GMT</pubDate>
		</item>

	</channel>

</rss>