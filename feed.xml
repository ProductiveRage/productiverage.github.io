<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="/Content/RSS.xslt" type="text/xsl" media="screen"?>
<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">

	<channel>
		
		<title>Productive Rage</title>
		<link>http://www.productiverage.com/</link>
		<atom:link href="http://www.productiverage.com/feed" rel="self" type="application/rss+xml"/>
		<description>Dan's techie ramblings</description>
		<language>en-gb</language>

		<lastBuildDate>Wed, 26 Jul 2017 20:31:00 GMT</lastBuildDate>
		<docs>http://blogs.law.harvard.edu/tech/rss</docs>
		
		<image>
			<title>Productive Rage</title>
			<url>http://www.productiverage.com/Content/Images/Grouch.jpg</url>
			<width>142</width>
			<height>142</height>
			<link>http://www.productiverage.com/</link>
		</image>

		<xhtml:meta xmlns:xhtml="http://www.w3.org/1999/xhtml" name="robots" content="noindex" /> 

		<item>
			<title>Trying to set a readonly auto-property value externally (plus, a little BenchmarkDotNet)</title>
            <link>http://www.productiverage.com/trying-to-set-a-readonly-autoproperty-value-externally-plus-a-little-benchmarkdotnet</link>
			<guid>http://www.productiverage.com/trying-to-set-a-readonly-autoproperty-value-externally-plus-a-little-benchmarkdotnet</guid>
			<description>&lt;p&gt;If you&#39;re thinking that you should try changing a readonly property.. well, in short, you almost certainly shouldn&#39;t try.&lt;/p&gt;

&lt;p&gt;For example, the following class has a property that should &lt;em&gt;only&lt;/em&gt; be set in its constructor and then never mutated again -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public sealed class Example
{
  public Example(int id)
  {
    Id = id;
  }
  public int Id { get; }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;And it is a good thing&lt;/em&gt; that we are able to write code so easily that communicates precisely when a property may (and may not) change.&lt;/p&gt;

&lt;p&gt;However..&lt;/p&gt;

&lt;p&gt;You might have some very particular scenario in mind where you really &lt;em&gt;do&lt;/em&gt; want to try to write to a readonly auto-property&#39;s value for an instance that has already been created. It&#39;s possible that you are writing some interesting deserialisation code, I suppose. For something that I was looking at, I was curious to look into how feasible it is (or isn&#39;t) and I came up with the following three basic approaches.&lt;/p&gt;

&lt;p&gt;I think that each approach demonstrates something a little off the beaten track of .NET - granted, there&#39;s absolutely nothing here that&#39;s never been done before.. but sometimes it&#39;s fun to be reminded of how flexible .NET &lt;em&gt;can&lt;/em&gt; be, if only to appreciate how hard it works to keep everything reliable and consistent.&lt;/p&gt;

&lt;h3&gt;TL;DR&lt;/h3&gt;

&lt;p&gt;I&#39;ll show three approaches, in decreasing order of ease of writing. They all depend upon a particular naming conventions in .NET&#39;s internals that is not documented and should not be considered reliable (ie. a future version of C# and/or the compiler could break it). Even if you ignore this potential time bomb, only the first of the three methods will actually work. Like I said at the start, this is something that you almost certainly shouldn&#39;t be attempting anyway!&lt;/p&gt;

&lt;h3&gt;Approach 1: Reflection (with some guesswork)&lt;/h3&gt;

&lt;p&gt;C# 6 introduced read-only auto-properties. Before thoses were available, you had two options to do something similar. You could use a private setter -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public sealed class Example
{
  public Example(int id)
  {
    Id = id;
  }
  public int Id { get; private set; }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. or you could manually create a private readonly backing field for the property -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public sealed class Example
{
  private readonly int _id;
  public Example(int id)
  {
    _id = id;
  }
  public int Id { get { return _id; } }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The first approach requires less code but the guarantees that it claims to make are less strict. When a field is readonly then it may &lt;em&gt;only&lt;/em&gt; be set within a constructor but when it has a private setter then it could feasibly change &lt;em&gt;at any point&lt;/em&gt; in the lifetime of the instance. In the class above, it&#39;s clear to see that it &lt;em&gt;is&lt;/em&gt; only set in the constructor but there are no compiler assurances that someone won&#39;t come along and add a method to the &lt;strong&gt;Example&lt;/strong&gt; class that mutates the private-setter &quot;Id&quot; property. If you have a readonly &quot;_id&quot; backing field then it would not be possible to write a method to mutate the value*.&lt;/p&gt;

&lt;p&gt;* &lt;em&gt;(Without resorting to the sort of shenanigans that we are going to look at here)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;So the second class is more reliable and more accurately conveys the author&#39;s intentions for the code (that the &quot;Id&quot; property of an &lt;strong&gt;Example&lt;/strong&gt; instance will never change during its lifetime). The disadvantage is that there is more code to write.&lt;/p&gt;

&lt;p&gt;The C# 6 syntax is the best of both worlds - as short (shorter, in fact, since there is &lt;em&gt;no&lt;/em&gt; setter defined) as the first version but with the stronger guarantees of the second version.&lt;/p&gt;

&lt;p&gt;Interestingly, the compiler generates IL that is essentially identical to that which result from the C# 5 syntax where you manually define a property that backs onto a readonly field. The only real difference relates to the fact that it wants to be sure that it can inject a readonly backing field whose name won&#39;t clash with any other field that the human code writer may have added to the class. To do this, it uses characters in the generated field names that are not valid to appear in C#, such as &quot;&amp;lt;Id&amp;gt;k__BackingField&quot;. The triangle brackets may not be used in C# code but they may be used in the IL code that the compiler generates. And, just to make things extra clear, it adds a &lt;strong&gt;[CompilerGenerated]&lt;/strong&gt; attribute to the backing field.&lt;/p&gt;

&lt;p&gt;This is sufficient information for us to try to identify the compiler-generated backing field using reflection. Going back to this version of the class:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public sealed class Example
{
  public Example(int id)
  {
    Id = id;
  }
  public int Id { get; }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. we can identify the backing field for the &quot;Id&quot; property with the following code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var type = typeof(Example);
var property = type.GetProperty(&quot;Id&quot;);

var backingField = type
  .GetFields(BindingFlags.NonPublic | BindingFlags.Instance | BindingFlags.Static)
  .FirstOrDefault(field =&amp;gt;
    field.Attributes.HasFlag(FieldAttributes.Private) &amp;amp;&amp;amp;
    field.Attributes.HasFlag(FieldAttributes.InitOnly) &amp;amp;&amp;amp;
    field.CustomAttributes.Any(attr =&amp;gt; attr.AttributeType == typeof(CompilerGeneratedAttribute)) &amp;amp;&amp;amp;
    (field.DeclaringType == property.DeclaringType) &amp;amp;&amp;amp;
    field.FieldType.IsAssignableFrom(property.PropertyType) &amp;amp;&amp;amp;
    field.Name.StartsWith(&quot;&amp;lt;&quot; + property.Name + &quot;&amp;gt;&quot;)
  );    
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With this backingField reference, we can start doing devious things. Like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Create an instance with a readonly auto-property
var x = new Example(123);
Console.WriteLine(x.Id); // Prints &quot;123&quot;

// Now change the value of that readonly auto-property!
backingField.SetValue(x, 456);
Console.WriteLine(x.Id); // Prints &quot;456&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We took an instance of a class that has a readonly property (meaning that it should never change after the instance has been constructed) and we &lt;em&gt;changed&lt;/em&gt; that property. Evil.&lt;/p&gt;

&lt;p&gt;One more time, though: this relies upon the current convention that the compiler-generated backing fields follow a particular naming convention. If that changes one day then this code will fail.&lt;/p&gt;

&lt;p&gt;Enough with the boring warnings, though - let&#39;s get to the real nub of the matter; reflection is slooooooooow, isn&#39;t it? Surely we should never resort to such a clunky technology??&lt;/p&gt;

&lt;h3&gt;Approach 2: Using LINQ Expressions to generate fast code to set the field&lt;/h3&gt;

&lt;p&gt;If &lt;strong&gt;Example&lt;/strong&gt; had a regular private field that we wanted to set - eg.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public sealed class Example
{
  private int _somethingElse;
  public Example(int id, int somethingElse)
  {
    Id = id;
    _somethingElse = somethingElse;
  }

  public int Id { get; }

  public int GetSomethingElse()
  {
    return _somethingElse;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then we could use reflection to get a reference to that field once and build a delegate using LINQ Expressions that would allow us to update that field value using something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var field = typeof(Example).GetField(&quot;_somethingElse&quot;, BindingFlags.Instance | BindingFlags.NonPublic);

var sourceParameter = Expression.Parameter(typeof(Example), &quot;source&quot;);
var valueParameter = Expression.Parameter(field.FieldType, &quot;value&quot;);
var fieldSetter =
  Expression.Lambda&amp;lt;Action&amp;lt;Example, int&amp;gt;&amp;gt;(
    Expression.Assign(
      Expression.MakeMemberAccess(sourceParameter, field),
      valueParameter
    ),
    sourceParameter,
    valueParameter
  )
  .Compile();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We could then cache that &quot;fieldSetter&quot; delegate and call it any time that we wanted to update the private &quot;_somethingElse&quot; field on an &lt;strong&gt;Example&lt;/strong&gt; instance. There would be a one-off cost to the reflection that identifies the field and a one-off cost to generating that delegate initially but any subsequent call should be comparably quick to hand-written field-updating code (obviously it&#39;s not possible to hand-write code to update a private field from outside the class.. but you get the point).&lt;/p&gt;

&lt;p&gt;There&#39;s one big problem with this approach, though; it doesn&#39;t work for readonly fields. The &quot;Expression.Assign&quot; call will throw an &lt;strong&gt;ArgumentException&lt;/strong&gt; if the specified member is readonly:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Expression must be writeable&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;SAD FACE.&lt;/p&gt;

&lt;p&gt;This is quite unfortunate. It had been a little while since I&#39;d played around with LINQ Expressions and I was feeling quite proud of myself getting the code to work.. only to fall at the last hurdle.&lt;/p&gt;

&lt;p&gt;Never mind.&lt;/p&gt;

&lt;p&gt;One bright side is that I also tried out this code in a .NET Core application and it worked to the same extent as the &quot;full fat&quot; .NET Framework - ie. I was able to generate a delegate using LINQ Expressions that would set a non-readonly private field on an instance. Considering that reflection capabilities were limited in the early days of .NET Standard, I found it a nice surprise that support seems so mature now.&lt;/p&gt;

&lt;h3&gt;Approach 3: Emitting IL&lt;/h3&gt;

&lt;p&gt;Time to bring out the big guns!&lt;/p&gt;

&lt;p&gt;If the friendlier way of writing code that dynamically compiles other .NET code (ie. LINQ Expressions) wouldn&#39;t cut it, surely the old fashioned (and frankly intimidating) route of writing code to directly emit IL would do the job?&lt;/p&gt;

&lt;p&gt;It&#39;s been a long time since I&#39;ve written any IL-generating code, so let&#39;s take it slow. If we&#39;re starting with the case that worked with LINQ Expressions then we want to create a method that will take an &lt;strong&gt;Example&lt;/strong&gt; instance and an &lt;strong&gt;int&lt;/strong&gt; value in order to set the &quot;_somethingElse&quot; field on the &lt;strong&gt;Example&lt;/strong&gt; instance to that new number.&lt;/p&gt;

&lt;p&gt;The first thing to do is to create some scaffolding. The following code is almost enough to create a new method of type &lt;strong&gt;Action&amp;lt;Example, int&amp;gt;&lt;/strong&gt; -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Set restrictedSkipVisibility to true to avoid any pesky &quot;visibility&quot; checks being made (in other
// words, let the IL in the generated method access any private types or members that it tries to)
var method = new DynamicMethod(
  name: &quot;SetSomethingElseField&quot;,
  returnType: null,
  parameterTypes: new[] { typeof(Example), typeof(int) },
  restrictedSkipVisibility: true
);

var gen = method.GetILGenerator();

// TODO: Emit require IL op codes here..

var fieldSetter = (Action&amp;lt;Example, int&amp;gt;)method.CreateDelegate(typeof(Action&amp;lt;Example, int&amp;gt;));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The only problem is that &quot;TODO&quot; section.. the bit where we have to know what IL to generate.&lt;/p&gt;

&lt;p&gt;There are basically two ways you can go about working out what to write here. You can learn enough about IL (and remember it again years after you learn some!) that you can just start hammering away at the keyboard.. or you can write some C# that basically does what you want, compile that using Visual Studio and then use a disassembler to see what IL is produced. I&#39;m going for plan b. Handily, if you use Visual Studio then you probably already have a disassembler installed! It&#39;s called ildasm.exe and I found it on my computer in &quot;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.6.1 Tools&quot; after reading this: &quot;&lt;a href=&quot;https://blogs.msdn.microsoft.com/lucian/2008/11/14/where-are-the-sdk-tools-where-is-ildasm/&quot;&gt;Where are the SDK tools? Where is ildasm?&lt;/a&gt;&quot;.&lt;/p&gt;

&lt;p&gt;To make things as simple as possible, I created a new class in a C# project -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class SomethingWithPublicField
{
  public int Id;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and then created a static method that I would want to look at the disassembly of:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;static void MethodToCopy(SomethingWithPublicField source, int value)
{
  source.Id = value;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I compiled the console app, opened the exe in ildasm and located the method. Double-clicking it revealed this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;.method private hidebysig static void  MethodToCopy(class Test.Program/SomethingWithPublicField source,
                                                    int32 &#39;value&#39;) cil managed
{
  // Code size         9 (0x9)
  .maxstack  8
  IL_0000:  nop
  IL_0001:  ldarg.0
  IL_0002:  ldarg.1
  IL_0003:  stfld      int32 Test.Program/SomethingWithPublicField::Id
  IL_0008:  ret
} // end of method Program::MethodToCopyTyped
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ok. That actually couldn&#39;t be much simpler. The &quot;ldarg.0&quot; code means &quot;load argument 0 onto the stack&quot;, &quot;ldarg.1&quot; means &quot;load argument 1 onto the stack&quot; and &quot;stfld&quot; means take the instance of the first object on the stack and set the specified field to be the second object on the stack. &quot;ret&quot; just means exit method (returning any value, if there is one - which there isn&#39;t in this case).&lt;/p&gt;

&lt;p&gt;This means that the &quot;TODO&quot; comment in my scaffolding code may be replaced with real content, resulting in the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var field = typeof(Example).GetField(&quot;_somethingElse&quot;, BindingFlags.Instance | BindingFlags.NonPublic);

var method = new DynamicMethod(
  name: &quot;SetSomethingElseField&quot;,
  returnType: null,
  parameterTypes: new[] { typeof(Example), typeof(int) },
  restrictedSkipVisibility: true
);
var gen = method.GetILGenerator();
gen.Emit(OpCodes.Ldarg_0);
gen.Emit(OpCodes.Ldarg_1);
gen.Emit(OpCodes.Stfld, field);
gen.Emit(OpCodes.Ret);

var fieldSetter = (Action&amp;lt;Example, int&amp;gt;)method.CreateDelegate(typeof(Action&amp;lt;Example, int&amp;gt;));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That&#39;s it! We now have a delegate that is a compiled method for writing a new &lt;strong&gt;int&lt;/strong&gt; into the private field &quot;_somethingElse&quot; for any given instance of &lt;strong&gt;Example&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Unfortunately, things go wrong at exactly the same point as they did with LINQ Expressions. The above code works fine for setting a regular private field but if we tried to set a &lt;em&gt;readonly&lt;/em&gt; field using the same approach then we&#39;d be rewarded with an error:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;System.Security.VerificationException&lt;/strong&gt;: &#39;Operation could destabilize the runtime.&#39;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Another disappointment!*&lt;/p&gt;

&lt;p&gt;* &lt;em&gt;(Though hopefully not a surprise if you&#39;re reading this article since I said right at the top that only the first of these three approaches would work!)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;But, again, to try to find a silver lining, I also tried the non-readonly-private-field-setting-via-emitted-IL code in a .NET Core application and I was pleasantly surprised to find that it worked. It required the packages &quot;System.Reflection.Emit.ILGeneration&quot; and &quot;System.Reflection.Emit.Lightweight&quot; to be added through NuGet but nothing more difficult than that.&lt;/p&gt;

&lt;p&gt;Although I decided last month that &lt;a href=&quot;http://www.productiverage.com/revisiting-net-core-tooling-visual-studio-2017&quot;&gt;I&#39;m still not convinced that .NET Core is ready for me to use in work&lt;/a&gt;, I &lt;em&gt;am&lt;/em&gt; impressed by how much &lt;em&gt;does&lt;/em&gt; work with it.&lt;/p&gt;

&lt;h3&gt;Performance comparison&lt;/h3&gt;

&lt;p&gt;So we&#39;ve ascertained that there is only one way* to set a readonly field on an existing instance and, regrettably, it&#39;s also the slowest. I guess that a pertinent question to ask, though, is just &lt;em&gt;how much&lt;/em&gt; slower is the slowest?&lt;/p&gt;

&lt;p&gt;* &lt;em&gt;(As further evidence that there isn&#39;t another way around this, I&#39;ve found an issue from EntityFramework&#39;s GitHub repo: &quot;&lt;a href=&quot;https://github.com/aspnet/EntityFramework/issues/6202&quot;&gt;Support readonly fields&lt;/a&gt;&quot; which says that it&#39;s possible to set a readonly property with reflection but that the issue-raiser encountered the same two failures that I&#39;ve demonstrated above when he tried alternatives and no-one has proposed any other ways to tackle it)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Obviously we can&#39;t compare the readonly-field-setting performance of the three approaches above because only one of them is actually capable of doing that. But we &lt;em&gt;can&lt;/em&gt; compare the performance of something similar; setting a private (but not readonly) field, since &lt;em&gt;all three&lt;/em&gt; are able to achieve that.&lt;/p&gt;

&lt;p&gt;Ordinarily at this point, I would write some test methods and run them in a loop and time the loop and divide by the number of runs and then maybe repeat a few times for good measure and come up with a conclusion. Today, though, I thought that I might try something a bit different because I recently heard again about something called &quot;&lt;a href=&quot;https://github.com/dotnet/BenchmarkDotNet&quot;&gt;BenchmarkDotNet&lt;/a&gt;&quot;. It claims that:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Benchmarking is really hard (especially microbenchmarking), you can easily make a mistake during performance measurements. BenchmarkDotNet will protect you from the common pitfalls (even for experienced developers) because it does all the dirty work for you: it generates an isolated project per each benchmark method, does several launches of this project, run multiple iterations of the method (include warm-up), and so on. Usually, you even shouldn&#39;t care about a number of iterations because BenchmarkDotNet chooses it automatically to achieve the requested level of precision.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This sounds ideal for my purposes!&lt;/p&gt;

&lt;p&gt;What I&#39;m most interesting in is how reflection compares to compiled LINQ expressions and to emitted IL when it comes to setting a private field. If this is of any importance whatsoever then presumably the code will be run over and over again and so it should be the execution time of the compiled property-setting code that is of interest - the time taken to actually compile the LINQ expressions / emitted IL can probably be ignored as it should disappear into insignificance when the delegates are called enough times. But, for a sense of thoroughness (and because BenchmarkDotNet makes it so easy), I&#39;ll &lt;em&gt;also&lt;/em&gt; measure the time that it takes to do the delegate compilation as well.&lt;/p&gt;

&lt;p&gt;To do this, I created a .NET Core Console application in VS2017, added the BenchmarkDotNet NuGet package and changed the .csproj file by hand to build for both .NET Core and .NET Framework 4.6.1 by changing&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;TargetFramework&amp;gt;netcoreapp1.1&amp;lt;/TargetFramework&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;to&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;TargetFrameworks&amp;gt;netcoreapp1.1;net461&amp;lt;/TargetFrameworks&amp;gt;
&amp;lt;PlatformTarget&amp;gt;AnyCPU&amp;lt;/PlatformTarget&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(as described in the &lt;a href=&quot;https://github.com/dotnet/BenchmarkDotNet/blob/master/docs/guide/FAQ.md&quot;&gt;BenchmarkDotNet FAQ&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Then I put the following together. There are six benchmarks in total; three to measure the creation of the different types of property-setting delegates and three to then measure the execution time of those delegates -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class Program
{
    static void Main(string[] args)
    {
        BenchmarkRunner.Run&amp;lt;TimedSetter&amp;gt;();
        Console.ReadLine();
    }
}

[CoreJob, ClrJob]
public class TimedSetter
{
    private SomethingWithPrivateField _target;
    private FieldInfo _field;
    private Action&amp;lt;SomethingWithPrivateField, int&amp;gt;
        _reflectionSetter,
        _linqExpressionSetter,
        _emittedILSetter;

    [GlobalSetup]
    public void GlobalSetup()
    {
        _target = new SomethingWithPrivateField();

        _field = typeof(SomethingWithPrivateField)
            .GetFields(BindingFlags.NonPublic | BindingFlags.Instance)
            .FirstOrDefault(f =&amp;gt; f.Name == &quot;_id&quot;);

        _reflectionSetter = ConstructReflectionSetter();
        _linqExpressionSetter = ConstructLinqExpressionSetter();
        _emittedILSetter = ConstructEmittedILSetter();
    }

    [Benchmark]
    public Action&amp;lt;SomethingWithPrivateField, int&amp;gt; ConstructReflectionSetter()
    {
        return (source, value) =&amp;gt; _field.SetValue(source, value);
    }

    [Benchmark]
    public Action&amp;lt;SomethingWithPrivateField, int&amp;gt; ConstructLinqExpressionSetter()
    {
        var sourceParameter = Expression.Parameter(typeof(SomethingWithPrivateField), &quot;source&quot;);
        var valueParameter = Expression.Parameter(_field.FieldType, &quot;value&quot;);
        var fail = Expression.Assign(
            Expression.MakeMemberAccess(sourceParameter, _field),
            valueParameter
        );
        return Expression.Lambda&amp;lt;Action&amp;lt;SomethingWithPrivateField, int&amp;gt;&amp;gt;(
                Expression.Assign(
                    Expression.MakeMemberAccess(sourceParameter, _field),
                    valueParameter
                ),
                sourceParameter,
                valueParameter
            )
            .Compile();
    }

    [Benchmark]
    public Action&amp;lt;SomethingWithPrivateField, int&amp;gt; ConstructEmittedILSetter()
    {
        var method = new DynamicMethod(
            name: &quot;SetField&quot;,
            returnType: null,
            parameterTypes: new[] { typeof(SomethingWithPrivateField), typeof(int) },
            restrictedSkipVisibility: true
        );
        var gen = method.GetILGenerator();
        gen.Emit(OpCodes.Ldarg_0);
        gen.Emit(OpCodes.Ldarg_1);
        gen.Emit(OpCodes.Stfld, _field);
        gen.Emit(OpCodes.Ret);
        return (Action&amp;lt;SomethingWithPrivateField, int&amp;gt;)method.CreateDelegate(
          typeof(Action&amp;lt;SomethingWithPrivateField, int&amp;gt;)
        );
    }

    [Benchmark]
    public void SetUsingReflection()
    {
        _reflectionSetter(_target, 1);
    }

    [Benchmark]
    public void SetUsingLinqExpressions()
    {
        _linqExpressionSetter(_target, 1);
    }

    [Benchmark]
    public void SetUsingEmittedIL()
    {
        _emittedILSetter(_target, 1);
    }
}

public class SomethingWithPrivateField
{
    private int _id;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &quot;GlobalSetup&quot; method will be run once and will construct the delegates for delegate-executing benchmark methods (&quot;SetUsingReflection&quot;, &quot;SetUsingLinqExpressions&quot; and &quot;SetUsingEmittedIL&quot;). The time that it takes to execute the [GlobalSetup] method does not contribute to any of the benchmark method times - the benchmark methods will record &lt;em&gt;only&lt;/em&gt; their own execution time.&lt;/p&gt;

&lt;p&gt;However, having delegate-creation benchmark methods (&quot;ConstructReflectionSetter&quot;, &quot;ConstructLinqExpressionSetter&quot; and &quot;ConstructEmittedILSetter&quot;) means that I&#39;ll have an idea how large the initial cost to construct each delegate is (or isn&#39;t), separate to the cost of executing each type of delegate.&lt;/p&gt;

&lt;p&gt;BenchmarkDotNet has capabilities beyond what I&#39;ve taken advantage of. For example, it can also build for Mono (though I don&#39;t have Mono installed on my computer, so I didn&#39;t try this) and it can test 32-bit vs 64-bit builds.&lt;/p&gt;

&lt;p&gt;Aside from testing .NET Core 1.1 and .NET Framework 4.6.1, I&#39;ve kept things fairly simple.&lt;/p&gt;

&lt;p&gt;After it has run, it emits the following summary about my computer:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;BenchmarkDotNet=v0.10.8, OS=Windows 8.1 (6.3.9600)&lt;/p&gt;
  
  &lt;p&gt;Processor=AMD FX(tm)-8350 Eight-Core Processor, ProcessorCount=8&lt;/p&gt;
  
  &lt;p&gt;Frequency=14318180 Hz, Resolution=69.8413 ns, Timer=HPET&lt;/p&gt;
  
  &lt;p&gt;dotnet cli version=1.0.4&lt;/p&gt;
  
  &lt;p&gt;[Host] : .NET Core 4.6.25211.01, 64bit RyuJIT [AttachedDebugger]&lt;/p&gt;
  
  &lt;p&gt;Clr    : Clr 4.0.30319.42000, 64bit RyuJIT-v4.6.1087.0&lt;/p&gt;
  
  &lt;p&gt;Core   : .NET Core 4.6.25211.01, 64bit RyuJIT&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;And produces the following table:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;                        Method |  Job | Runtime |           Mean |         Error |        StdDev |
------------------------------ |----- |-------- |----------------|---------------|---------------|
     ConstructReflectionSetter |  Clr |     Clr |       9.980 ns |     0.2930 ns |     0.4895 ns |
 ConstructLinqExpressionSetter |  Clr |     Clr | 149,552.853 ns | 1,752.4151 ns | 1,639.2100 ns |
      ConstructEmittedILSetter |  Clr |     Clr | 126,454.797 ns | 1,143.9593 ns | 1,014.0900 ns |
            SetUsingReflection |  Clr |     Clr |     158.784 ns |     3.1892 ns |     3.6727 ns |
       SetUsingLinqExpressions |  Clr |     Clr |       1.139 ns |     0.0542 ns |     0.0742 ns |
             SetUsingEmittedIL |  Clr |     Clr |       1.832 ns |     0.0689 ns |     0.1132 ns |
                               |      |         |                |               |               |
     ConstructReflectionSetter | Core |    Core |       9.465 ns |     0.1083 ns |     0.0904 ns |
 ConstructLinqExpressionSetter | Core |    Core |  66,430.408 ns | 1,303.5243 ns | 2,104.9488 ns |
      ConstructEmittedILSetter | Core |    Core |  38,483.764 ns |   605.3819 ns |   536.6553 ns |
            SetUsingReflection | Core |    Core |   2,626.527 ns |    24.1110 ns |    22.5534 ns |
       SetUsingLinqExpressions | Core |    Core |       1.063 ns |     0.0516 ns |     0.0688 ns |
             SetUsingEmittedIL | Core |    Core |       1.718 ns |     0.0599 ns |     0.0560 ns |
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The easiest thing to interpret is the &quot;Mean&quot; - BenchmarkDotNet did a few &quot;pilot runs&quot; to try to see how long the benchmark methods would take and then tries to decide what is an appropriate number of runs to do for real in order to get reliable results.&lt;/p&gt;

&lt;p&gt;The short version is that when delegates are compiled using LINQ Expressions and emitted-IL that they both execute &lt;em&gt;a lot&lt;/em&gt; faster than reflection; over 85x faster for .NET Framework 4.6.1 and 1,500x faster for .NET Core 1.1!&lt;/p&gt;

&lt;p&gt;The huge difference between reflection and the other two approaches, though, may slightly overshadow the fact that the LINQ Expression delegates are actually about 1.6x faster than the emitted-IL delegates. I hadn&#39;t expected this at all, I would have thought that they would be almost identical - in fact, I&#39;m still surprised and don&#39;t currently have any explanation for it.&lt;/p&gt;

&lt;p&gt;The mean value doesn&#39;t usually tell the whole story, though. When looking at the mean, it&#39;s also useful to look at the Standard Deviation (&quot;StdDev&quot; in the table above). The mean might be within a small spread of values or a very large spread of values. A small spread is better because it suggests that the single mean value that we&#39;re looking at is representative of behaviour in the real world and that values aren&#39;t likely to vary too wildly - a large standard deviation means that there was much more variation in the recorded values and so the times could be all over the place in the real world. (Along similar lines, the &quot;Error&quot; value is described as being &quot;Half of 99.9% confidence interval&quot; - again, the gist is that smaller values suggest that the mean is a more useful indicator of what we would see in the real world for any given request).&lt;/p&gt;

&lt;p&gt;What I&#39;ve ignored until this point are the &quot;ConstructReflectionSetter&quot; / &quot;ConstructLinqExpressionSetter&quot; / &quot;ConstructEmittedILSetter&quot; methods. If we first look at the generation of the LINQ Expression delegate on .NET 4.6.1, we can see that the mean time to generate that delegate was around 150ms - compared to approx 10ms for the reflection delegate. Each time the LINQ Expressions delegate is used to set the field instead of the reflection delegate we save around 0.16ms. That means that we need to call the delegate around 950 times in order to pay of the cost of constructing it!&lt;/p&gt;

&lt;p&gt;As I suggested earlier, it would only make sense to investigate these sort of optimisations if you expect to execute the code over and over and over again (otherwise, why not just keep it simple and stick to using plain old reflection).. but it&#39;s still useful to have the information about just how much &quot;upfront cost&quot; there is to things like this, compared to how much you hope to save in the long run.&lt;/p&gt;

&lt;p&gt;It&#39;s also interesting to see the discrepancies between .NET Framework 4.6.1 and .NET Core 1.1 - the times to compile LINQ Expressions and emitted-IL delegates are noticeably shorter and the time to set the private field by reflection noticeably longer. In fact, these differences mean that you only need to set the field 25 times before you start to offset the cost of creating the LINQ Expressions delegate (when you compare it to updating the field using reflection) and only 14 times to offset the cost of creating the emitted-IL delegate!&lt;/p&gt;

&lt;h3&gt;BenchmarkDotNet is fun&lt;/h3&gt;

&lt;p&gt;I&#39;m really happy with how easy BenchmarkDotNet makes it to measure these sorts of very short operations. Whenever I&#39;ve tried to do something similar in the past, I&#39;ve felt niggling doubts that maybe I&#39;m not running it enough times or maybe there are some factors that I should try to average out. Even when I get a result, I&#39;ve sometimes just looked at the single average (ie. the mean) time taken, which is a bit sloppy since the spread of results can be of vital importance as well. That BenchmarkDotNet presents the final data in such a useful way and with so few decisions on my part is fantastic.&lt;/p&gt;

&lt;h3&gt;.NET Core upsets me again&lt;/h3&gt;

&lt;p&gt;On the other hand, unfortunately .NET Core has been hard work for me again when it came to BenchmarkDotNet. I made it sound very easy earlier to get everything up and running because I didn&#39;t want dilute my enthusiasm for the benchmarking. However, I did have a myriad of problems before everything started working properly.&lt;/p&gt;

&lt;p&gt;When I was hand-editing the .csproj file to target multiple frameworks (I still don&#39;t know why this isn&#39;t possible within VS when editing project properties), Visual Studio would only seem to intermittently acknowledge that I&#39;d changed it and offer to reload. This wasn&#39;t super-critical but it also didn&#39;t fill me with confidence.&lt;/p&gt;

&lt;p&gt;When it &lt;em&gt;was&lt;/em&gt; ready to build and target both .NET Framework 4.6.1 and .NET Core 1.1, I got a cryptic warning:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Detected package downgrade: Microsoft.NETCore.App from 1.1.2 to 1.1.1 &lt;/p&gt;
  
  &lt;p&gt;CoreExeTest (&gt;= 1.0.0) -&gt; BenchmarkDotNet (&gt;= 0.10.8) -&gt; Microsoft.NETCore.App (&gt;= 1.1.2) 
   CoreExeTest (&gt;= 1.0.0) -&gt; Microsoft.NETCore.App (&gt;= 1.1.1)               &lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Everything seemed to build alright but I didn&#39;t know if this was something to worry about or not (I like my projects to be zero-warning). It suggested to me that I was targeting .NET Core 1.1 and BenchmarkDotNet was expecting .NET Core 1.1.2 - sounds simple enough, surely I can upgrade? I first tried changing the .csproj to target &quot;netcoreapp1.1.2&quot; but that didn&#39;t work. In fact, it &quot;didnt work&quot; in a very unhelpful way; when I ran the project it would open in a window and immediately close, with no way to break and catch the exception in the debugger. I used &quot;dotnet run&quot;* on the command line to try to see more information and was then able to see the error message:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The specified framework &#39;Microsoft.NETCore.App&#39;, version &#39;1.1.2&#39; was not found.&lt;/p&gt;
  
  &lt;ul&gt;
  &lt;li&gt;&lt;p&gt;Check application dependencies and target a framework version installed at:&lt;/p&gt;
  
  &lt;p&gt;C:\Program Files\dotnet\shared\Microsoft.NETCore.App&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;The following versions are installed:&lt;/p&gt;
  
  &lt;p&gt;1.0.1&lt;/p&gt;
  
  &lt;p&gt;1.0.4&lt;/p&gt;
  
  &lt;p&gt;1.1.1&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;Alternatively, install the framework version &#39;1.1.2&#39;.&lt;/p&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;* &lt;em&gt;(Before being able to use &quot;dotnet run&quot; I had to manually edit the .csproj file to only target .NET Core - if you target multiple frameworks and try to use &quot;dotnet run&quot; then you get an error &quot;Unable to run your project. Please ensure you have a runnable project type and ensure &#39;dotnet run&#39; supports this project&quot;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I changed the .csproj file back from &quot;netcoreapp1.1.2&quot; to &quot;netcoreapp1.1&quot; and went to the NuGet UI to see if I could upgrade the &quot;Microsoft.NETCore.App&quot; package.. but the version dropdown wouldn&#39;t let me change it (stating that the other versions that it was aware of were &quot;Blocked by project&quot;).&lt;/p&gt;

&lt;p&gt;I tried searching online for a way to download and install 1.1.2 but got nowhere.&lt;/p&gt;

&lt;p&gt;Finally, I saw that VS 2017 had an update pending entitled &quot;Visual Studio 15.2 (26430.16)&quot;. The &quot;15.2&quot; caught me out for a minute because I initially presumed it was an update for VS &lt;em&gt;2015&lt;/em&gt;. The update includes .NET Core 1.1.2 (see &lt;a href=&quot;https://github.com/dotnet/core/issues/622&quot;&gt;this dotnet GitHub issue&lt;/a&gt;) and, when I loaded my solution again, the warning above had gone. Looking at the installed packages for my project, I saw that &quot;Microsoft.NETCore.App&quot; was now on version 1.1.2 and that all other versions were &quot;Blocked by project&quot;. This does not feel friendly and makes me worry about sharing code with others - if they don&#39;t have the latest version of Visual Studio then the code may cause them warnings like the above that don&#39;t happen on my PC. Yuck.&lt;/p&gt;

&lt;p&gt;After all this, I got the project compiling (without warnings) and running, only for it to intermittently fail as soon as it started:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Access to the path &#39;BDN.Generated.dll&#39; is defined&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This relates to an output folder created by BenchmarkDotNet. Sometimes this folder would be locked and it would not be possible to overwrite the files on the next run. Windows wouldn&#39;t let me delete the folder directly but I could trick it by renaming the folder and &lt;em&gt;then&lt;/em&gt; deleting it. I didn&#39;t encounter this problem if I created an old-style .NET Framework project and used BenchmarkDotNet there - this would prevent me from running tests against multiple frameworks but it might have also prevented me from teetering over the brink of insanity.&lt;/p&gt;

&lt;p&gt;This is not how I would expect mature tooling to behave. For now, I continue to consider .NET Core as the Top Gear boys (when they still &lt;em&gt;were&lt;/em&gt; the Top Gear boys) described old Alfa Romeos; &quot;you want to believe that it can be something wonderful but you couldn&#39;t, in all good conscience, recommend it to a friend&quot;.&lt;/p&gt;

&lt;h3&gt;Summary&lt;/h3&gt;

&lt;p&gt;I suspect that, to some, this may seem like one of my more pointless blog posts. I tried to do something that .NET really doesn&#39;t want you to do (and that whoever wrote the code containing the readonly auto-properties really doesn&#39;t expect you to do) and then tried to optimise that naughty behaviour - then spent a lot more time explaining how it wasn&#39;t posible to do so!&lt;/p&gt;

&lt;p&gt;However, along the way I discovered BenchmarkDotNet and I&#39;m counting that as a win - I&#39;ll be keeping that in my arsenal for future endeavours. And I also enjoyed revisiting what is and isn&#39;t possible with reflection and reminding myself of the ways that .NET allows you to write code that could make &lt;em&gt;my&lt;/em&gt; code appear to work in surprising ways.&lt;/p&gt;

&lt;p&gt;Finally, it was interesting to see how the .NET Framework compared to .NET Core in terms of performance for these benchmarks &lt;em&gt;and&lt;/em&gt; to see take another look at the question of how mature .NET Core and its tooling is (or isn&#39;t). And when you learn a few things, can it ever really count as a waste of time?&lt;/p&gt;</description>
			<pubDate>Wed, 26 Jul 2017 20:31:00 GMT</pubDate>
		</item>
		<item>
			<title>Revisiting .NET Core tooling (Visual Studio 2017)</title>
            <link>http://www.productiverage.com/revisiting-net-core-tooling-visual-studio-2017</link>
			<guid>http://www.productiverage.com/revisiting-net-core-tooling-visual-studio-2017</guid>
			<description>&lt;p&gt;In November last year, I migrated a fairly small but non-trivial project to .NET Core to see what I thought about the process and whether I was happy to make .NET Core projects my default. At that point I wasn&#39;t happy to, there were too many rough edges.&lt;/p&gt;

&lt;p&gt;Since then, things have changed. The project.json format has been replaced with a .csproj format that is supported by the now-available Visual Studio 2017 and various other aspects of .NET Core development have had a chance to mature. So I thought that it was worth revisiting.&lt;/p&gt;

&lt;h3&gt;TL;DR&lt;/h3&gt;

&lt;p&gt;Things have come on a long way since the end of last year. But you don&#39;t get the level of consistency and stability with Visual Studio when you use it to develop .NET Core applications that you do when you use it to develop .NET Framework applications. To avoid frustration (and because I don&#39;t currently have use cases that would benefit from multi-platform support), I&#39;m still not going to jump to .NET Core for my day-to-day professional development tasks. I will probably dabble with it for personal projects.&lt;/p&gt;

&lt;h3&gt;The Good&lt;/h3&gt;

&lt;p&gt;First off, I&#39;m a huge fan of the new .csproj format. The &quot;legacy&quot; .csproj is huge and borderline-incomprehensible. Just what do the &quot;ProjectTypeGuids&quot; values mean and what are the acceptable choices? An incorrect one will mean that VS won&#39;t load the project and it won&#39;t readily give you information as to why. The new format is streamlined and beautiful. They took the best parts of project.json and made it into a format that would play better with MSBuild (and avoiding frightening developers who see &quot;project.json&quot; and get worried they&#39;re working on a frontend project that may have a terrifying Babel configuration hidden somewhere). I like that files in the folder structure are included by default, it makes sense (the legacy format required that every file explicitly be &quot;opted in&quot; to the project).&lt;/p&gt;

&lt;p&gt;Next big win: Last time I tried .NET Core, one of the things that I wanted to examine was how it easy it would be to migrate a solution with multiple projects. Could I change one project from being .NET Framework to .NET Core and then reference that project from the .NET Framework projects? It was possible but only with an ugly hack (where you had to edit the legacy .NET Framework .csproj files and manually create the references). That wasn&#39;t the end of it, though, since this hack confused VS and using &quot;Go To Definition&quot; on a reference that lead into a .NET Core dependency would take you to a &quot;from metadata&quot; view instead of the file in the Core project. Worse, the .NET Framework project wouldn&#39;t know that it had to be rebuilt if the Core project that it referenced was rebuilt. All very clumsy. The good news is that VS2017 makes this all work perfectly!&lt;/p&gt;

&lt;p&gt;Shared projects may also now be referenced from .NET Core projects. This didn&#39;t work in VS2015. There were workarounds but, again, they were a bit ugly (see the Stack Overflow question &lt;a href=&quot;https://stackoverflow.com/questions/38523457/how-do-i-reference-a-visual-studio-shared-project-in-a-net-core-class-library&quot;&gt;How do I reference a Visual Studio Shared Project in a .NET Core Class Library&lt;/a&gt;). With 2017, everything works as you would expect.&lt;/p&gt;

&lt;p&gt;The final positive isn&#39;t something that&#39;s changed since last year but I think that it&#39;s worth shouting out again - the command line experience with .NET Core is really good. Building projects, running tests and creating NuGet packages are all really simple. In many of my older projects, I&#39;ve had some sort of custom NuGet-package-creating scripts but any .NET Core projects going forward won&#39;t need them. (One thing that I particularly like is that if you have a unit test project that builds for multiple frameworks - eg. .NET Core 1.1 and .NET Framework 4.5.2 - then the tests will all be run against both frameworks when &quot;dotnet test&quot; is executed).&lt;/p&gt;

&lt;h3&gt;The Bad&lt;/h3&gt;

&lt;p&gt;Let&#39;s look at the not-so-good stuff. Firstly, I still find some of the terminology around .NET Core confusing. And, reading around, I&#39;m not the only one. When I create a new project, I can choose a &quot;.NET Core Class Library&quot; and I can also choose a &quot;.NET Standard Class Library&quot;. Now, as I understand it, the basic explanation is that .NET Standard is a baseline &quot;standard&quot; that may have multiple implementations - all of them have to provide the full API that .NET Standard specifies. And .NET Core is one of the implementations of .NET Standard, so that means that a .NET Core class library has access to everything that .NET Standard dictates must be available.. plus (potentially) a bit more. Now, what that &quot;bit more&quot; might entail isn&#39;t 100% clear to me. I guess that the short answer is the you would need to create a &quot;.NET Core Class Library&quot; if you want to reference something that uses APIs that only .NET Core (and not .NET Standard) surface.&lt;/p&gt;

&lt;p&gt;Another way to look at it is that it&#39;s best to start with a &quot;.NET Standard Class Library&quot; (rather than a &quot;.NET Core Class Library&quot;) unless you have a really compelling reason not to because more people / platforms / frameworks will be able to use the library that you use; .NET Standard assemblies may be referenced by .NET Core project and .NET Framework projects (and, if I have this right, Mono or Xamarin projects as well).&lt;/p&gt;

&lt;p&gt;I&#39;ve &lt;a href=&quot;http://quoteinvestigator.com/2013/03/06/artists-steal/&quot;&gt;stolen&lt;/a&gt; the following from an MSDN post by &lt;a href=&quot;https://social.msdn.microsoft.com/profile/Immo+Landwerth+[MSFT]&quot;&gt;Immo Landwerth&lt;/a&gt; that relates to this:&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;Unnecessary Optional instantiation&quot; src=&quot;http://www.productiverage.com/Content/Images/Posts/NETStandard.png&quot; class=&quot;NoBorder FullWidth&quot;&gt;&lt;/p&gt;

&lt;p&gt;However, this still leaves another problem. If you want access to more APIs then you might have to change from .NET Standard to .NET Core. &lt;em&gt;Or&lt;/em&gt; you might have be able to stick with .NET Standard but use a later version. If you create a .NET Standard Class Library then you can tell it what version of .NET Standard that you want to support by going to the project properties and changing the Target Framework. In general, if you&#39;re building a library for use by other people then you probably want to build it against the lowest version of .NET Standard possible. Maybe it&#39;s better to say the &quot;most accessible&quot; version of .NET Standard. If your library might be referenced by a project that targets .NET Standard 1.6 then it won&#39;t work if your library requires .NET Standard 2.0 (you&#39;ll force the consumer to require the later version or they&#39;ll decide not to use your library).&lt;/p&gt;

&lt;p&gt;Currently, .NET Core 1.1 and .NET Framework 4.6 implement .NET Standard 1.6 and so it&#39;s probably not the end of the world to take 1.6 as an acceptable minimum for .NET Standard libraries. But .NET Standard 2.0 &lt;a href=&quot;https://blogs.msdn.microsoft.com/dotnet/2017/05/10/announcing-net-core-2-0-preview-1/&quot;&gt;is in beta&lt;/a&gt; and I&#39;m not really sure what that will run on (will .NET Framework 4.6 be able to reference .NET Standard 2.0 or will we need 4.7?).. my point is that this is still quite confusing. That&#39;s not the fault of the tooling but it&#39;s still something you&#39;ll have to butt up against if you start going down the .NET Core / .NET Standard path.&lt;/p&gt;

&lt;p&gt;My final whinge about .NET Standard versions is that it&#39;s often hard to know &lt;em&gt;when&lt;/em&gt; to change version. While doing research for this post, I re-created one of my projects &lt;em&gt;again&lt;/em&gt; and tried to start with the minimum framework version each time. I had some reflection code that uses BindingFlags.GetField and it was refusing to compile. Because I was using .NET Standard 1.3. If I changed to .NET Standard 1.6 then it compiled fine. The problem is that it&#39;s hard to know what to do, it feels like a lot of guess work - do I need to change the .NET Standard version or do I need to switch to a .NET Core Class Library?&lt;/p&gt;

&lt;p&gt;Let me try and get more tightly focused on the tooling again. Earlier, I said that one of the plusses is that it&#39;s so easy to create NuGet packages with &quot;dotnet pack&quot;. One of the &lt;em&gt;problems&lt;/em&gt; (maybe &quot;mixed blessing&quot; would be more accurate) with this is that the packages are built entirely from metadata in the .csproj file. So you need to add any extra NuGet-specific information there. This actually works great - for example, here is one of my project files:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;Project Sdk=&quot;Microsoft.NET.Sdk&quot;&amp;gt;

  &amp;lt;PropertyGroup&amp;gt;
    &amp;lt;TargetFrameworks&amp;gt;netstandard1.6;net45&amp;lt;/TargetFrameworks&amp;gt;

    &amp;lt;PackageId&amp;gt;FullTextIndexer.Serialisation.Json&amp;lt;/PackageId&amp;gt;
    &amp;lt;PackageVersion&amp;gt;1.1.0&amp;lt;/PackageVersion&amp;gt;
    &amp;lt;Authors&amp;gt;ProductiveRage&amp;lt;/Authors&amp;gt;
    &amp;lt;Copyright&amp;gt;Copyright 2017 Productive Rage&amp;lt;/Copyright&amp;gt;
    &amp;lt;PackageTags&amp;gt;C# full text index search&amp;lt;/PackageTags&amp;gt;
    &amp;lt;PackageIconUrl&amp;gt;https://secure.gravatar.com/avatar/6a1f781d4d5e2d50dcff04f8f049767a?s=200&amp;lt;/PackageIconUrl&amp;gt;
    &amp;lt;PackageProjectUrl&amp;gt;https://bitbucket.org/DanRoberts/full-text-indexer&amp;lt;/PackageProjectUrl&amp;gt;
  &amp;lt;/PropertyGroup&amp;gt;

  &amp;lt;ItemGroup&amp;gt;
    &amp;lt;PackageReference Include=&quot;Newtonsoft.Json&quot; Version=&quot;10.0.2&quot; /&amp;gt;
  &amp;lt;/ItemGroup&amp;gt;

  &amp;lt;ItemGroup&amp;gt;
    &amp;lt;ProjectReference Include=&quot;..\FullTextIndexer.Common\FullTextIndexer.Common.csproj&quot; /&amp;gt;
    &amp;lt;ProjectReference Include=&quot;..\FullTextIndexer.Core\FullTextIndexer.Core.csproj&quot; /&amp;gt;
  &amp;lt;/ItemGroup&amp;gt;

  &amp;lt;ItemGroup Condition=&quot;&#39;$(TargetFramework)&#39; == &#39;netstandard1.6&#39;&quot;&amp;gt;
    &amp;lt;PackageReference Include=&quot;System.Reflection.TypeExtensions&quot;&amp;gt;
      &amp;lt;Version&amp;gt;4.3.0&amp;lt;/Version&amp;gt;
    &amp;lt;/PackageReference&amp;gt;
  &amp;lt;/ItemGroup&amp;gt;

&amp;lt;/Project&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I&#39;ve got everything I need; package id, author, copyright, icon, tags, .. My issue isn&#39;t how this works, it&#39;s that this doesn&#39;t seem to well documented. Searching on Google presents articles such as &lt;a href=&quot;https://docs.microsoft.com/en-us/nuget/guides/create-net-standard-packages-vs2017&quot;&gt;Create .NET standard packages with Visual Studio 2017&lt;/a&gt; which is very helpful but &lt;em&gt;doesn&#39;t&lt;/em&gt; link anywhere to a definitive list of what properties are and aren&#39;t supported. I came up with the above by hoping that it would work, calling &quot;dotnet pack&quot; and then examining the resulting .nupkg file in NuGet Package Explorer.&lt;/p&gt;

&lt;p&gt;My next beef is with unit testing. Earlier, I said that &quot;dotnet test&quot; is great because it executes the test against every framework that your project targets. And that &lt;em&gt;is&lt;/em&gt; great. But getting your unit test project to that point can be hard work. I like xUnit and they have a great article about &lt;a href=&quot;http://xunit.github.io/docs/getting-started-dotnet-core&quot;&gt;Getting started with xUnit.net (.NET Core / ASP.NET Core)&lt;/a&gt; but I dislike that there&#39;s copy-pasting into the .csproj file required to make it work, I wish that the GUI tooling was mature enough to be up to the job for people who wish to take that avenue. But it isn&#39;t. There is no way to do this without manually hacking about your .csproj file. I like that the command line interface is so solid but I&#39;m not sure that it&#39;s ok to &lt;em&gt;require&lt;/em&gt; that the CLI / manual-file-editing be used - .NET is such a well-established and well-used technology that not everyone wants to have to rely upon the CLI. I suspect that 90% of .NET users want Visual Studio to be able to everything for them because it has historically been able to - and I don&#39;t think that anyone should judge those people and tell them they&#39;re wrong and should embrace the CLI.&lt;/p&gt;

&lt;p&gt;To make things worse, in order to use xunit with .NET Core (or .NET Standard) you need to use pre-release versions of the libraries. Why? They&#39;ve been pre-release for a long time now, I find it hard to believe that they&#39;re not sufficiently stable / well-tested to make it to a &quot;real&quot; NuGet package release. Microsoft is suggesting that .NET Core is ready for mainstream use but other common dependencies aren&#39;t (this doesn&#39;t go for all major NuGet packages - AutoMapper, Json.NET and Dapper, for example, all work with .NET Standard without requiring pre-release versions).&lt;/p&gt;

&lt;p&gt;Oh, one more thing about unit tests (with xunit, at least) - after you follow the instructions and get the tests recognised in the VS Test Explorer, they only get run for one framework. I&#39;m not sure which, if you specify multiple. Which is disappointing. Since the CLI is so good and runs tests for all supported frameworks, I wish that the Test Explorer integration would as well.&lt;/p&gt;

&lt;p&gt;Last bugbear: When I create a .NET Framework Web Project and run it and see the result in the browser, so long as I have disabled &quot;Enable Edit and Continue&quot; in the Project Properties / Web pane then I can make changes, rebuild and then refresh in the browser without &quot;running&quot; (ie. attaching the debugger). This often shortens the edit-build-retry cycle (sometimes only slightly but sometimes by a few valuable seconds) but it&#39;s something I can&#39;t reproduce with .NET Core Web Projects; once the project is stopped, I can&#39;t refresh the page in the browser until I tell VS to run again. Why can&#39;t it leave the site running in IIS Express??&lt;/p&gt;

&lt;h3&gt;The Ugly&lt;/h3&gt;

&lt;p&gt;I&#39;ve been trying to find succinct examples of this problem while writing this article and I&#39;ve failed.. While looking into VS2017 tooling changes, I migrated my &quot;&lt;a href=&quot;https://bitbucket.org/DanRoberts/full-text-indexer&quot;&gt;Full Text Indexer&lt;/a&gt;&quot; code across. It&#39;s not a massive project by any means but it spans multiple projects within the same solution and builds NuGet packages for consumption by both .NET Standard and .NET Framework. Last year, I got it working with the VS2015 tooling and the project.json format. This year, I changed it to use the new .csproj format and got it building nicely in VS2017. One of the most annoying things that I found during this migration was that I would make change to projects (sometimes having to edit the project files directly) and the changes would refuse to apply themselves without me restarting VS (probably closing and re-opening the solution would have done it too). This was very frustrating. More frustrating at this very minute, frankly, is that I&#39;m unable to start a clean project and come up with an example of having to restart VS to get a change applied. But the feeling that I was left with was that the Visual Studio tooling was flakey. If I built everything using the CLI then it was fine - another case where I felt that if you don&#39;t mind manual editing and the command line then you&#39;ll be fine; but that&#39;s not, in my mind, a .NET release that is ready for &quot;prime time&quot;.&lt;/p&gt;

&lt;p&gt;Another flakey issue I had is that I have a &quot;FullTextIndexer&quot; project that doesn&#39;t have any code of its own, it only exists to generate a single NuGet package that pulls in the other five projects / packages in one umbrella add-this-and-you-get-everything package. When I first created the project and used &quot;dotnet pack&quot; then the resulting package only listed the five dependencies for .NET Standard and &lt;em&gt;not&lt;/em&gt; for .NET Framework. I couldn&#39;t work out what was causing the problem.. then it went away! I couldn&#39;t put my finger on anything that had changed but it started emitting correct packages (with correct dependencies) at some point. I had another problem building my unit test project because one of the referenced projects needed the &quot;System.Text.RegularExpressions&quot; package when built as .NET Standard and it complained that it couldn&#39;t load version 4.1.1.0. One of the projects reference 4.3.1.0 but I could never find where the 4.1.1.0 requirement came in and I couldn&#39;t find any information about assembly binding like I&#39;m used to in MVC projects (where the web.config will say &quot;for versions x through y, just load y&quot;). This problem, also, just disappeared and I couldn&#39;t work out what had happened to make it go away.&lt;/p&gt;

&lt;p&gt;In my multi-framework-targeting example solution, I have some conditional compilation statements. I use the Full Text Indexer to power the search on this blog and I serialise a search index using the BinaryFormatter. In order to do this, the search index classes have to have the [Serializable] attribute. But this attribute is not available in .NET Standard.. So the .NET Standard builds of the Full Text Indexer don&#39;t have [Serializable] attributes, while the .NET Framework builds &lt;em&gt;do&lt;/em&gt; have it. That way I can produce nice, clean, new .NET Standard libraries without breaking backwards compatibility for .NET Framework consumers (like my Blog). To do this end, I have code like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#if NET45
  [Serializable]
#endif
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I have two problems with this. Firstly, the conditional compile strings are a little bit &quot;magic&quot; and are not statically analysed. If, for example, you change &quot;#if NET45&quot; to &quot;#if net45&quot; then the code would not be included in .NET Framwork 4.5 builds. You wouldn&#39;t get any warning or indication of this, it would happen silently. Similarly, if your project builds for &quot;netstandard1.6&quot; and &quot;net45&quot; and you include a conditional &quot;#if NET452&quot; then that condition will never be met because you should have used &quot;NET45&quot; and not &quot;NET452&quot;. Considering the fact that I use languages like C# that are statically typed so that the compiler can identify silly mistakes like this that I might make, this is frustrating when I get it wrong. The second issue I have is that the conditional statement highlighting is misleading when the debugger steps through code. If I have a project that has target frameworks &quot;netstandard1.6;net45&quot; and I reference this from a .NET Framework Console Application and I step through into the library code, any &quot;#if NET45&quot; code will appear &quot;disabled&quot; in the IDE when, really, that code is in play. That&#39;s misleading and makes me sad.&lt;/p&gt;

&lt;h3&gt;To summarise..?&lt;/h3&gt;

&lt;p&gt;I&#39;m really impressed with how much better the experience has been in writing .NET Core / .NET Standard projects (or projects that build for .NET Core / Standard &lt;em&gt;and&lt;/em&gt; &quot;full fat&quot; Framework). However.. I&#39;m just still not that confident that the technology is mature yet. I&#39;ve encountered too many things that work ok only 95% of the time - and this makes me think that if I tried to encourage everyone at work to adopt .NET Core / Standard today then I&#39;d regret it. There would just be too many occurrences where someone would hit a &quot;weird issue that may or may not go away.. and if it does then we&#39;re not sure why&quot; problems.&lt;/p&gt;

&lt;p&gt;I think that the future is bright for .NET Core.. but it seems like the last two years have permanently had us feeling that &quot;in just six months or so, .NET Core will be ready to rock&#39;n&#39;roll&quot;. And things like &quot;&lt;a href=&quot;https://www.theregister.co.uk/2017/05/09/dot_net_compatibility/&quot;&gt;ASP.NET Core 2.0 won&#39;t be supported on .NET Framework&lt;/a&gt;&quot; mix-ups don&#39;t help (TL;DR: It was said that &quot;ASP.NET Core&quot; wouldn&#39;t work on the &quot;full fat&quot; .NET Framework and that that was by design - but then it turned out that this was a miscommunication and that it would only be a temporary sitation and ASP.NET Core &lt;em&gt;would&lt;/em&gt; work within .NET Framework as well).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;To sumarise the summary:&lt;/strong&gt; I hope to move to .NET Core in the foreseeable future. But, professionally, I&#39;m not going to today (personal projects maybe, but not at work).&lt;/p&gt;</description>
			<pubDate>Mon, 19 Jun 2017 19:55:00 GMT</pubDate>
		</item>
		<item>
			<title>Face or no face (finding faces in photos using C# and Accord.NET)</title>
            <link>http://www.productiverage.com/face-or-no-face-finding-faces-in-photos-using-c-sharp-and-accordnet</link>
			<guid>http://www.productiverage.com/face-or-no-face-finding-faces-in-photos-using-c-sharp-and-accordnet</guid>
			<description>&lt;p&gt;I&#39;ve always been intrigued as to how facial detection and recognition is handled by computers. It&#39;s one of the few effects that is seen in Hollywood films that seems convincing - where someone is tracked through a city, live CCTV feeds finding their face as they move from one street to the next. For a more mundane example, facial detection has been built into digital cameras and smartphones for quite some time now (my phone puts green squares around faces in photos and, I presume, uses this information to decide where to focus).&lt;/p&gt;

&lt;p&gt;I knew literally nothing about the subject to begin with and so I hit the internet and tried to find out everything I could about it. Turns out that there&#39;s a &lt;em&gt;lot&lt;/em&gt; that&#39;s been written about this, with various different methods and approaches discussed and documented. It&#39;s an interesting subject because it&#39;s been so affected by the rate of change in technology - for example, the digital cameras that introduced it had access to much less processing power than modern day devices do. I&#39;ve had a hard time finding definitive information about when digital cameras started offering this facility but I&#39;m getting a rough impression that it&#39;s over the last ten years (citation needed!) - when you think about how phones, for example, have changed in ten years in terms of their power.. well, it&#39;s a lot!&lt;/p&gt;

&lt;p&gt;Having said that, digital cameras (and web cams) can take some shortcuts and only concern themselves with subjects that are facing directly towards the camera, since these are more likely to be subjects that are of interest - it&#39;s not as important to worry about people looking at ninety degrees from the camera lens, for example). So, straight away, it&#39;s apparent there are going to different techniques and different requirements for different situations and some will be more expensive than others.&lt;/p&gt;

&lt;p&gt;I suspect that this is going to be an ongoing investigation for me; so far I&#39;ve only managed to implement facial detection functionality and not facial &lt;em&gt;recognition&lt;/em&gt; (once a face has been detected, is it someone from a database of known faces?). I thought I&#39;d present what I have at this point, though - if it&#39;s interesting to me then maybe it will be of interest to other people! And I couldn&#39;t find much material out there that goes into enough detail that it&#39;s reproducible without being a really dense research paper (and certainly not ones that make it easy to reproduce their findings using C#), so I thought that I&#39;d try to fill that gap. There &lt;em&gt;are&lt;/em&gt; articles out there about using libraries to do all the hard work for you - but where&#39;s the fun in that?! I want to know &lt;em&gt;how&lt;/em&gt; the code I&#39;m calling is working.&lt;/p&gt;

&lt;h3&gt;tl;dr&lt;/h3&gt;

&lt;p&gt;My current code (&lt;a href=&quot;https://github.com/ProductiveRage/FacialRecognition&quot;&gt;github.com/ProductiveRage/FacialRecognition&lt;/a&gt;) identifies faces in photos by using two passes. First, it looks for regions of skin that have holes (that we presume may be eyes and a nose) and that are very roughly the right proportions to be a face.&lt;/p&gt;

&lt;p&gt;The image content from these regions is passed through a &quot;linear support vector machine&quot;, which will have been trained to classify a potential image region as a face or as not-a-face.&lt;/p&gt;

&lt;p&gt;Before embarking on this adventure, I had no idea what a &quot;support vector machine&quot; even was. If you don&#39;t either then maybe you&#39;ll come away from this feeling like you know a little something about machine learning. I know that I did! And &quot;machine learning&quot; is a phrase that I&#39;ve been interested in finding out more about for quite some time now, it seems like it&#39;s cropping up everywhere! I have a strong feeling that trying to perform facial &lt;em&gt;recognition&lt;/em&gt; is going to involve even more magical sounding terms - such as &quot;deep convolutional neural network&quot; - but that&#39;s going to have to wait for another day, it&#39;s one (baby) step at a time for me.&lt;/p&gt;

&lt;h3&gt;The &quot;Naked People Skin Filter&quot;&lt;/h3&gt;

&lt;p&gt;In a past life, I worked in IT Support and, one day, I was asked by the Head Clinician why I&#39;d been printing out porn. Which confused me considerably since I had most certainly &lt;em&gt;not&lt;/em&gt; been printing out explicit material. There had been complaints about the colour tone and contrast of a new colour laser printer and so I&#39;d sent a full-page test image to it that included greyscale content, colour grids and images that contained a lot of skin. Apparently printing out pictures with lots of skin in it raises eyebrows and maybe it&#39;s for similar reasons that a research paper entitled &quot;Naked People Skin Filter&quot; amusingly sounds a touch pervy (or maybe that&#39;s just a British thing.. we do love to keep most of our naked-skin-unvealing behind closed doors - because it&#39;s so cold!).&lt;/p&gt;

&lt;p&gt;This &quot;Naked People Skin Filter&quot; paper was published by Margaret M. Fleck and David A Forsyth and is available at &lt;a href=&quot;http://mfleck.cs.illinois.edu/naked-skin.html&quot;&gt;http://mfleck.cs.illinois.edu/naked-skin.html&lt;/a&gt;. It essentially describes an algorithm to detect regions of skin in an image. In 1997, another paper &quot;&lt;a href=&quot;http://web.archive.org/web/20090723024922/http:/geocities.com/jaykapur/face.html&quot;&gt;Face Detection in Color Images&lt;/a&gt;&quot; described starting with that skin-tone-identifying approach and then using it to try to find solid regions of skin that have a few holes in, on the basis that this is &lt;em&gt;probably&lt;/em&gt; a face (where the holes are eyes, nose, mouth).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.productiverage.com/Content/Images/Posts/TigerWoods.gif&quot; alt=&quot;Tiger Woods&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note: That &quot;Face Detection in Color Images&quot; link use the &lt;a href=&quot;http://archive.org/web/&quot;&gt;Wayback Machine&lt;/a&gt; to make available a GeoCities page. Maybe it&#39;s a sad reminder that there was actually worthwhile content on GeoCities as well as the many harshly-coloured-and-bizarely-animated eye-watering monstrosities. By the way, if you (quite rightly, imho) think that the web has lost something by having less places for people to upload content for free then check out &lt;a href=&quot;https://neocities.org/browse&quot;&gt;NeoCities&lt;/a&gt; - it&#39;s awesome because it makes that possible again (and all the good and bad that comes with it)!&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&quot;Face Detection in Color Images&quot; uses an example image of Tiger Woods and so I will use that same image while I discuss what I&#39;ve done, based upon these articles.&lt;/p&gt;

&lt;p&gt;There are a set of steps that are followed when trying to identify faces in an image:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Scale the image down (depending upon its size)&lt;/li&gt;
&lt;li&gt;Avoid desaturation by bring colour down values so that the minimum is true zero&lt;/li&gt;
&lt;li&gt;Transform RGB colours into the RG/BY colour space, alongside an Intensity spectrum&lt;/li&gt;
&lt;li&gt;Calculate &quot;texture amplitude&quot; across the image (how quickly intensity changes - the theory is that areas of skin have a relatively low texture amplitude)&lt;/li&gt;
&lt;li&gt;Calculate hue and sauturation across the image (in order to identify areas that seem to be skin-coloured)&lt;/li&gt;
&lt;li&gt;Combine the texture amplitude and hue data to identify areas of skin (the texture amplitude, hue and saturation must be within acceptable &quot;this is probably skin&quot; bounds)&lt;/li&gt;
&lt;li&gt;Expand these areas of skin using a more relaxed texture amplitide / hue / saturation &quot;this is probably skin&quot; filter&lt;/li&gt;
&lt;li&gt;Check these areas for holes - no holes (for non-skin facial features, such as eyes) means no face

&lt;ul&gt;
&lt;li&gt;Any skin areas that are very small are ignored&lt;/li&gt;
&lt;li&gt;Any skin areas that are extreme aspect ratios are ignored (very, very wide areas or very, very tall areas are probably not faces)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Expand the areas slightly because, even with the flexible-skin-filter expansions, the area that a human would identify as being a person&#39;s face is slightly larger than the areas identified by this algorithm&lt;/li&gt;
&lt;/ol&gt;

&lt;h2&gt;Calculating hue, saturation and texture amplitude (steps 1-7) to identify skin areas&lt;/h2&gt;

&lt;p&gt;The first few steps are fairly simple. If the image is either taller or wider than 400px then it&#39;s shrunk down so that the largest side is 400px. &quot;Naked People Skin Filter&quot; talks about applying &quot;smoothing&quot; (which I&#39;ll cover in a moment) relative to the size of the image but doesn&#39;t suggest any restrictions on the image size. &quot;Face Detection in Color Images&quot; mentions that only images up to 250px were used. With the test images I used, I found that capping at 400px provided a reasonable balance between identifying faces and not taking too long to process. I used the Tiger Woods image initially to try to ensure that my calculations were matching those described in &quot;Face Detection in Color Images&quot; but I used other test images (which will appear further down) to double-check that everything seemed to be working as expected. There are some differences between how Fleck &amp;amp; Forsyth (&quot;Naked People Skin Filter&quot;) and Kapur (&quot;Face Detection in Colour Images&quot;) perform the analysis and my test images led me to tweak a few parameters myself.&lt;/p&gt;

&lt;p&gt;Fleck &amp;amp; Forsyth recommend pulling down the colour values so that the minimum value is zero because it &quot;avoids potentially significant desaturation of opponent color values if the zero-response is far from zero&quot;. What this means is that the darkest parts of an image are probably not &quot;true black&quot; (ie. RGB 0, 0, 0) and the theory is that we should get better results if everything is adjusted down so that the darkest colour &lt;em&gt;is&lt;/em&gt; black. In practice, I look for the lowest of any of the R, G or B values from any pixel in the image and subtract that value &lt;em&gt;from&lt;/em&gt; every R, G and B value across all pixels in the image.&lt;/p&gt;

&lt;p&gt;Next, I generate &quot;RgByI&quot; values for every pixel. This a trio of values; an RG (red-green) spectrum value, a BY (blue-yellow) spectrum value and an intensity. Since I&#39;m going to spend a lot of time taking the pixels from an image and loading them into a 2D array and then performing a range of operations on them, I&#39;ve written a &lt;strong&gt;DataRectangle&amp;lt;T&amp;gt;&lt;/strong&gt; class that makes this easier. When I first read the data from an image, the pixels are loaded and used to populate a &lt;strong&gt;DataRectangle&lt;/strong&gt; of RGB values. When I need to get RgByI values from them, I can do the following (using Kapur&#39;s formulae from &quot;Face Detection in Colour Images) -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var rgByIValues = colourData.Transform(colour =&amp;gt;
{
    Func&amp;lt;byte, double&amp;gt; L = x =&amp;gt; (105 * Math.Log10(x + 1));
    return new IRgBy(
        rg: L(colour.R) - L(colour.G),
        by: L(colour.B) - ((L(colour.G) + L(colour.R)) / 2),
        i: (L(colour.R) + L(colour.B) + L(colour.G)) / 3
    );
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In order to reduce unhelpful detail in the image somewhat, the RgByI values have a &quot;windowing median filter&quot; applied to them. From &quot;Face Detection in Colour Images&quot; -&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The Rg and By matrices are then filtered with a windowing median filter .. with sides of length 4*SCALE. The SCALE value is calculated as being the closest integer value to (height+width)/320&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;em&gt;(The &quot;Naked People Skin Filter&quot; paper recommends using 2*SCALE for the RG and BY values, which I have gone with because I found the results to be just as accurate and it&#39;s less computational work than 4*SCALE)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;To do this, you go through every pixel and take its value and the values from the surrounding pixels, sort them, take the middle value (the &quot;median&quot;) and use that as the new value for the current pixel. The idea is that this reduces noise by discarding any outlier pixel colours in a given range. I take a square block around where the initial pixel is but it would probably be better to approximate a circular area if the media filter radius is large (in my filter, it&#39;s never more than three).&lt;/p&gt;

&lt;p&gt;After smoothing the RG/BY data, their values are combined to generate hue and saturation -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var smoothedHues = smoothedRG.CombineWith(
    smoothedBY,
    (rg, by) =&amp;gt; new
    {
        Hue = RadianToDegree(Math.Atan2(rg, by)),
        Saturation = Math.Sqrt((rg * rg) + (by * by))
    }
);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;http://www.productiverage.com/Content/Images/Posts/SkinMaskGeneration-Hue.png&quot; alt=&quot;Hue&quot;&gt;
&lt;img src=&quot;http://www.productiverage.com/Content/Images/Posts/SkinMaskGeneration-Saturation.png&quot; alt=&quot;Saturation&quot;&gt;&lt;/p&gt;

&lt;p&gt;The two images here are the hue and saturation values generated from the original image.&lt;/p&gt;

&lt;p&gt;In order to generate greyscale images, I had to translate the hue and saturation values into the 0-255 range. The hue values will be in the range of -180 to +180 degrees so I just added 180 and then divided by 2. The saturation values are always positive and won&#39;t exceed 0-255 if multiplied by 2, so I just did that.&lt;/p&gt;

&lt;p&gt;Generating texture amplitude is more complicated. We start with the intensity values from the RgByI data. We then run that through a median filter of 2 * SCALE and calculate the difference between every point in the median filter result and the original intensity. Finally, we run the result of the calculation through a median filter of 3 * SCALE. It may be clearer with some code -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var smoothedIntensity = rgByIValues.MedianFilter(
    value =&amp;gt; value.I,
    2 * scale
);
var differenceBetweenOriginalIntensityAndSmoothedIntensity = rgByIValues.CombineWith(
    smoothedIntensity,
    (x, y) =&amp;gt; Math.Abs(x.I - y)
);
var textureAmplitude = differenceBetweenOriginalIntensityAndSmoothedIntensity.MedianFilter(
    value =&amp;gt; value,
    3 * scale
);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;http://www.productiverage.com/Content/Images/Posts/SkinMaskGeneration-TextureAmplitude.png&quot; alt=&quot;Texture Amplitude&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(Note that the values 2 and 3 in the above code are much lower than Kapur suggests - 8*SCALE and 12*SCALE - but the median filtering algorithm that I threw together was very slow using the higher values and using lower values - which meant that the processing completed much more quickly - did not seem to affect the outcomes)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Hopefully it&#39;s apparent that around the lower face there is low texture amplitude. Texture amplitude is higher around the facial features but that&#39;s to expected. There is low texture amplitude elsewhere in the image (particularly in the sky behind him, the texture is very smooth there and so the texture amplitude is &lt;em&gt;very&lt;/em&gt; low) but what we&#39;re going to look for is areas that appear to be skin in hue/saturation &lt;em&gt;and&lt;/em&gt; in texture amplitude.&lt;/p&gt;

&lt;p&gt;Now that we have all of the information required to guess whether a given pixel is within the acceptable bounds of &quot;probably skin&quot;, we can create a skin mask (a &lt;strong&gt;DataRectangle&amp;lt;bool&amp;gt;&lt;/strong&gt;) -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var skinMask = smoothedHues
    .CombineWith(textureAmplitude, (hs, t) =&amp;gt; new
    {
        Hue = hs.Hue,
        Saturation = hs.Saturation,
        TextureAmplitude = t
    })
    .Transform(hst =&amp;gt;
    {
        return (
            ((hst.Hue &amp;gt;= 105) &amp;amp;&amp;amp; (hst.Hue &amp;lt;= 160) &amp;amp;&amp;amp; (hst.Saturation &amp;gt;= 10) &amp;amp;&amp;amp; (hst.Saturation &amp;lt;= 60)) ||
            ((hst.Hue &amp;gt;= 160) &amp;amp;&amp;amp; (hst.Hue &amp;lt;= 180) &amp;amp;&amp;amp; (hst.Saturation &amp;gt;= 30) &amp;amp;&amp;amp; (hst.Saturation &amp;lt;= 30))
        )
        &amp;amp;&amp;amp; (hst.TextureAmplitude &amp;lt;= 5);
    });
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is another point at which I have added my own tweaks to the processing. There are slightly different ranges of acceptable hue / saturation / texture amplitude suggested by Fleck &amp;amp; Forsyth than are suggested by Kaypur and I found that &lt;em&gt;I&lt;/em&gt; wanted to change them a little based upon the test images that I was using.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.productiverage.com/Content/Images/Posts/SkinMask2.png&quot; alt=&quot;Skin mask&quot;&gt;&lt;/p&gt;

&lt;p&gt;The final step in generating the skin mask is to try to identify any skin areas just outside the identified mask. I&#39;ve used the approach suggested by Kapur, who recommends it because it &quot;helps to enlarge the skin map regions to include skin/background border pixels, regions near hair or other features, or desaturated areas&quot;.&lt;/p&gt;

&lt;p&gt;The idea is that we look at negative values in the &lt;strong&gt;DataRectangle&amp;lt;bool&amp;gt;&lt;/strong&gt; skin mask and check whether any of the adjacent values is positive &lt;em&gt;and&lt;/em&gt; if the colour of the pixels that resulted in the false value passes a more relaxed skin filter. The relaxed skin filter test demands only that the hue is within the range 110-180 and that the saturation is with 0-180 (the text amplitude is not considered). This expansion work is performed twice.&lt;/p&gt;

&lt;h3&gt;Recognising skin objects (and checking for holes)&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;http://www.productiverage.com/Content/Images/Posts/SkinMask3.png&quot; alt=&quot;Detailed skin mask&quot;&gt;&lt;/p&gt;

&lt;p&gt;In order to get back some of the details that are within the skin mask areas, the original image is changed to greyscale and then the skin mask is combined with it to produce a new skin mask that is slightly more restrictive; any negative content from the skin mask remains negative while positive content is only allowed if the greyscale intensity is within an acceptable range. Everything from this point (including this step) comes from the &quot;Face Detection in Color Images&quot; article, since &quot;Naked People Skin Filter&quot; ends when the skin regions are detected (it has no interest in faces, specifically).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;skinMask = colourData.CombineWith(
    skinMask,
    (colour, mask) =&amp;gt;
    {
        if (!mask)
            return false;
        var intensity = colour.ToGreyScale();
        return (intensity &amp;gt;= 90) &amp;amp;&amp;amp; (intensity &amp;lt;= 240);
    }
);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In order to turn this &lt;strong&gt;DataRectangle&amp;lt;bool&amp;gt;&lt;/strong&gt; mask into groups of points (where each group represents a distinct area of skin), I used a variation of the &quot;Stack based implementation&quot; from this article: &lt;a href=&quot;https://simpledevcode.wordpress.com/2015/12/29/flood-fill-algorithm-using-c-net/&quot;&gt;Flood Fill algorithm (using C#.Net)&lt;/a&gt;. If you&#39;re looking my code, it&#39;s the &quot;TryToGetPointsInObject&quot; method in the &lt;a href=&quot;https://github.com/ProductiveRage/FacialRecognition/blob/master/FaceDetection/FaceDetector.cs&quot;&gt;FaceDetector.cs&lt;/a&gt; class. I&#39;m not stressing out about performance at this point, I just wanted to get things working and then considering measuring &amp;amp; improving in the future - so finding a nice simple flood fill algorithm was very helpful (I&#39;m not saying that it&#39;s &lt;em&gt;not&lt;/em&gt; a well-performing method, I&#39;m just saying that at this point in time it&#39;s not critically important to me one way or the other).&lt;/p&gt;

&lt;p&gt;Any skin objects that are very small (have less than 64 * SCALE points) are ignored. What is slightly more complicated is to identify any completely enclosed holes in the skin object. But it&#39;s not &lt;em&gt;much&lt;/em&gt; more complicated - the basic approach is, for each skin object, take a negative point within the bounds of the skin object and use the flood fill algorithm again; if the fill reaches the edges of the bounds then the negative point was not part of an enclosed hole. Then move on to the next negative point within the bounds that hasn&#39;t already been included in a flood fill operation and continue doing this until a fully enclosed hole is found that&#39;s larger than a given size (1 * SCALE).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.productiverage.com/Content/Images/Posts/TigerWoods-FaceDetected.png&quot; alt=&quot;Tiger Woods&amp;#39; face detected&quot;&gt;&lt;/p&gt;

&lt;p&gt;With the Tiger Woods image, we could actually stop here. It successfully identifies only his face as a possible face region. However, with other test images I used, some more work was required. I found that I could quite easily eliminate a few false positives by ignoring any regions that were obviously the wrong aspect ratio (either very long and shallow sections or very tall and narrow sections). I also found that, depending upon lighting or what background a face was against, sometimes the detection process up to this point would result in a region that is too tight over the facial features - expanding the matched area a little meant that the next filtering in the next stage would get better results.&lt;/p&gt;

&lt;p&gt;The real problem, though, is false positives - the algorithm will often identify areas that are &lt;em&gt;not&lt;/em&gt; faces.&lt;/p&gt;

&lt;h2&gt;Applying this to other images&lt;/h2&gt;

&lt;p&gt;I wanted to try applying this facial detection logic to some other images. I&#39;d used the image from one of the articles so that I could try to produce intermediate images that looked similar to those in &quot;Face Detection in Color Images&quot; so that I could reassure myself that I was (at least approximately) doing things correctly as described. But now I wanted to try it on some other test images.&lt;/p&gt;

&lt;p&gt;In my Pictures folder, some photos from an Easter weekend* night out I went to a couple of years ago jumped out at me. Initially, I only used them because they made me laugh but, on closer inspection, they&#39;re actually really useful for illustrating what the skin tone detection process (and the subsequent support vector machine classification) is good at and where their limitations come into play.&lt;/p&gt;

&lt;p&gt;* &lt;em&gt;(which goes some way to explaining the eggs and the costumes.. the photos were put up on Facebook by a professional photographer, I hope he doesn&#39;t mind me reproducing them here - I tried getting in touch to ask but he didn&#39;t get back to me)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.productiverage.com/Content/Images/Posts/EggMan.jpg&quot; alt=&quot;Egg Man&quot;&gt;&lt;/p&gt;

&lt;p&gt;Firstly, we have this fine gentleman. The skin tone pass has identified his face perfectly but it&#39;s also misidentified two regions that contain colours and textureus that the skin filter allows through. &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.productiverage.com/Content/Images/Posts/EggMan-SkinMask.jpg&quot; alt=&quot;Egg Man Skin Mask&quot;&gt;&lt;/p&gt;

&lt;p&gt;Looking at the generated skin mask (where each distinct skin object is filled with a different colour), it should be clear why this has happened.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.productiverage.com/Content/Images/Posts/Group1.jpg&quot; alt=&quot;Group Photo One&quot;&gt;&lt;/p&gt;

&lt;p&gt;If we take another photo then another problem is apparent - with this group of people, there are lots of enclosed skin objects that have holes in that are being identified as faces but that are actually hands holding things.&lt;/p&gt;

&lt;p&gt;Also, because we ignore regions that are very small, there is a limit to what faces the skin tone filter will identify. If you look closely at the group photo there is an undetected bearded face along the left edge (near the top) but I am happy for it to exclude him since I think that it is a reasonable compromise to prefer faces that are in the foreground. On the whole, it does a good job of detecting faces but it may also produce a lot of false positives.&lt;/p&gt;

&lt;p&gt;So the next step is to try to find a way to filter these regions down further, to try to get rid of those pesky false positives. To do this, I went back to the internet. One of the pages that I found incredibly useful was the &quot;&lt;a href=&quot;http://mccormickml.com/2013/05/09/hog-person-detector-tutorial/&quot;&gt;HOG Person Detector Tutorial&lt;/a&gt;&quot; by Chris McCormick. He introduces a popular and modern technique, touches on its history and then goes into an illustrated explanation of how to implement it. It&#39;s almost redundant me going over it again! .. but it&#39;s &lt;em&gt;my&lt;/em&gt; blog and I&#39;m talking about how &lt;em&gt;I&lt;/em&gt; implemented it, so I&#39;m going to start from the beginning anyway :)&lt;/p&gt;

&lt;p&gt;At the very highest level, what we&#39;re going to do is this -&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Take a load of images that are known to be faces and a load of images that are known to &lt;em&gt;not&lt;/em&gt; be faces - these sets of &quot;positive&quot; and &quot;negative&quot; images are our &quot;training data&quot;&lt;/li&gt;
&lt;li&gt;Extract a load of numbers from each image - each image must be processed in a manner that results in them each producing the same amount of numbers (this is called &quot;feature extraction&quot;)&lt;/li&gt;
&lt;li&gt;The resulting data (which is a big list of entries, one from each training image, where each entry is a list of features from that image and a boolean value for whether the image was a face or not) is used to train a &quot;Support Vector Machine&quot; (which I&#39;ll explain in a second)&lt;/li&gt;
&lt;li&gt;Now that we have a trained SVM, we can use the same feature extraction process from step 2 on each of the sub-images generated by the skin tone detection process and the SVM should tell us whether each one is a face or not a face&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;So what &lt;em&gt;is&lt;/em&gt; a Support Vector Machine?&lt;/h3&gt;

&lt;p&gt;This is a concept that I had never heard of before, so I know first-hand what it&#39;s like trying to search around on the internet for a good description. The problem is that a lot of articles seem to be aimed at people who know at least &lt;em&gt;something&lt;/em&gt; about machine learning and they go immediately to showing lots of formulae and talking about techniques that I&#39;d never heard of. One example is &quot;&lt;a href=&quot;http://crsouza.com/2010/04/27/kernel-support-vector-machines-for-classification-and-regression-in-c/&quot;&gt;Kernel Support Vector Machines for Classification and Regression in C#&lt;/a&gt;&quot; by C&#233;sar Souza (who wrote much, if not all, of the Accord.NET library - which I will be using later); this is a really well-written article that I appreciate now but it was on my &quot;come back to this when you know more&quot; list for quite a while after I first found it!&lt;/p&gt;

&lt;p&gt;So I&#39;m going to take a real beginner approach and try to describe it in a manner that would have helped me a few weeks ago.&lt;/p&gt;

&lt;p&gt;Firstly, an SVM tends to be a &quot;binary classifier&quot;. This means that it will be trained with data that is all categorised as either yes or no. After it has been trained, you may only ask it to classify further data as being a yes-or-no result. (Actually, there is a such a thing as a &quot;Multi-Class Support Vector Machine&quot; that can return a greater range than binary but it&#39;s not relevant here so I&#39;m going to forget about it for now).&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(Time for some graphs. Shout out to &lt;a href=&quot;http://xkcdgraphs.com/&quot;&gt;Create your own XKCD-style Graphs&lt;/a&gt;, which helped me come up with what&#39;s below!)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.productiverage.com/Content/Images/Posts/ManagerDecisionHistory.jpg&quot; alt=&quot;Manager Decision History&quot;&gt;&lt;/p&gt;

&lt;p&gt;To conjure up an example that almost sounds feasible, imagine that every development task at a software shop requires that its strategic benefit to the company be quantified, along with the percentage of the work that a customer is just dying to pay in order for it to be done (presuming that there &lt;em&gt;is&lt;/em&gt; a customer who wants it and that it&#39;s not just a task for internal purposes). Since every task needs to have these pieces of information before your manager will decide whether or not the work will be scheduled, there exists historical records of estimated tasks which have two &quot;features&quot; (strategic value and percentage-that-will-be-immediately-paid-for-by-customers) and an outcome of either &quot;yes, it was done&quot; or &quot;no, we didn&#39;t do it&quot;. &lt;/p&gt;

&lt;p&gt;If something is of high strategic value to us &lt;em&gt;and&lt;/em&gt; it happens to be a feature that a customer is chomping at the bit for (and so will contribute significantly towards) then it seems like a dead cert that we should do it. Unfortunately, this is not a very common case.. More often, a customer will want something that is important to &lt;em&gt;them&lt;/em&gt; specifically. This may not be something that is necessarily very easy to resell to other customers (which would give it more strategic value) or something that will help us deal with internal issues, such as scaleability or technical debt (developments that &lt;em&gt;do&lt;/em&gt; help with those will also have higher strategic value).&lt;/p&gt;

&lt;p&gt;It seems like there&#39;s a roughly linear correlation between a development&#39;s strategic value, the fraction that customer(s) are willing to immediately pay for it and whether the work got the go-ahead. It&#39;s like we could draw a line and everything above it tends to get the green light by our manager and everything below it gets rejected.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.productiverage.com/Content/Images/Posts/ManagerDecisionHistory-Predictions.jpg&quot; alt=&quot;Manager Decision History&quot;&gt;&lt;/p&gt;

&lt;p&gt;An SVM is (according to &lt;a href=&quot;https://en.wikipedia.org/wiki/Support_vector_machine&quot;&gt;wikipedia&lt;/a&gt;), a:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&quot;model with associated learning algorithms.. An SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible&quot;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;What this boils down to is that we&#39;re essentially trying to come up with a formula to split those two sets of points. Then, when we get a new feature-pair (strategic value vs amount-customer-will-pay) we can plug those two numbers into the formula and work out which side of the line we&#39;re on (where one side means I-predict-yes and the other side means I-predict-no).&lt;/p&gt;

&lt;p&gt;The hard work in the machine learning algorithm is working out &lt;em&gt;where&lt;/em&gt; that line should go. Ideally, you want all historical &quot;yes&quot; results on one side and all historical &quot;no&quot; results on the other side. In some cases, though, this is not possible and there will be some outliers (remember that task that you had to do that seemed to have no strategic value, that no customer was willing to pay for and yet someone at one of those customer companies had convinced one of your directors to do it at as a favour...?). Whether there are any outliers or not, there are probably still many slight variations on what line could be drawn. It&#39;s common for an algorithm to try to arrange the line so that it is equally distant from the positive results as it is from the negative results but there are some variations on this theme. This is really what the magic is in this process - giving the computer the results and letting it find the best match.&lt;/p&gt;

&lt;h3&gt;Higher dimension planes and non-linear methods&lt;/h3&gt;

&lt;p&gt;In the above example, each data point only had two features which made it very easy to envisage on a 2D graph how they related to each other (which isn&#39;t surprising since it&#39;s an example I made up to illustrate that very point!). Sometimes, though, more data is required. It could be that you plotted strategic-value against amount-customer-will-pay and could see no correlation, though one &lt;em&gt;was&lt;/em&gt; there - but hidden due to the interaction of another feature. Imagine if you find historical data that shows that a customer wanted to pay for a new feature and the feature was of reasonable strategic value but it wasn&#39;t authorised. Or if there were a bunch features with comparatively low value that &lt;em&gt;did&lt;/em&gt; get the go-ahead. And if you realised that these were not outliers and that &lt;em&gt;lots&lt;/em&gt; of results looked to be &quot;all over the place&quot;. Well, it could be that the reason that the high value work wasn&#39;t done was because the development team were already stacked out with important work. Whereas those low value jobs that &lt;em&gt;did&lt;/em&gt; get done happened to come in during quiet periods where they could be easily slotted into the schedule. This would mean that there was actually a third factor at play; available developer resources.&lt;/p&gt;

&lt;p&gt;What would be required to model this would be to include some quantified &quot;available developer resources&quot; value with each data point. With this additional information, we should be able to construct a useful prediction model. However, the points could no longer be plotted on a simple 2D graph - now they would have to be plotted in 3D space. And so the formula that would be produced from the training data would no longer draw a simple line, it would have to describe a plane that cuts through the 3D space (with the &quot;yes&quot; values on one side and the &quot;no&quot; values on the other).&lt;/p&gt;

&lt;p&gt;If you had more than three features to each data point then you would no longer be able to draw a graph of it, your points would now live in the mysterious-sounding &quot;n-dimensional space&quot;. It&#39;s still possible to concoct a formula that divides the data point within that space, it&#39;s just not very easy to imagine it in a visual manner. On the 2D graph, it was a one-dimensional line that split the data points and allowed us to make predictions against future data points. On the 3D graph, it would be a &lt;em&gt;two&lt;/em&gt;-dimensional plane that divides the points. In n-dimensional space, it will be a &quot;hyperplane&quot; (which just means that it is a structure with one less dimension than the space that it exists in). Once you start throwing terms around like &quot;Linear SVM&quot; and &quot;hyperplane&quot;, it&#39;s hard &lt;em&gt;not&lt;/em&gt; to feel like you&#39;re making real progress towards getting a handle on all this machine learning lark! I mention these higher dimensional spaces, though, not just because they sound exciting but because they will be relevant soon..&lt;/p&gt;

&lt;p&gt;Something else that&#39;s interesting when considering SVMs is that, even with all of the required data, it still might not be possible to delineate the positive and negative results with a linear formula. If we go back to imagining a 2D graph, there could be curious curves in the data that are predictable but can not be cleanly cut with a straight line. In this case, we would have two choices - have the algorithm try to find a line that cuts the results as well as possible (but accept that this will have a lower accuracy because we know that a straight line will get a lot of predictions wrong, based upon the data we&#39;ve got) or we can allow it to try to come up with a non-linear separation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.productiverage.com/Content/Images/Posts/ManagerDecisionHistory-Stricter.jpg&quot; alt=&quot;The Stricter Manager&amp;#39;s Decision History&quot;&gt;&lt;/p&gt;

&lt;p&gt;If, say, you had a manager who leant more towards authorising tasks that had greater strategic value, even if there weren&#39;t immediately customers to finance the work (maybe they favour tasks that improve efficiency for work that customers &lt;em&gt;are&lt;/em&gt; excited about or maybe it&#39;s to lay the groundwork required before customer-led work can continue) then it might not be possible to perfectly fit a straight line to the data. One option, when trying to model this, would be to specify that the learning algorithm could use a non-linear kernel, would would hopefully generate better predictions. &lt;em&gt;Or&lt;/em&gt; you could stick with the linear approach and accept that it will be less accurate. Looking at the graph here, there is a curved line that matches the data precisely and there is a straight line that tries its best but can&#39;t help splitting the data so that some negative values are above the line (where only positive values should be) and some positive values are below the line (where only negative values should be).&lt;/p&gt;

&lt;p&gt;This is another illustration that machine learning can&#39;t perform miracles. Before, I described an example where a third feature would need to be added to the data (&quot;available developer resources&quot;) - without which it would be impossible to train a useful model. Here, we can see that sometimes we need to have some insight into what prediction models may or may not work for us (ie. we need to know that a linear kernel won&#39;t yield the best results).&lt;/p&gt;

&lt;p&gt;I&#39;m not going to dwell on this any longer here because, happily, a linear SVM &lt;em&gt;is&lt;/em&gt; a good fit for the data that we&#39;ll be working with for classifying face / not-a-face. I don&#39;t think that I yet understand enough about the subject to explain &lt;em&gt;why&lt;/em&gt; a linear model works so well for what we want to do but I &lt;em&gt;do&lt;/em&gt; have links to articles  (which I&#39;ll include at the end of this post) that seem happy to explain why this is the case.&lt;/p&gt;

&lt;h3&gt;Feature extraction for maybe-a-face images&lt;/h3&gt;

&lt;p&gt;So, if an SVM can be trained on pre-existing data and then used to decide whether other data (that it&#39;s never seen before) should be classified as a yes or a no, how can this be applied to our &quot;potential face regions&quot;? The first thing that we need to do is decide how to extract features from an image. In the &quot;Manager Decision History&quot; examples above, there were only two or three features but there&#39;s nothing stopping us from extracting many more features from our inputs. Whatever we do, though, we need to generate the &lt;em&gt;same&lt;/em&gt; number of features for every input into the SVM - so we&#39;ll almost certainly have to resize each maybe-a-face sub-image before processing it. We &lt;em&gt;could&lt;/em&gt; consider taking all of the pixel data from each resized sub-image (every red value, green value and blue value from every pixel across the entire sub-image).. but we&#39;re not going to. Not only would there be a lot of data to work with, there would also be too much variance introduced by the environment of the source photograph. We want to try to give the SVM meaningful data for it to analyse and remove irrelevant variables where possible. By &quot;irrelevant variables&quot;, I mean that it would be great if we could ignore the lighting in an image and focus on the shapes.&lt;/p&gt;

&lt;p&gt;Sometimes the lighting varies not just from one image to another but within the same image - if there is a person outside and sunlight falls on one side of them, for example. And it would be great if we could ignore the contrast and the shades of colour - if you can imagine fiddling with an image in Photoshop (or the excellent &lt;a href=&quot;http://www.getpaint.net/index.html&quot;&gt;Paint.NET&lt;/a&gt;), recolouring an image or playing with the contrast and brightness doesn&#39;t affect your ability to recognise what&#39;s in the image (unless you really stretch some of those dials to the max) and we don&#39;t want those sorts of differences to have any impact on the SVM&#39;s predictions. What we really want is to discount as much of this variance as possible when preparing the inputs (both the inputs used to train it &lt;em&gt;and&lt;/em&gt; the subsequent inputs that we want predictions for).&lt;/p&gt;

&lt;p&gt;A method that works well to achieve this is to generate normalised &quot;Histograms of Oriented Gradients&quot; from the sub-image. Again, I&#39;ve come to this conclusion from doing a lot of reading around and then attempting to implement described methods to see how they work - I&#39;ll include references to articles with more details at the end of this post.&lt;/p&gt;

&lt;p&gt;The premise is fairly simple, you need to:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Ensure that every image to process is a consistent size before starting (if an image is the wrong aspect ratio for whatever the &quot;consistent size&quot; that you decide on is then add some black bars above-below or left-right, depending upon which sides are too long/short)&lt;/li&gt;
&lt;li&gt;Greyscale the image&lt;/li&gt;
&lt;li&gt;For every pixel, work out the direction that the intensity of light on the image is changing and how quickly it&#39;s changing&lt;/li&gt;
&lt;li&gt;Split the data into blocks&lt;/li&gt;
&lt;li&gt;Reduce each block into a histogram of gradients&lt;/li&gt;
&lt;li&gt;Apply some sort of normalisation to reduce impact of contrast or brightness differences from image to image&lt;/li&gt;
&lt;li&gt;Use the resulting data as the image&#39;s feature set in the next step!&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I think that steps 3 and 5 are probably the ones that require further explanation. Since we have a greyscale image by step 3, instead of colour data we effectively have &quot;intensity&quot; values for each pixel. To work out the angle in which the intensity is changing, take the intensity of the pixel below the current one and substract it from the intensity of the pixel above the current one. This is the &quot;y difference&quot; - or &quot;delta y&quot;, so I&#39;ll name it &quot;dy&quot; (so that I can sound all maths-y). &quot;dx&quot;, as you can probably imagine, is the difference between the intensity of the pixel to the right minus the intensity of the pixel to the left. We can calculate the angle at which intensity is changing (and how big the change is) with the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var angleInDegrees = RadiansToDegrees(Math.Atan2(dx, dy)));
var magnitude = Math.Sqrt((dx * dx) + (dy * dy));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that &quot;Math.Atan2&quot; returns a value in radians. Radians are another unit of measurement of angles but instead of describing a full circle by going from 0-360 degrees, the scale goes from 0-2π radians. So, to convert from radians into easier-to-deal-with-in-this-case degrees we can do this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;private static double RadiansToDegrees(double angle)
{
    return angle * (180d / Math.PI);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Also note that Atan2 actually returns values in the range of -π to +π and so calling RadiansToDegrees on the result will give us a value from -180 to +180.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(When calculating gradient angles and magnitudes, any pixels on image edges won&#39;t have pixels surrounding in every direction and so I just record them as having both an angle and magnitude of zero)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.productiverage.com/Content/Images/Posts/HistogramExample.jpg&quot; alt=&quot;A Histogram&quot;&gt;&lt;/p&gt;

&lt;p&gt;Now for step 5, we want to produce &quot;histograms of oriented gradients&quot; (one for each block that we&#39;ve split the image into in step 4). I always envisage a histogram like a bar chart - it shows how values are distributed across a range.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(The example on the left is from &lt;a href=&quot;https://en.wikipedia.org/wiki/Histogram&quot;&gt;wikipedia&lt;/a&gt; and reproduced here under the &lt;a href=&quot;https://creativecommons.org/licenses/by-sa/3.0/&quot;&gt;creative commons&lt;/a&gt; license)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;To generate our histograns, we want to assign the gradients with the block across nine buckets, spaced twenty-degrees apart. The first will be at 10 degrees, the second at 30 and they&#39;ll go all the way up to 170.&lt;/p&gt;

&lt;p&gt;We&#39;re going to use &quot;unsigned gradients&quot;, which means that if we have a gradient where intensity is changing only vertically, we don&#39;t really care if it&#39;s getting brighter as it goes down or brighter as it goes up, we only care that the line is precisely vertical. Similarly, imagine the intensity increasing up-and-right, at 45 degrees - we&#39;re happy to treat this the same as intensity going in precisely the opposite direction; down-and-left and 225 degrees (or -135 degrees since our current angleInDegrees values are in the -180 to +180 range). What this essentially means is that we want to tweak our angles so that they are all within 0-180 (instead of -180 to +180). To do, so we just add 180 degrees to any values less than zero.&lt;/p&gt;

&lt;p&gt;Every gradient needs to be assigned to one or more of these buckets. If a gradient&#39;s angle is precisely 10 degrees then the entirety of the gradient&#39;s magnitude is added to the 10 bucket. However, if it&#39;s between 10 and 30 then its magnitude is split proportionally between 10 and 30 (so a gradient of 20 degrees is split evenly between 10 and 30 while a gradient of 25 degrees will contribute 3/4 of its magnitude to the 30 bucket - which it is 5 degrees away from - and 1/4 of its magnitude to the 10 bucket - which it is 15 degrees away from).&lt;/p&gt;

&lt;p&gt;Performing this transformation on the image is an effective way of reducing the amount of data that we need to deal with. If we decide that the standardised size of the images that we want an SVM to classify is 128x128 then we 128 x 128 x 3 = 49,152 values (since every pixel has three colour values; red, green and blue). If use a block size of 8 when generating the HOG data then the 128 x 128 image will be 16 x 16 blocks and each block has 9 values (since each histogram has values in nine bins), which gives a total of 2,304 values.&lt;/p&gt;

&lt;p&gt;Another nice thing about this representation of the data is that, if you sort of squint, you can &lt;em&gt;kind of&lt;/em&gt; make out the shapes that were in the source image -&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.productiverage.com/Content/Images/Posts/HOG-EggMan.jpg&quot; alt=&quot;Histogram of gradients render for Egg Man&amp;#39;s face&quot;&gt;&lt;/p&gt;

&lt;p&gt;If you get curious and want to try generating HOGs for your own images, there is code in my &lt;a href=&quot;https://github.com/ProductiveRage/FacialRecognition&quot;&gt;my GitHub project&lt;/a&gt; to do this..&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;using (var image = new Bitmap(imagePath))
{
    using (var resizedImage = new Bitmap(image, new Size(128, 128)))
    {
        // (Can ignore GetFor&#39;s return value when only interested in generating a preview image)
        FaceClassifier.FeatureExtractor.GetFor(
            resizedImage,
            blockSize: 8,
            optionalHogPreviewImagePath: &quot;HOG.png&quot;,
            normaliser: GlobalNormaliser.Normalise
        );
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. which brings me neatly on to HOG normalisation (since there&#39;s a mysterious reference to a normaliser in the code above). When the gradient magnitudes are calculated, some images may have many pixels that vary wildly in intensity from their neighbours while other images may have smoother, gentler gradients. In both cases, it is the relative flow of intensity that is important, since that helps identify the shapes. If you took an image and produced HOG data for it and then increased the contrast of the image and re-processed it, you would get greater gradient magnitude values in the increased-constrast version of the image, even though they both - so far as we are interested (in the context of trying to detect faces) - contain exactly the same information.&lt;/p&gt;

&lt;p&gt;What we want to do is to align all of the magnitudes to a common base line. A simple (and fairly effective) way to do this is to find the largest value of any of the buckets across all of the histograms generated for an image and to then divide &lt;em&gt;every&lt;/em&gt; magnitude by this value. In the example above, the 128x128 input image is transformed into 16x16 blocks, each of which is a histogram that contains 9 values. So we get the largest value from each of those 16x16x9 values and then divide all of them by it. This means that the largest value is now precisely 1.0 and every other value is somewhere between zero and one. This is what the &quot;GlobalNormaliser.Normalise&quot; delegate in the code above does. All it essentially has to do is this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public static DataRectangle&amp;lt;HistogramOfGradient&amp;gt; Normalise(DataRectangle&amp;lt;HistogramOfGradient&amp;gt; hogs)
{
    if (hogs == null)
        throw new ArgumentNullException(nameof(hogs));

    var maxMagnitude = hogs.Enumerate()
        .Select(pointAndHistogram =&amp;gt; pointAndHistogram.Item2)
        .Max(histogram =&amp;gt; histogram.GreatestMagnitude);
    return hogs.Transform(hog =&amp;gt; hog.Multiply(1 / maxMagnitude));
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However, there is a variation on this that has been found to produce more accurate results; &quot;block normalisation&quot;. The original description of this process comes from (as I understand it) the original research into using HOGs for this form of detection (&quot;&lt;a href=&quot;http://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf&quot;&gt;Histograms of Oriented Gradients for Human Detection [PDF]&lt;/a&gt;&quot;) -&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;For better invariance to illumination, shadowing, etc., it is also useful to contrast-normalize the local
  responses before using them. This can be done by accumulating a measure of local histogram “energy” over somewhat larger spatial regions (“blocks”) and using the results to normalize all of the cells in the block&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;What this means in practical terms is explained well by one of the articles that I linked earlier (&quot;&lt;a href=&quot;http://mccormickml.com/2013/05/09/hog-person-detector-tutorial/&quot;&gt;HOG Person Detector Tutorial&lt;/a&gt;&quot;). In essence, it means that we can get better results from normalising over smaller areas of the image. Instead of taking the max magnitude across the entire set of data, the idea is to group the histograms into blocks of four and normalising over those.&lt;/p&gt;

&lt;p&gt;Let&#39;s go back to Tiger Woods&#39; face to illustrate what I mean.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.productiverage.com/Content/Images/Posts/TigerWoods-Block1.jpg&quot; alt=&quot;First 2x2 block of histograms to normalise&quot;&gt;
&lt;img src=&quot;http://www.productiverage.com/Content/Images/Posts/TigerWoods-Block2.jpg&quot; alt=&quot;Second 2x2 block of histograms to normalise&quot;&gt;&lt;/p&gt;

&lt;p&gt;We first take the 2x2 histograms from the top left of the image - we get the max magnitude from within those four histograms and use it to normalise the values within them. These four normalised histograms will provide the first sets of values that we extract from the image.&lt;/p&gt;

&lt;p&gt;Then we move across one to get another set of 2x2 histograms and repeat the process; get the max magnitude from within those four histograms, use it to normalise them and then take those four normalised histograms as the next set of values that we have extracted from the image.&lt;/p&gt;

&lt;p&gt;What you might notice here is that, as we look at the 2x2 blocks of histograms, some of them will appear multiple times. The histograms from the edges of the image won&#39;t but the others &lt;em&gt;will&lt;/em&gt;. In the Tiger images here, you can see that the two histograms at the right hand side of the block in the first image are included &lt;em&gt;again&lt;/em&gt; in the second image (now they are the two histograms on the left hand side of the block).&lt;/p&gt;

&lt;p&gt;This means that this &quot;block normalisation&quot; process will result in more data being produced. When we &quot;globally normalised&quot; the HOGs then we had 16x16x9 = 2,304 values. However, if we block-normalise (using blocks of 2x2) then we generate 30 blocks across (there are two edge blocks that are only counted once but the other 14 blocks are all counted twice, so the total is 2 + (14&lt;em&gt;2) = 30). For the same reason, we will generate 30 blocks worth of data as we go *down&lt;/em&gt; the image. This means that we end up with a total of 30x30x9 = 8,100 values.&lt;/p&gt;

&lt;p&gt;To extract those features, we would tweak the code from before -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;const int inputWidth = 128;
const int inputHeight = 128;
const int blockSizeForHogGeneration = 8;
const int blockSizeForLocalNormalisation = 2;

IEnumerable&amp;lt;double&amp;gt; features;
using (var image = new Bitmap(imagePath))
{
    using (var resizedImage = new Bitmap(image, new Size(inputWidth, inputHeight)))
    {
        var blockNormaliser = new OverlappingBlockwiseNormaliser(blockSizeForLocalNormalisation);
        features = FaceClassifier.FeatureExtractor.GetFor(
            resizedImage,
            blockSize: blockSizeForHogGeneration,
            optionalHogPreviewImagePath: null,
            normaliser: blockNormaliser.Normalise
        );
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;(Note that I&#39;m setting &quot;optionalHogPreviewImagePath&quot; to null so that &quot;FeatureExtractor.GetFor&quot; doesn&#39;t generate a &quot;HOG preview&quot; image - this is because it&#39;s much harder to recognise the shapes that the gradients were extracted from when this form of normalisation is use since most of the HOGs appear multiple times, so the preview images are of less interest)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;When I tried comparing the results of global normalisation vs block normalisation, I found that I got better result (ie. better accuracy) when using block normalisation, just as the authors of  &quot;Histograms of Oriented Gradients for Human Detection&quot; did. The number of images that I&#39;ve been testing with is, I&#39;m sure, much smaller than the number used by Dalal and Triggs in their research but it was gratifying that I could see improvementswith my data set from using block normalisation - if only because it reassured me that I was moving in the right direction!&lt;/p&gt;

&lt;h3&gt;Training an SVM&lt;/h3&gt;

&lt;p&gt;It&#39;s really coming together now. We&#39;ve got a way to extract data from an image that we will be able to pass to an SVM in order for it to classify the image as &quot;face&quot; or &quot;not face&quot;. There&#39;s just one thing that we&#39;re missing.. a trained SVM. Or, another way to look at it, &lt;em&gt;two&lt;/em&gt; things we&#39;re missing - a mechanism to train an SVM and the data to train it with.&lt;/p&gt;

&lt;p&gt;Let&#39;s start with a general way to train an SVM. For this, I&#39;m going to use a package called &lt;a href=&quot;http://accord-framework.net/&quot;&gt;Accord.NET&lt;/a&gt;. It&#39;s a library entirely written in C#, which was a plus to me because I like to see how things work and when I was doing my early reading around on the subject of face detection/recognition, a lot of people were recommending &lt;a href=&quot;http://opencv.org/&quot;&gt;OpenCV&lt;/a&gt;. This is a C++ library (which can be called by C# using a .NET wrapper called &lt;a href=&quot;http://www.emgu.com&quot;&gt;Emgu&lt;/a&gt;), while I would be happier with C# source code that I could more confidently dig around in. (Also, I&#39;m toying with the idea of trying to port some of this work to a &lt;a href=&quot;http://bridge.net/&quot;&gt;Bridge.NET&lt;/a&gt; project so that I can try making it work in the browser - this wouldn&#39;t be possible if I took a dependency on a library like OpenCV).&lt;/p&gt;

&lt;p&gt;Accord.NET really does make it easy.. once you know where to get started. There are a lot of examples on the &lt;a href=&quot;http://accord-framework.net/samples.html&quot;&gt;accord-framework.net&lt;/a&gt; site and on the &lt;a href=&quot;https://github.com/accord-net/framework/wiki/Getting%20started&quot;&gt;GitHub wiki&lt;/a&gt;, though some of the code samples are out of date and will generate compile warnings if you try to use them directly. (Having said that, the warnings &lt;em&gt;can&lt;/em&gt; be ignored and it&#39;s not too hard to find other examples that compile without warnings - and, from reading some of the GitHub issues, I know that C&#233;sar is aware that some of the docs need updating and is intending to do so when he can make time).&lt;/p&gt;

&lt;p&gt;To demonstrate, let&#39;s revisit the &quot;Manager Decision History&quot; example from earlier. We&#39;ll formulate some example data where we pretend that the Manager is &lt;em&gt;super&lt;/em&gt;-consistent and will always authorise work if the percentage that the customer will pay for immediately (as a value between 0 and 1) plus the strategic value (also somehow quantified as a value from 0 to 1) add up to more than 1. (So strategic value 0.9 plus customer-will-pay-immediately 0.2 will be authorised as 0.9 + 0.2 &gt; 1 but strategic value 0.8 with customer-will-pay-immediately of 0.15 will &lt;em&gt;not&lt;/em&gt; be authorised as 0.8 + 0.15 &amp;lt; 1). We can then use that data to train an SVM and then try other values against the model -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Make up some data (in the real world we&#39;d use some proper pre-classified training
// data but this is just an example to demonstate how to train an SVM using Accord.NET)
var decisionHistory = Enumerable.Range(0, 10).Select(x =&amp;gt; x / 10d)
    .SelectMany(x =&amp;gt; Enumerable.Range(0, 10).Select(y =&amp;gt; y / 10d).Select(y =&amp;gt; new
    {
        StrategicValue = x,
        ImmediateCustomerContribution = y,
        Authorised = (x + y) &amp;gt; 1
    }));

// From the data, extract the input features (strategic-value and amount-customer-will-
// pay-now)..
var inputs = decisionHistory
    .Select(decision =&amp;gt; new[]
    {
        decision.StrategicValue,
        decision.ImmediateCustomerContribution
    })
    .ToArrary();

// .. and the true/false outputs for each of those sets of features
var outputs = decisionHistory.Select(decision =&amp;gt; decision.Authorised).ToArray();

// Then use the inputs and outputs to train an SVM
var smo = new SequentialMinimalOptimization&amp;lt;Linear&amp;gt;();
var svm = smo.Learn(inputs, outputs);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;strong&gt;SequentialMinimalOptimization&lt;/strong&gt; defines the process by which it will be decided when the best match has been found for the data that it&#39;s been provided. We&#39;re specified that a linear kernel be used, which means that we&#39;re presuming that it will be possible to neatly classify our data with a straight line.&lt;/p&gt;

&lt;p&gt;Now that it&#39;s been trained, we can ask the SVM to predict an output by calling its &quot;Decide&quot; method and giving it a pair of values -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var easyWin = svm.Decide(new[] { 0.81, 0.79 });
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This returns true - which is what we would expect since the numbers indicate a feature that has high strategic value (0.81) &lt;em&gt;and&lt;/em&gt; there are customers who want it so much &lt;em&gt;right now&lt;/em&gt; that they are already getting their chequebooks out (the customer-will-immediately-pay value is 79%).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var lowPriority = svm.Decide(new[] { 0.26, 0.14 });
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This returns false - which we&#39;d also expect, since the numbers indicate a feature of low strategic value and one that no customer is excited about contributing much towards the development cost of.&lt;/p&gt;

&lt;p&gt;Time for another gotcha. We saw earlier that a linear kernel is not always going to be capable of perfectly classifying the results in the training data. Sometimes you might need to use a non-linear kernel (or stick with a linear kernel but accept a lower accuracy). I didn&#39;t talk about what other kernel options there are (because it&#39;s not relevant to what I want to do here) but it was an important point that machine learning will sometimes need some external insight in order to be as effective as it can be. Another example of this is that sometimes you need to tweak the training parameters, depending upon the data that you&#39;re using. In the below example, I&#39;m going to try to train an SVM in a very similar manner to what we just looked at, but with much less data -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var decisionHistory  = new[]
{
    // Strategic wins
    new { StrategicValue = 0.95, ImmediateCustomerContribution = 0.1, Authorised = true },
    new { StrategicValue = 0.85, ImmediateCustomerContribution = 0.2, Authorised = true },

    // Customer wins
    new { StrategicValue = 0.15, ImmediateCustomerContribution = 0.9, Authorised = true },
    new { StrategicValue = 0.2, ImmediateCustomerContribution = 0.9, Authorised = true },

    // Everybody is happy
    new { StrategicValue = 0.8, ImmediateCustomerContribution = 0.8, Authorised = true },

    // Low priority
    new { StrategicValue = 0.2, ImmediateCustomerContribution = 0.1, Authorised = false },
    new { StrategicValue = 0.4, ImmediateCustomerContribution = 0.2, Authorised = false }
};

var inputs = decisionHistory
    .Select(decision =&amp;gt; new[]
    {
        decision.StrategicValue,
        decision.ImmediateCustomerContribution
    })
    .ToArrary();

var outputs = decisionHistory.Select(decision =&amp;gt; decision.Authorised).ToArray();

var smo = new SequentialMinimalOptimization&amp;lt;Linear&amp;gt;();
var svm = smo.Learn(inputs, outputs);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The training data is very similar to before in that all authorised decisions still have a feature sum of more than 1 and all rejected decisions have a sum of 1 or less. However, something seems to have gone wrong because when I ask the trained SVM what it thinks of the &quot;lowPriority&quot; example -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var lowPriority = svm.Decide(new[] { 0.26, 0.14 });
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. it returns true! This is not what I want.&lt;/p&gt;

&lt;p&gt;The only way that this could happen is if the prediction model that has been generated is completely wonky somehow. To put this to the test, I&#39;m going to use a mechanism that Accord has where you can double-check your trained SVM by running the training data back through it to see how well the prediction model managed to fit it. This can be useful in cases where you&#39;re not sure if the SVM kernel that you&#39;re using is appropriate, since it can highlight a badly-fitting model. To calculate the error rate when the training data is passed back through the SVM, do the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var predicted = svm.Decide(inputs);
var error = new ZeroOneLoss(outputs).Loss(predicted);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This just uses the model to calculate a prediction for each of the inputs and then compares the results to the expected values (then it works out what proportion are incorrect). In this case, the error rate is 0.2857142857142857, which is 2/7. Looking at the predicted values (an array of bool), every value is true! This isn&#39;t right, the last two inputs (the &quot;low priority&quot; data points) should result in a false prediction. I guess that that explains why the &quot;lowPriority&quot; example returns true from this model - it seems to return for &lt;em&gt;everything&lt;/em&gt;!&lt;/p&gt;

&lt;p&gt;We know that a linear model &lt;em&gt;will&lt;/em&gt; fit this data because we know that it&#39;s a straight line on a graph that separates everything above 1 (which are &quot;decision authorised&quot; results) from everything else (which are &quot;decision rejected&quot; results). So it&#39;s not the kernel that&#39;s the problem. The only other thing to do is to look for some options to fiddle with. Poring through the documentation, one that sounds promising is &quot;Complexity&quot; (also referred to as &quot;cost&quot;) -&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The cost parameter C controls the trade off between allowing training errors and forcing rigid margins. It creates a soft margin that permits some misclassifications. Increasing the value of C increases the cost of misclassifying points and forces the creation of a more accurate model that may not generalize well.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;That description makes it sound like a bad thing to try increasing the complexity, I think I &lt;em&gt;want&lt;/em&gt; a model that will generalise well. However, leaving the complexity at its default is clearly not working well for us in this case. So, I tried changing the &lt;strong&gt;SequentialMinimalOptimization&lt;/strong&gt; initialisation to:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var smo = new SequentialMinimalOptimization&amp;lt;Linear&amp;gt; { Complexity = 10 };
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. and re-running. This time, the predicted array precisely matched the output array, which means that the error rate is now zero. When I ask the new model to predict a value for the &quot;lowPriority&quot; features, it returns false - which is much better!&lt;/p&gt;

&lt;p&gt;I&#39;ve only experienced this problem when working with small amounts of data. To train a face classification SVM, we&#39;re going to throw a lot of training data at it and so this shouldn&#39;t be a problem (there will be no need to fiddle with Complexity or any other settings). I only mention it now in case you decide to do a few experiments of your own and fall into the same trap that I did!&lt;/p&gt;

&lt;h3&gt;Training data for faces&lt;/h3&gt;

&lt;p&gt;We have all the tools that we need now, the final piece of the puzzle is that we need training data to teach an SVM what looks like a face and what doesn&#39;t.&lt;/p&gt;

&lt;p&gt;I &lt;em&gt;could&lt;/em&gt; take the pictures that I found on my computer, run them through the skin tone face detector, manually categorise each maybe-a-face region and then use that information to train an SVM. I would use the HOG feature extractor to generate the training inputs and the list of output values would be the manual classifications that I would have to prepare (eg. sub-image-1 is a face, sub-image-2 is not a face, etc..). This should result in an SVM that could them tell apart each of the sub images automatically. However, that would be cheating! What I want to do is train a classifier with one lot of data and then use the SVM on my Easter Weekend photos to prove that it&#39;s worked. (Testing an SVM using the same data used to train it is a bit pointless, it gives you no indication whether you have produced something that is useful for general purpose or if you&#39;ve only succeeded in training an SVM that is specialised and only works with that particular set of inputs).&lt;/p&gt;

&lt;p&gt;It&#39;s crucial to train it using both positive images (ie. face images) and negative images (non-face images), otherwise the SVM will have no idea to classify. If, for example, you tried to train an SVM using only positive images then all you teach it is that &lt;em&gt;everything&lt;/em&gt; is a positive image! (By always returning true, it would produce a zero error rate for the training data but it&#39;s not very useful to alway returns true when trying to classify real work data). So we need both kinds of input and I think that we ideally want to have an equal number of positive and negative images.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(If I had to really think about it, maybe it should be the case that we want &lt;strong&gt;at least&lt;/strong&gt; as many negative images as positive as there are only so many variations of a face that exist but there are &lt;strong&gt;loads&lt;/strong&gt; of things that &lt;strong&gt;aren&#39;t&lt;/strong&gt; faces.. however, an equal number of positive/negative has worked well for me and so I haven&#39;t felt the need to experiment with different ratios)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I&#39;ve found various places that have databases of images of faces but I found it difficult to decide how to get a set of negative images. I&#39;m sure that in some things I read, people spoke about using images of scenery.. but I can&#39;t see why there should be any particular kind of negative image (such as scenery) that should be used - it could be &lt;em&gt;anything&lt;/em&gt; (so long as it&#39;s not a face)!&lt;/p&gt;

&lt;p&gt;What I did in the end was download the &quot;&lt;a href=&quot;http://www.vision.caltech.edu/Image_Datasets/Caltech_10K_WebFaces/&quot;&gt;Caltech 10,000 Web Faces&lt;/a&gt;&quot; data set, which includes lots of photos downloaded from various Google image searches along with a text file that, crucially, has coordinates of the faces in the photos. From this data set, I extracted the faces from images but also extracted other parts of the images at random that &lt;em&gt;didn&#39;t&lt;/em&gt; contain faces. For each face entry in the &quot;Ground Truth&quot; text file, I extracted three images - one where the edges of the sub-image were close around the face and then two where there was a little more background around the face. This should help produce an SVM that can recognise faces when the region around the face is very tight and when it&#39;s less so, which is important for classifying the skin tone face detection results - Tiger Woods&#39; detected-face-region is quite tight while the other photos show that much looser face regions may be identified for other photos.&lt;/p&gt;

&lt;p&gt;There&#39;s no exciting implementation details here. The &quot;&lt;a href=&quot;https://github.com/ProductiveRage/FacialRecognition/blob/master/FaceClassifier/CalTechWebFacesSvmTrainer.cs&quot;&gt;CalTechWebFacesSvmTrainer&lt;/a&gt;&quot; class is given a bunch of configuration options: path containing the CalTech web face images, path to the Ground Truth text file (which lists all of the face regions in the images), standard size of image to use to generate SVM inputs (128x128), block size for HOG generation (8), normaliser (the &lt;strong&gt;OverlappingBlockwiseNormaliser&lt;/strong&gt; we saw earlier, with block size 2) and number of training images to process. These options will train an SVM on positive and negative images from the Caltech data set and the &quot;TrainFromCaltechData&quot; method will return an &lt;strong&gt;IClassifyPotentialFaces&lt;/strong&gt; implementation, which wraps the SVM and exposes has a single method -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bool IsFace(Bitmap image);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, we can take the possible-face sub-images identified by the skin tone pass and pass them to &quot;IsFace&quot;. This method will resize the specified Bitmap to the standard image size that the SVM requires, generate normalised HOG data from it and then query the SVM - which returns the final result; &lt;em&gt;is this or is this not a face?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The only configuration option that I haven&#39;t mentioned until now is &quot;number of training images to process&quot;. We saw before that trying to train an SVM with very little data can be ineffective (unless you know what kernel settings to fiddle with) but it&#39;s very difficult to come up with a hard and fast rule of how much data &lt;em&gt;is&lt;/em&gt; enough to train from. I think that the best thing to do is just to experiment. My gut told me that it would surely have to be at least 100s or 1000s of images since the SVM has to be trained to classify many variations of faces (and not-faces) and each of the inputs has a lot of values (8,100 - as we calculated earlier) and so it seems like it&#39;s going to need to have a lot of information at its disposal so that it can work out what is and isn&#39;t important.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.productiverage.com/Content/Images/Posts/Dancing.jpg&quot; alt=&quot;Dancing Classification Fail&quot;&gt;&lt;/p&gt;

&lt;p&gt;So, initially, I tried specifying 500 training images, which resulted in an SVM that actually did a pretty good job. The group photo shown before had all of the maybe-face regions correctly classified (the faces were all identified as faces and the hands in the photos were all correctly classified as not-faces).&lt;/p&gt;

&lt;p&gt;However, there was a problem with the image shown here (the problem, btw, clearly &lt;em&gt;isn&#39;t&lt;/em&gt; the moves being thrown on the dancefloor, since they look like the very essence of poetry in motion). The problem is that one of the maybe-face regions identified by the skin tone pass has been mis-classified by the SVM as a face, when it&#39;s really an arm.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(Note that there are some faces in the background that were not identified by the skin tone pass, but I&#39;m not worried about that - it is to be expected that they would be ignored because they are small, relative to the size of the image.. though it should be possible to tweak the skin tone filter parameters if you wanted to try to capture background faces).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Increasing the number of training images to 1,000 didn&#39;t address the problem with this particular image and, in fact, made another image worse (where a non-face region was correctly classifed as not-a-face before, with 1,000 training images the SVM thought that it &lt;em&gt;was&lt;/em&gt; a face). Increasing to 1,500 training images corrected the classification of the dancing image regions but there were still false positives in other images. Such as this one of that same individual who appears to now be wearing a flag (which reminds me of a scene from &lt;em&gt;Fear and Loathing in Las Vegas&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.productiverage.com/Content/Images/Posts/FlagWearer.jpg&quot; alt=&quot;Another Classification Failure&quot;&gt;&lt;/p&gt;

&lt;p&gt;2,000 training images seemed to be the sweet spot. The classifier produced the correct results for every case that I expected it to.&lt;/p&gt;

&lt;p&gt;The quantity of training data is going to vary from application to application and will depend upon what sort of feature extraction logic you are using. I think that it makes sense to start with lower quantities, if only because it&#39;s faster to train an SVM with less data and you can experiment more quickly with the other variables. To illustrate, on my computer (with the code as it currently stands, which is in a completely unoptimised state, but running a release build) it takes 17s to load the 2,000 images and extract the features from them, it then takes 27s to train the SVM on them. With 500 images it takes 5s to load the image data and 2s to train the SVM. With 5,000 images it takes 42s and 168s.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.productiverage.com/Content/Images/Posts/EggMan-Final.jpg&quot; alt=&quot;Egg Man&amp;#39;s Glorious Face&quot;&gt;&lt;/p&gt;

&lt;p&gt;There&#39;s one final tweak that I&#39;d like to mention before bringing everything to a close. The SVM requires that the inputs be fixed dimensions, so the &quot;IsFace&quot; implementation resizes the input image if it&#39;s too big or too small. If it&#39;s the wrong &lt;em&gt;dimensions&lt;/em&gt;, though, then it leaves black bars above-and-below or left-and-right of the input (which is part of the process that I described in &quot;Feature extraction for maybe-a-face images&quot;). I found that the classification accuracy improved slightly if I expanded the maybe-a-face region so that it matched the aspect ratio of the SVM&#39;s input dimensions first. For example, if the skin tone pass identified a face region that was 91x241px and I knew that the SVM was configured to work with input images of 128x128px then I would first expand the 91x241px region to 241x241px (similarly, if I had a region that was 123x56px then I would expand it to 123x123px).&lt;/p&gt;

&lt;h3&gt;Performance&lt;/h3&gt;

&lt;p&gt;In the context of this kind of work, &quot;performance&quot; relates to two distinct areas - how &lt;em&gt;accurately&lt;/em&gt; does it perform its work and how &lt;em&gt;quickly&lt;/em&gt; does it do it? To an extent, the two are related since we could make the skin tone filter work more quickly (if we reduced the limit from no-input-dimension-may-be-more-than-400px to 300px or 200px) but at the risk of it no longer identifying regions that we want it to. As we&#39;ve just seen with the SVM training, the time it takes to train it depends upon the quantity of data (both the size of the input image that features are extracted from and the number of input images) - so the inputs could be reduced (so that the training time would be shorter) but this negatively affects the accuracy of the classifier.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(A handy tip that I used when I was happy with the SVM training but experimenting with the skin tone pass was that the Accord.NET SVM class may be serialised using the BinaryFormatter - so I was persisting it to disk between runs, rather than having to re-train it on every run)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Once the SVM is trained, it is very quick at classifying. Which makes sense, since all it needs to do is take a feature set, apply its linear formula and see which side of the hyperplane the data point sits on. There is a little work required to extract the feature data from a maybe-face sub-image in order to give the SVM something to work on but it takes less then 15ms per sub-image on my computer. The skin tone pass is slower, taking between 150 and 300ms for the images that I&#39;ve been testing with (which are all about 1000px on the largest side and so are resized down to 400px before being processed). I&#39;d like to cut this time down because I like the idea of being able to apply this processing to video and I feel that it needs to be able to process at least five frames a second to convincingly track faces. I haven&#39;t made any effort towards this yet but reducing the total time down to 200ms feels reasonable since the code has been written to be easy to follow and tweak, rather than to be fast, so there is surely plenty of performance tuning potential.&lt;/p&gt;

&lt;p&gt;Another reason that I haven&#39;t spent any time optimising the current code is that it might be wiser to spend time researching alternative algorithms. For example, Accord.NET has some support for face detection, such as the &lt;strong&gt;FaceHaarCascade&lt;/strong&gt; class (which I believe is the same class of detector as the &lt;a href=&quot;https://en.wikipedia.org/wiki/Viola%E2%80%93Jones_object_detection_framework&quot;&gt;Viola–Jones object detection framework&lt;/a&gt;) -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Rectangle[] faceRegions;
using (var image = new Bitmap(path))
{
    var detector = new HaarObjectDetector(new FaceHaarCascade());
    faceRegions = detector.ProcessFrame(image);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However, it gave poor results for my images. It seemed to do well with very small faces but struggled with  larger images (or the images where the faces took up a large proportion of the image). It&#39;s very possible that I could improve the accuracy by tweaking the parameters. If my images are too large, maybe they should just be shrunk down further before processing? That might help the false negative rate but I&#39;m also concerned about the false positive rate and I&#39;m not sure that shrinking the image first would help with that.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.productiverage.com/Content/Images/Posts/TigerWoods-SlidingWindow.jpg&quot; alt=&quot;TigerWoods - Sliding Window SVM Results&quot;&gt;&lt;/p&gt;

&lt;p&gt;One approach that sounds very promising: if the SVM classifier is so fast and effective then maybe we should get rid of the skin tone processing altogether and just generate segments from all over the image, then filter them with the classifier. This is discussed and recommended by many people and is referred to as the &quot;sliding window&quot; approach (see &lt;a href=&quot;http://www.pyimagesearch.com/2015/03/23/sliding-windows-for-object-detection-with-python-and-opencv/&quot;&gt;Sliding Windows for Object Detection with Python and OpenCV&lt;/a&gt;, for example). The idea is that you pick a window size (eg. 50px if limiting the source image to no more than 400px on a side) and then start at the top left of the image and take that top-left 50x50px as the first sub-image to pass to the classifer. Then you move the window over a bit (say, 10px) and then use that as the second sub-image. Keep going until you can&#39;t go any further and then go back to the left but go down 10px - moving across then back-and-down until you have covered the entire image. Then do the same with some other window sizes (maybe 75px and then 100px), since you don&#39;t know whether the faces on an image are going to be small (relative to the size of the image) or large. This will give you many, &lt;em&gt;many&lt;/em&gt; regions to classify - but that&#39;s no problem because linear SVM classification is very fast. Unfortunately, I did try this but I got an enormous false positive rate (as you can see here with Tiger - if you can make him out from behind all the green &quot;this is a face!&quot; boxes). Surely this approach &lt;em&gt;can&lt;/em&gt; be made to work since it&#39;s so commonly presented as a solution to this problem.. it may just be that I need to tweak how the SVM is trained, maybe it needs more than 2,000 training images (though I did also try it with 10,000 training images and the results were not much better).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.productiverage.com/Content/Images/Posts/BlackAndWhiteFaceFails.jpg&quot; alt=&quot;Black and White Face Detection Fail&quot;&gt;&lt;/p&gt;

&lt;p&gt;To return to the performance of the skin-tone-pass-following-by-(2k-image-trained-)SVM-classification model that I&#39;ve talked about for most of this post; in terms of accuracy I&#39;m very happy with it. In the test images that I&#39;ve been using, there are zero false positives. There are a couple of false negatives but I think that they are understandable. The first obvious failing that it has is with a black-and-white photo - since the skin tone face detector looks for particular hues and saturations, it&#39;s not going to work on if there is no colour in the image. That&#39;s just a built-in limitation of the approach.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.productiverage.com/Content/Images/Posts/OccludedFace.jpg&quot; alt=&quot;Semi-occluded&quot;&gt;&lt;/p&gt;

&lt;p&gt;Another failure is with a face that is half hidden behind someone else (in the top right of this group photo). The skin tone face detector identifies the face but the SVM classifies it as not-a-face. Since the face is half-hidden, I think that this is acceptable.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.productiverage.com/Content/Images/Posts/VomitSkinMask.jpg&quot; alt=&quot;Vomit shot with skin mask&quot;&gt;
&lt;img src=&quot;http://www.productiverage.com/Content/Images/Posts/VomitFalseNegative.jpg&quot; alt=&quot;Vomit shot false negative&quot;&gt;&lt;/p&gt;

&lt;p&gt;Last but not least is this unfortunate guy who foolishly entered some sort of drinking contest. I don&#39;t know if he won but he doesn&#39;t look too good about it one way or the other. The red liquid erupting from his lips falls within the first phase&#39;s looks-like-skin-tones bounds, as you can see by looking at the skin mask image. Not only is his head tilted in the shot but the vomit-detected-as-skin means that his face isn&#39;t centered in the maybe-face region. So I&#39;m not surprised that the SVM doesn&#39;t correctly classify it.&lt;/p&gt;

&lt;p&gt;In summary, it&#39;s not perfect but I&#39;m still very happy with the results.&lt;/p&gt;

&lt;h3&gt;Article references&lt;/h3&gt;

&lt;p&gt;I want to close this post off by linking to the sites and papers that helped me get to this point. It&#39;s been a fun journey and is hopefully only my first step on the machine learning track!&lt;/p&gt;

&lt;p&gt;The &quot;&lt;a href=&quot;http://mfleck.cs.illinois.edu/naked-skin.html&quot;&gt;Naked People Skin Filter (Fleck &amp;amp; Forsyth)&lt;/a&gt;&quot; and &quot;&lt;a href=&quot;http://web.archive.org/web/20090723024922/http:/geocities.com/jaykapur/face.html&quot;&gt;Face Detection in Color Images&lt;/a&gt;&quot; papers were amongst the first that I found that had easily understandable material (for a face detection novice) that didn&#39;t use a third party library in some way.&lt;/p&gt;

&lt;p&gt;Once I had implemented the skin tone logic and was looking to improve the results, one of the articles that I found most inspiring (partly because it was so approachable) was &quot;&lt;a href=&quot;https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78#.v32w6yht7&quot;&gt;Machine Learning is Fun! Part 4: Modern Face Recognition with Deep Learning&lt;/a&gt;&quot; (by Adam Geitgey). This talks about taking things to the next step and performing facial &lt;em&gt;recognition&lt;/em&gt; - not just locating faces but also trying to identify who they are, against a known database. I haven&#39;t got to that advanced point yet but it&#39;s definitely something that I&#39;d like to explore. The only thing that I would say about this article is that it doesn&#39;t go into a lot of depth about generating HOGs or normalising them or what sort of input image size and other parameters to use. He does link to some Python code that uses third party libraries in lieu of more details, though. Since I wanted to write C# and (ideally) only take C# dependencies this wasn&#39;t very helpful to me. The article itself, though, got me really excited about the possibilities and I enjoyed the guy&#39;s writing style (and intend to read the rest of his &quot;Machine Learning is Fun&quot; series).&lt;/p&gt;

&lt;p&gt;An article that &lt;em&gt;did&lt;/em&gt; go into the details was the &quot;&lt;a href=&quot;http://mccormickml.com/2013/05/09/hog-person-detector-tutorial/&quot;&gt;HOG Person Detector Tutorial&lt;/a&gt;&quot; (by Chris McCormick). This was also really well written and very approachable. Although it talks about detecting a person in a photo, rather than a face, the principles are the same. The description of the HOG generation and normalisation really helped me clarify in my mind how it should all work. This article links to the original &lt;a href=&quot;http://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf&quot;&gt;HOG person detector paper by Dalal and Triggs [PDF]&lt;/a&gt;, which is full of research information and graphs and equations - so if you want to dig deep then this is the place to start!&lt;/p&gt;

&lt;p&gt;The Triggs and Dalal paper state in the introduction that &quot;For simplicity and speed, we use linear SVM as a baseline classifier throughout the study&quot;. Later on they mention that they tried a non-linear kernel but that &quot;Replacing the linear SVM with a Gaussian kernel one improves performance by about 3%.. at the cost of much higher run times&quot;. If this &quot;why is a linear SVM classifier appropriate&quot; explanation feels too light for you then it is discussed in greater depth in &quot;&lt;a href=&quot;https://arxiv.org/pdf/1406.2419.pdf&quot;&gt;Why do linear SVMs trained on HOG features perform so well? [PDF]&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Finally, I want to mention again the article &quot;&lt;a href=&quot;http://crsouza.com/2010/04/27/kernel-support-vector-machines-for-classification-and-regression-in-c/&quot;&gt;Kernel Support Vector Machines for Classification and Regression in C#&lt;/a&gt;&quot; by Accord.NET&#39;s C&#233;sar Souza. Here, he introduces the SVM in a much more formal way than I&#39;ve done here, he includes a video about &quot;&lt;a href=&quot;https://www.youtube.com/watch?v=3liCbRZPrZA&quot;&gt;The Kernel Trick&lt;/a&gt;&quot; (which is a simple yet mind-blowing way in which the linear kernel can be used to classify data that doesn&#39;t immediately look like it would work for), he talks about other kernels (aside from linear he also describes polynomial and gaussian) and he describes how the Sequential Minimal Optimization learning algorithm works. There&#39;s a lot of code and there&#39;s illustrated examples at the end. When I found this article fairly early on in my research, I recognised how much care had gone into preparing it but also knew that I needed something a little more beginner-level first! So I bookmarked it for future reference and now I think that I understand (almost all of) it. Hopefully, if you started off knowing nothing but have followed all the way through this post, then you will too!&lt;/p&gt;</description>
			<pubDate>Sun, 12 Feb 2017 18:39:00 GMT</pubDate>
		</item>
		<item>
			<title>Migrating my Full Text Indexer to .NET Core (supporting multi-target NuGet packages)</title>
            <link>http://www.productiverage.com/migrating-my-full-text-indexer-to-net-core-supporting-multitarget-nuget-packages</link>
			<guid>http://www.productiverage.com/migrating-my-full-text-indexer-to-net-core-supporting-multitarget-nuget-packages</guid>
			<description>&lt;p&gt;So it looks increasingly like .NET Core is going to be an important technology in the near future, in part because Microsoft is developing much of it in the open (in a significant break from their past approach to software), in part because some popular projects support it (&lt;a href=&quot;https://github.com/StackExchange/dapper-dot-net&quot;&gt;Dapper&lt;/a&gt;, &lt;a href=&quot;http://automapper.org/&quot;&gt;AutoMapper&lt;/a&gt;, &lt;a href=&quot;http://www.newtonsoft.com/json&quot;&gt;Json.NET&lt;/a&gt;) and in part because of excitement from blog posts such as &lt;a href=&quot;http://web.ageofascent.com/asp-net-core-exeeds-1-15-million-requests-12-6-gbps/&quot;&gt;ASP.NET Core – 2300% More Requests Served Per Second&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;All I really knew about it was that it was a cut-down version of the .NET framework which should be able to run on platforms other than Windows, which &lt;em&gt;might&lt;/em&gt; be faster in some cases and which may still undergo some important changes in the near future (such as moving away from the new &quot;project.json&quot; project files and back to something more traditional in terms of Visual Studio projects - see &lt;a href=&quot;https://wildermuth.com/2016/05/12/The-Future-of-project-json-in-ASP-NET-Core&quot;&gt;The Future of project.json in ASP.NET Core&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;To try to find out more, I&#39;ve taken a codebase that I wrote years ago and have migrated it to .NET Core. It&#39;s not enormous but it spans multiple projects, has a (small-but-better-than-nothing) test suite and supports serialising search indexes to and from disk for caching purposes. My hope was that I would be able to probe some of the limitations of .NET Core with this non-trivial project but that it wouldn&#39;t be such a large task that it take more than a few sessions spaced over a few days to complete.&lt;/p&gt;

&lt;h4&gt;Spoilers&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Would I be able to migrate one project at a time within the solution to .NET Core and still have the whole project building successfully (while some of the other projects were still targeting the &quot;full fat&quot; .NET framework)?&lt;/strong&gt; Yes, but some hacks are required.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Would it be easy (or even possible) to create a NuGet package that would work on both .NET Core and .NET 4.5?&lt;/strong&gt; Yes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Would the functionality that is no longer present in .NET Core cause me problems?&lt;/strong&gt; Largely, no. The restricted reflection capabilities, no - if I pull in an extra dependency. The restricted serialisation facilities, &lt;em&gt;yes&lt;/em&gt; (but I&#39;m fairly happy with the solution and compromises that I ended up with).&lt;/p&gt;

&lt;h3&gt;What, really, is .NET Core (and what is the Full Text Indexer)?&lt;/h3&gt;

&lt;p&gt;Essentially,&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;.NET Core is a new cross-platform .NET Product .. [and] is composed of the following parts: A .NET runtime .. A set of framework libraries .. A set of SDK tools and language compilers .. The &#39;dotnet&#39; app host, which is used to launch .NET Core apps&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;em&gt;(from Scott Hanselman&#39;s &quot;&lt;a href=&quot;http://www.hanselman.com/blog/NETCore10IsNowReleased.aspx&quot;&gt;.NET Core 1.0 is now released!.NET Core 1.0 is now released!&lt;/a&gt;&quot; post)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;What .NET Core means in the context of this migration is that there are new project types in Visual Studio to use that target new .NET frameworks. Instead of .NET 4.6.1, for example, there is &quot;netstandard1.6&quot; for class libraries and &quot;netcoreapp1.0&quot; for console applications.&lt;/p&gt;

&lt;p&gt;The new Visual Studio project types become available after you install the &lt;a href=&quot;https://www.microsoft.com/net/core#windows&quot;&gt;Visual Studio Tooling&lt;/a&gt; - alternatively, the &quot;dotnet&quot; command line tool makes things very easy so you can could create projects using nothing more than notepad and &quot;dotnet&quot; if you want to! Since I was just getting started, I chose to stick in my Visual Studio comfort zone.&lt;/p&gt;

&lt;p&gt;The Full Text Indexer code that I&#39;m migrating was something that I wrote a few years ago while I was working with a Lucene integration (&quot;this full text indexing lark.. &lt;em&gt;how hard could it really be!&lt;/em&gt;&quot;). It&#39;s a set of class libraries; &quot;Common&quot; (which has no dependencies other than the .NET framework), &quot;Core&quot; (which depends upon Common), &quot;Helpers&quot; (which depends upon both Common and Core), and &quot;Querier&quot; (which also depends upon Common and Core). Then there is a &quot;UnitTests&quot; project and a &quot;Tester&quot; console application, which loads some data from a Sqlite database file, constructs an index and then performs a search or two (just to demonstrate how it works end-to-end).&lt;/p&gt;

&lt;p&gt;My plan was to try migrating one project at a time over to .NET Core, to move in baby steps so that I could be confident that everything would remain in a working state for most of the time.&lt;/p&gt;

&lt;h3&gt;Creating the first .NET Core project&lt;/h3&gt;

&lt;p&gt;The first thing I did was delete the &quot;Common&quot; project entirely (deleted it from Visual Studio and then manually deleted all of the files) and then created a brand new .NET Core class library called &quot;Common&quot;. I then used my source control client to revert the deletions of the class files so that they appeared within the new project&#39;s folder structure. I expected to then have to &quot;Show All Files&quot; and explicitly include these files in the project but it turns out that .NET Core project files don&#39;t specify files to include, it&#39;s presumed that all files in the folder will be included. Makes sense!&lt;/p&gt;

&lt;p&gt;It wouldn&#39;t compile, though, because some of the classes have the &lt;strong&gt;[Serializable]&lt;/strong&gt; attribute on them and this doesn&#39;t exist in .NET Core. As I understand it, that&#39;s because the framework&#39;s serialisation mechanisms have been stripped right back with the intention of the framework being able to specialise at framework-y core competencies and for there to be an increased reliance on external libraries for other functionality.&lt;/p&gt;

&lt;p&gt;This attribute is used through my library because there is an &lt;strong&gt;IndexDataSerialiser&lt;/strong&gt; that allows an index to be persisted to disk for caching purposes. It uses the &lt;strong&gt;BinaryFormatter&lt;/strong&gt; to do this, which requires that the types that you need to be serialised be decorated with the &lt;strong&gt;[Serializable]&lt;/strong&gt; attribute or they implement the &lt;strong&gt;ISerializable&lt;/strong&gt; interface. Neither the &lt;strong&gt;BinaryFormatter&lt;/strong&gt; nor the &lt;strong&gt;ISerializable&lt;/strong&gt; interface are available within .NET Core. I will need to decide what to do about this later - ideally, I&#39;d like to continue to be able to support reading and writing to the same format as I have done before (if only to see if it&#39;s possible when migrating to Core). For now, though, I&#39;ll just remove the &lt;strong&gt;[Serializable]&lt;/strong&gt; attributes and worry about it later.&lt;/p&gt;

&lt;p&gt;So, with very little work, the Common project was compiling for the &quot;netstandard1.6&quot; target framework.&lt;/p&gt;

&lt;p&gt;Unfortunately, the projects that rely on Common weren&#39;t compiling because their references to it were removed when I removed the project from the VS solution. And, if I try to add references to the new Common project I&#39;m greeted with this:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;A reference to &#39;Common&#39; could not be added. An assembly must have a &#39;dll&#39; or &#39;exe&#39; extension in order to be referenced.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The problem is that Common is being built for &quot;netstandard1.6&quot; but &lt;em&gt;only&lt;/em&gt; that framework. I also want it to support a &quot;full fat&quot; .NET framework, like 4.5.2 - in order to do this I need to edit the project.json file so that the build process creates multiple versions of the project, one .NET 4.5.2 as well as the one for netstandard. That means changing it from this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &quot;version&quot;: &quot;1.0.0-*&quot;,

  &quot;dependencies&quot;: {
    &quot;NETStandard.Library&quot;: &quot;1.6.0&quot;
  },

  &quot;frameworks&quot;: {
    &quot;netstandard1.6&quot;: {
      &quot;imports&quot;: &quot;dnxcore50&quot;
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;to this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &quot;version&quot;: &quot;1.0.0-*&quot;,

  &quot;dependencies&quot;: {},

  &quot;frameworks&quot;: {
    &quot;netstandard1.6&quot;: {
      &quot;imports&quot;: &quot;dnxcore50&quot;,
      &quot;dependencies&quot;: {
        &quot;NETStandard.Library&quot;: &quot;1.6.0&quot;
      }
    },
    &quot;net452&quot;: {}
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Two things have happened - an additional entry has been added to the &quot;frameworks&quot; section (&quot;net452&quot; joins &quot;netstandard1.6&quot;) and the &quot;NETStandard.Library&quot; dependency has moved from being something that is always required by the project to something that is only required by the project when it&#39;s being built for netstandard.&lt;/p&gt;

&lt;p&gt;Now, Common may be added as a reference to the other projects.&lt;/p&gt;

&lt;p&gt;However.. they won&#39;t compile. Visual Studio will be full of errors that required classes do not exist in the current context.&lt;/p&gt;

&lt;h3&gt;Adding a reference to a .NET Core project from a .NET 4.5.2 project in the same solution&lt;/h3&gt;

&lt;p&gt;Although the project.json configuration does mean that two version of the Common library are being produced (looking in the bin/Debug folder, there are two sub-folders &quot;net452&quot; and &quot;netstandard1.6&quot; and each have their own binaries in), it seems that the &quot;Add Reference&quot; functionality in Visual Studio doesn&#39;t (currently) support adding references. There is an issue on GitHub about this; &lt;a href=&quot;https://github.com/dotnet/core/issues/231&quot;&gt;Allow &quot;Add Reference&quot; to .NET Core class library that uses .NET Framework from a traditional class library&lt;/a&gt; but it seems like the conclusion is that this will be fixed in the future, when the changes have been completed that move away from .NET Core projects having a project.json file and towards a new kind of &quot;.csproj&quot; file.&lt;/p&gt;

&lt;p&gt;There is a workaround, though. Instead of selecting the project from the Add Reference dialog, you click &quot;Browse&quot; and then select that file in the &quot;Common/bin/Debug/net452&quot; folder. Then the project &lt;em&gt;will&lt;/em&gt; build. This isn&#39;t a perfect solution, though, since it will &lt;em&gt;always&lt;/em&gt; reference the Debug build. When building in Release configuration, you also want the referenced binaries from other projects to be built in Release configuration. To do that, I had to open each &quot;.csproj&quot; file notepad and change&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;Reference Include=&quot;Common&quot;&amp;gt;
  &amp;lt;HintPath&amp;gt;..\Common\bin\Debug\net452\Common.dll&amp;lt;/HintPath&amp;gt;
&amp;lt;/Reference&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;to&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;Reference Include=&quot;Common&quot;&amp;gt;
  &amp;lt;HintPath&amp;gt;..\Common\bin\$(Configuration)\net452\Common.dll&amp;lt;/HintPath&amp;gt;
&amp;lt;/Reference&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A little bit annoying but not the end of the world (credit for this fix, btw, goes to this Stack Overflow answer to &lt;a href=&quot;http://stackoverflow.com/a/37323585/3813189&quot;&gt;Attach unit tests to ASP.NET Core project&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;What makes it even more annoying is the link from the referencing project (say, the Core project) to the &lt;em&gt;referenced&lt;/em&gt; project (the Common project) is not as tightly integrated as when a project reference is normally added through Visual Studio. For example, while you can set breakpoints on the Common project and they will be hit when the Core project calls into that code, using &quot;Go To Definition&quot; to navigate from code in the Core project into code in the referenced Common project &lt;em&gt;doesn&#39;t&lt;/em&gt; work (it takes you to a &quot;from metadata&quot; view rather than taking you to the actual file). On top of this, the referencing project doesn&#39;t know that it needs to be rebuilt if the referenced project is rebuilt - so, if the Common library is changed and rebuilt then the Core library may continue to work against an old version of the Common binary unless you explicitly rebuild Core as well.&lt;/p&gt;

&lt;p&gt;These are frustrations that I would not want to live with long term. However, the plan here is to migrate all of the projects over to .NET Core and so I think that I can put up with these limitations so long as they only affect me as I migrate the projects over one-by-one.&lt;/p&gt;

&lt;h3&gt;The second project (additional dependencies required)&lt;/h3&gt;

&lt;p&gt;I repeated the procedure for second project; &quot;Core&quot;. This also contained files with types marked as &lt;strong&gt;[Serializable]&lt;/strong&gt; (which I just removed for now) and there was the &lt;strong&gt;IndexDataSerialiser&lt;/strong&gt; class that used the &lt;strong&gt;BinaryFormatter&lt;/strong&gt; to allow data to be persisted to disk - this also had to go, since there was no support for it in .NET Core (I&#39;ll talk about what I did with serialisation later on). I needed to add a reference to the Common project - thankfully adding a reference to a .NET Core project &lt;em&gt;from&lt;/em&gt; a .NET Core project works perfectly, so the workaround that I had to apply before (when the Core project was still .NET 4.5.2) wasn&#39;t necessary.&lt;/p&gt;

&lt;p&gt;However, it still didn&#39;t compile.&lt;/p&gt;

&lt;p&gt;In the &quot;Core&quot; project lives the &lt;strong&gt;EnglishPluralityStringNormaliser&lt;/strong&gt; class, which
is used to adjust tokens (ie. words) so that the singular and plural versions of the same word are considered equivalent (eg. &quot;cat&quot; and &quot;cats&quot;, &quot;category&quot; and &quot;categories&quot;). Internally, it generates a compiled LINQ expression to try to perform its work as efficiently as possible and it requires reflection to do that. Calling &quot;GetMethod&quot; and &quot;GetProperty&quot; on a &lt;strong&gt;Type&lt;/strong&gt; is not supported in netstandard, though, and an additional dependency is required. So the Core project.json file needed to be changed to look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &quot;version&quot;: &quot;1.0.0-*&quot;,

  &quot;dependencies&quot;: {
    &quot;Common&quot;: &quot;1.0.0-*&quot;
  },

  &quot;frameworks&quot;: {
    &quot;netstandard1.6&quot;: {
      &quot;imports&quot;: &quot;dnxcore50&quot;,
      &quot;dependencies&quot;: {
        &quot;NETStandard.Library&quot;: &quot;1.6.0&quot;
        &quot;System.Reflection.TypeExtensions&quot;: &quot;4.1.0&quot;
      }
    },
    &quot;net452&quot;: {}
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The Common project is a dependency regardless of what the target framework is during the build process but the &quot;System.Reflection.TypeExtensions&quot; package is also required when building for netstandard (but not .NET 4.5.2), as this includes extensions methods for &lt;strong&gt;Type&lt;/strong&gt; such as &quot;GetMethod&quot; and &quot;GetProperty&quot;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note: Since these are extension methods in netstandard, a &quot;using System.Reflection;&quot; statement is required at the top of the class - this is not required when building for .NET 4.5.2 because &quot;GetMethod&quot; and &quot;GetProperty&quot; are instance methods on &lt;strong&gt;Type&lt;/strong&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;There was one other dependency that was required for Core to build - &quot;System.Globalization.Extensions&quot;. This was because the &lt;strong&gt;DefaultStringNormaliser&lt;/strong&gt; class includes the line&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var normalisedValue = value.Normalize(NormalizationForm.FormKD);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;which resulted in the error&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&#39;string&#39; does not contain a definition for &#39;Normalize&#39; and no extension method &#39;Normalize&#39; accepting a first argument of type &#39;string&#39; could be found (are you missing a using directive or an assembly reference?)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is another case of functionality that is in .NET 4.5.2 but that is an optional package for .NET Core. Thankfully, it&#39;s easy to find out what additional package needs to be included - the &quot;lightbulb&quot; code fix options will try to look for a package to resolve the problem and it correctly identifies that &quot;System.Globalization.Extensions&quot; contains a relevant extension method (as illustrated below).&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;TODO&quot; src=&quot;http://www.productiverage.com/Content/Images/Posts/SystemGlobalizationExtensionsDependency.png&quot; class=&quot;AlwaysFullWidth NoBorder&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note: Selecting the &quot;Add package System.Globalization.Extensions 4.0.1&quot; option will add the package as a dependecy for netstandard in the project.json file and it will add the required &quot;using System.Globalization;&quot; statement to the class - which is very helpful of it!&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;All that remained now was to use the workaround from before to add the .NET Core version of the &quot;Core&quot; project as a reference to the projects that required it.&lt;/p&gt;

&lt;h3&gt;The third and fourth projects (both class libraries)&lt;/h3&gt;

&lt;p&gt;The process for the &quot;Helpers&quot; and &quot;Querier&quot; class libraries was simple. Neither required anything that wasn&#39;t included in netstandard1.6 and so it was just a case of going through the motions.&lt;/p&gt;

&lt;h3&gt;The &quot;Tester&quot; Console Application&lt;/h3&gt;

&lt;p&gt;At this point, all of the projects that constituted the actual &quot;Full Text Indexer&quot; were building for both the netstandard1.6 framework and .NET 4.5.2 - so I could have stopped here, really (aside from the serialisation issues I had been putting off). But I thought I might as well go all the way and see if there were any interesting differences in migrating Console Applications and xUnit test suite projects.&lt;/p&gt;

&lt;p&gt;For the Tester project; no, not much was different. It has an end-to-end example integration where it loads data from a Sqlite database file of Blog posts using Dapper and then creates a search index. The posts contain markdown content and so three NuGet packages were required - &lt;a href=&quot;https://www.nuget.org/packages/Dapper&quot;&gt;Dapper&lt;/a&gt;, &lt;a href=&quot;https://www.nuget.org/packages/System.Data.Sqlite&quot;&gt;System.Data.Sqlite&lt;/a&gt; and &lt;a href=&quot;https://www.nuget.org/packages/MarkdownSharp&quot;&gt;MarkdownSharp&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Dapper supports .NET Core and so that was no problem but the other two did not. Thankfully, though, there were alternatives that &lt;em&gt;did&lt;/em&gt; support netstandard - &lt;a href=&quot;https://www.nuget.org/packages/Microsoft.Data.Sqlite&quot;&gt;Microsoft.Data.Sqlite&lt;/a&gt; and &lt;a href=&quot;https://www.nuget.org/packages/Markdown&quot;&gt;Markdown&lt;/a&gt;. Using Microsoft.Data.Sqlite required some (very minor) code changes while Markdown exposed exactly the same interface as MarkdownSharp.&lt;/p&gt;

&lt;h3&gt;The xUnit Test Suite Project&lt;/h3&gt;

&lt;p&gt;The &quot;UnitTests&quot; project didn&#39;t require anything &lt;em&gt;very&lt;/em&gt; different but there are a few gotchas to watch out for..&lt;/p&gt;

&lt;p&gt;The first is that you need to create a &quot;Console Application (.NET Core)&quot; project since xUnit works with the &quot;netcoreapp1.0&quot; framework (which console applications target) and not &quot;netstandard1.6&quot; (which is what class libraries target).&lt;/p&gt;

&lt;p&gt;The second is that, presuming you want the Visual Studio test runner integration (which, surely, you do!) you need to not only add the &quot;xunit&quot; NuGet package but also the &quot;dotnet-test-xunit&quot; package. Thirdly, you need to enable the &quot;Include prerelease&quot; option in the NuGet Package Manager to locate versions of these packages that work with .NET Core (this will, of course, change with time - but as of November 2016 these packages are only available as &quot;prereleases&quot;).&lt;/p&gt;

&lt;p&gt;Fourthly, you need to add a line&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&quot;testRunner&quot;: &quot;xunit&quot;,
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;to the project.json file.&lt;/p&gt;

&lt;p&gt;Having done all of this, the project should compile &lt;em&gt;and&lt;/em&gt; the tests should appear in the Test Explorer window.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note: I wanted to fully understand each step required to create an xUnit test project but you could also just follow the instructions at &lt;a href=&quot;https://xunit.github.io/docs/getting-started-dotnet-core.html&quot;&gt;Getting started with xUnit.net (.NET Core / ASP.NET Core)&lt;/a&gt; which provides you a complete project.json to paste in - one of the nice things about .NET Core projects is that changing (and saving) the project.json is all it takes to change from being a class library (and targeting netstandard) to being a console application (and targeting netcoreapp). Similarly, references to other projects and to NuGet packges are all specified there and saving changes to that project file results in those reference immediately being resolved and any specified packages being downloaded.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In the class library projects, I made them all target both netstandard and net452. With the test suite project, if the project.json file is changed to target both .NET Core (&quot;netcoreapp1.0&quot;, since it&#39;s a console app) and full fat .NET (&quot;net452&quot;) then two different versions of the suite will be built. The clever thing about this is that if you use the command line to run the tests -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dotnet test
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. then it will run the tests in both versions. Since there are going to be some differences between the different frameworks and, quite feasibly, between different versions of dependencies then it&#39;s a very handy tool to be able to run the tests against all of the versions of .NET that your libraries target.&lt;/p&gt;

&lt;p&gt;There is a &quot;but&quot; here, though. While the command line test process will target both frameworks, the Visual Studio Test Explorer doesn&#39;t. I &lt;em&gt;think&lt;/em&gt; that it only targets the first framework that is specified in the project.json file but I&#39;m not completely sure. I just know that it doesn&#39;t run them both. Which is a pity. On the bright side, I do like that .NET Core is putting the command line first - not only because I&#39;m a command line junkie but also because it makes it very easy to integrate into build servers and continuous integration  processes. I do hope that one day (soon) that the VS integration will be as thorough as the command line tester, though.&lt;/p&gt;

&lt;h3&gt;Building NuGet packages&lt;/h3&gt;

&lt;p&gt;So, now, there are no errors and everything is building for .NET Core &lt;em&gt;and&lt;/em&gt; for &quot;classic&quot;* .NET.&lt;/p&gt;

&lt;p&gt;* &lt;em&gt;I&#39;m still not sure what the accepted terminology is for non-.NET-Core projects, I don&#39;t really think that &quot;full fat framework&quot; is the official designation :)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;There are no nasty workarounds required for the references (like when the not-yet-migrated .NET 4.5.2 projects were referencing the .NET Core projects). It&#39;s worth mentioning that that workaround was only required when the .NET 4.5.2 project wanted to reference a .NET Core project &lt;em&gt;from within the same solution&lt;/em&gt; - if the project that targeted both &quot;netstandard1.6&quot; and &quot;net452&quot; was built into a NuGet package then that package could be added to a .NET Core project &lt;em&gt;or&lt;/em&gt; to a .NET 4.5.2 project &lt;em&gt;without any workarounds&lt;/em&gt;. Which makes me think that now is a good time to talk about building NuGet packages from .NET Core projects..&lt;/p&gt;

&lt;p&gt;The project.json file has enough information that the &quot;dotnet&quot; command line can create a NuGet package from it. So, if you run the following command (you need to be in the root of the project that you&#39;re interested in to do this) -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dotnet pack
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. then you will get a NuGet package built, ready to distribute! This is very handy, it makes things very simple. And if the project.json targets both netstandard1.6 and net452 then you will get a NuGet package that may be added to either a .NET Core project or a .NET 4.5.2 (or later) project.&lt;/p&gt;

&lt;p&gt;I hadn&#39;t created the Full Text Indexer as a NuGet package before now, so this seemed like a good time to think about how exactly I wanted to do it.&lt;/p&gt;

&lt;p&gt;There were a few things that I wanted to change with what &quot;dotnet pack&quot; gave me at this point -&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The name and ID of the package comes from the project name, so the Core project resulted into a package named &quot;Core&quot;, which is too vague&lt;/li&gt;
&lt;li&gt;I wanted to include additional metadata in the packages such as a description, project link and icon url&lt;/li&gt;
&lt;li&gt;If each project would be built into a separate package then it might not be clear to someone what packages are required and how they work together, so it probably makes sense to have a combined package that pulls in everything&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For points one and two, the &quot;&lt;a href=&quot;https://docs.microsoft.com/en-us/dotnet/articles/core/tools/project-json&quot;&gt;project.json reference&lt;/a&gt;&quot; documentation has a lot of useful information. It describes the &quot;name&quot; attribute -&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The name of the project, used for the assembly name as well as the name of the package. The top level folder name is used if this property is not specified.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So, it sounds like I could add a line to the Common project -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&quot;name&quot;: &quot;FullTextIndexer.Common&quot;,
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. which would result in the NuGet package for &quot;Common&quot; having the ID &quot;FullTextIndexer.Common&quot;. And it does!&lt;/p&gt;

&lt;p&gt;However, there is a problem with doing this.&lt;/p&gt;

&lt;p&gt;The &quot;Common&quot; project is going to be built into a NuGet package called &quot;FullTextIndexer.Common&quot; so the projects that depend upon it will need updating - their project.json files need to change the dependency from &quot;Common&quot; to &quot;FullTextIndexer.Common&quot;. If the Core project, for example, wasn&#39;t updated to state &quot;FullTextIndexer.Common&quot; as a dependency then the &quot;Core&quot; NuGet package would have a dependency on a package called &quot;Common&quot;, which wouldn&#39;t exist (because I want to publish it as &quot;FullTextIndexer.Common&quot;). The issue is that if Core&#39;s project.json is updated to specify &quot;FullTextIndexer.Common&quot; as a dependency then the following errors are reported:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;NuGet Package Restore failed for one or more packages. See details in the Output window.&lt;/p&gt;
  
  &lt;p&gt;The dependency FullTextIndexer.Common &gt;= 1.0.0-* could not be resolved.&lt;/p&gt;
  
  &lt;p&gt;The given key was not present in the dictionary.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;To cut a long story short, after some trial and error experimenting (and having been unable to find any documentation about this or reports of anyone having the same problem) it seems that the problem is that .NET Core dependencies within a solution depend upon the project folders having the same name as the package name - so my problem was that I had a project folder called &quot;Common&quot; that was building a NuGet package called &quot;FullTextIndexer.Common&quot;. Renaming the &quot;Common&quot; folder to &quot;FullTextIndexer.Common&quot; fixed it. It probably makes sense to keep the project name, package name and folder name consistent in general, I just wish that the error messages had been more helpful because the process of discovering this was very frustrating!&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note: Since I renamed the project folder to &quot;FullTextIndexer.Common&quot;, I didn&#39;t need the &quot;name&quot; option in the project.json file and so I removed it (the default behaviour of using the top level folder name is fine).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://docs.microsoft.com/en-us/dotnet/articles/core/tools/project-json&quot;&gt;project.json reference&lt;/a&gt; made the second task simple, though, by documenting the &quot;&lt;a href=&quot;https://docs.microsoft.com/en-us/dotnet/articles/core/tools/project-json#packoptions&quot;&gt;packOptions&lt;/a&gt;&quot; section. To cut to the chase, I changed the Common&#39;s project.json to the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &quot;version&quot;: &quot;1.0.0-*&quot;,

  &quot;packOptions&quot;: {
    &quot;iconUrl&quot;: &quot;https://secure.gravatar.com/avatar/6a1f781d4d5e2d50dcff04f8f049767a?s=200&quot;,
    &quot;projectUrl&quot;: &quot;https://bitbucket.org/DanRoberts/full-text-indexer&quot;,
    &quot;tags&quot;: [ &quot;C#&quot;, &quot;full text index&quot;, &quot;search&quot; ]
  },
  &quot;authors&quot;: [ &quot;ProductiveRage&quot; ],
  &quot;copyright&quot;: &quot;Copyright 2016 ProductiveRage&quot;,

  &quot;dependencies&quot;: {},

  &quot;frameworks&quot;: {
    &quot;netstandard1.6&quot;: {
      &quot;imports&quot;: &quot;dnxcore50&quot;,
      &quot;dependencies&quot;: {
        &quot;NETStandard.Library&quot;: &quot;1.6.0&quot;
      }
    },
    &quot;net452&quot;: {}
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I updated the other class library projects similarly and updated the dependency names on all of the projects in the solution so that everything was consistent and compiling.&lt;/p&gt;

&lt;p&gt;Finally, I created an additional project named simply &quot;FullTextIndexer&quot; whose only role in life is to generate a NuGet package that includes all of the others (it doesn&#39;t have any code of its own). Its project.json file looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &quot;version&quot;: &quot;1.0.0-*&quot;,

  &quot;packOptions&quot;: {
    &quot;summary&quot;: &quot;A project to try implementing a full text index service from scratch in C# and .NET Core&quot;,
    &quot;iconUrl&quot;: &quot;https://secure.gravatar.com/avatar/6a1f781d4d5e2d50dcff04f8f049767a?s=200&quot;,
    &quot;projectUrl&quot;: &quot;https://bitbucket.org/DanRoberts/full-text-indexer&quot;,
    &quot;tags&quot;: [ &quot;C#&quot;, &quot;full text index&quot;, &quot;search&quot; ]
  },
  &quot;authors&quot;: [ &quot;ProductiveRage&quot; ],
  &quot;copyright&quot;: &quot;Copyright 2016 ProductiveRage&quot;,

  &quot;dependencies&quot;: {
    &quot;FullTextIndexer.Common&quot;: &quot;1.0.0-*&quot;,
    &quot;FullTextIndexer.Core&quot;: &quot;1.0.0-*&quot;,
    &quot;FullTextIndexer.Helpers&quot;: &quot;1.0.0-*&quot;,
    &quot;FullTextIndexer.Querier&quot;: &quot;1.0.0-*&quot;
  },

  &quot;frameworks&quot;: {
    &quot;netstandard1.6&quot;: {
      &quot;imports&quot;: &quot;dnxcore50&quot;,
      &quot;dependencies&quot;: {
        &quot;NETStandard.Library&quot;: &quot;1.6.0&quot;
      }
    },
    &quot;net452&quot;: {}
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;One final note about NuGet packages before I move on - the default behaviour of &quot;dotnet pack&quot; is to build the project in Debug configuration. If you want to build in release mode then you can use the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dotnet pack --configuration Release
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;&quot;Fixing&quot; the serialisation problem&lt;/h3&gt;

&lt;p&gt;Serialisation in .NET Core seems to a bone of contention - the Microsoft Team are sticking to their guns in terms of not supporting it and, instead, promoting other solutions:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Binary Serialization&lt;/p&gt;
  
  &lt;p&gt;Justification. After a decade of servicing we&#39;ve learned that serialization is incredibly complicated and a huge compatibility burden for the types supporting it. Thus, we made the decision that serialization should be a protocol implemented on top of the available public APIs. However, binary serialization requires intimate knowledge of the types as it allows to serialize object graphs including private state.&lt;/p&gt;
  
  &lt;p&gt;Replacement. Choose the serialization technology that fits your goals for formatting and footprint. Popular choices include data contract serialization, XML serialization, JSON.NET, and protobuf-net.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;em&gt;(from &quot;&lt;a href=&quot;https://github.com/dotnet/corefx/blob/2b15de70c1cf9f585c4878a722de4dbe42b4940b/Documentation/project-docs/porting.md#binary-serialization&quot;&gt;Porting to .NET Core&lt;/a&gt;&quot;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Meanwhile, people have voiced their disagreement in GitHub issues such as &quot;&lt;a href=&quot;https://github.com/dotnet/corefx/issues/6564&quot;&gt;Question: Serialization support going forward from .Net Core 1.0&lt;/a&gt;&quot;.&lt;/p&gt;

&lt;p&gt;The problem with recommendations such as &lt;a href=&quot;http://www.newtonsoft.com/json&quot;&gt;Json.NET&lt;/a&gt;) and &lt;a href=&quot;https://github.com/mgravell/protobuf-net&quot;&gt;protobuf-net&lt;/a&gt; is that they require changes to code that previously worked with BinaryFormatter - there is no simple switchover. Another consideration is that I wanted to see if it was possible to migrate my code over to supporting .NET Core while still making it compatible with any existing installation, such that it could still deserialise any disk-cached data that had been persisted in the past (the chances of this being a realistic use case are exceedingly slim - I doubt that anyone but me has used the Full Text Indexer - I just wanted to see if it seemed feasible).&lt;/p&gt;

&lt;p&gt;For the sake of this post, I&#39;m going to cheat a little. While I have come up with a way to serialise index data that works with netstandard, it would probably best be covered another day (and it isn&#39;t compatible with historical data, unfortunately). A good-enough-for-now approach was to use &quot;&lt;a href=&quot;https://msdn.microsoft.com/en-us/library/4y6tbswk.aspx&quot;&gt;conditional directives&lt;/a&gt;&quot;, which are basically a way to say &quot;if you&#39;re building in this configuration then include this code (and if you&#39;re not, then don&#39;t)&quot;. This allowed me the restore all of the &lt;strong&gt;[Serializable]&lt;/strong&gt; attributes that I removed earlier - but only if building for .NET 4.5.2 (and not for .NET Core). For example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#if NET452
    [Serializable]
#endif
    public class Whatever
    {
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;strong&gt;[Serializable]&lt;/strong&gt; attribute will be included in the binaries for .NET 4.5.2 and not for .NET Core.&lt;/p&gt;

&lt;p&gt;You need to be careful with precisely what conditions you specify, though. When I first tried this, I used the line &quot;if #net452&quot; (where the string &quot;net452&quot; is consistent with the framework target string in the project.json files) but the attribute wasn&#39;t being included in the .NET 4.5.2 builds. There was no error reported, it just wasn&#39;t getting included. I had to look up the supported values to see if I&#39;d made a silly mistake and it was the casing - it needs to be &quot;NET452&quot; and not &quot;net452&quot;.&lt;/p&gt;

&lt;p&gt;I used the same approach to restore the &lt;strong&gt;ISerializable&lt;/strong&gt; implementations that some classes had and I used it to conditionally compile the entirety of the &lt;strong&gt;IndexDataSerialiser&lt;/strong&gt; (which I got back out of my source control history, having deleted it earlier).&lt;/p&gt;

&lt;p&gt;This meant that if the &quot;FullTextIndexer&quot; package is added to a project building for the &quot;classic&quot; .NET framework then all of the serialisation options that were previously available still will be - any disk-cached data may be read back using the &lt;strong&gt;IndexDataSerialiser&lt;/strong&gt;. It &lt;em&gt;wouldn&#39;t&lt;/em&gt; be possible if the package is added to a .NET Core project but this compromise felt much better than nothing.&lt;/p&gt;

&lt;h3&gt;Final tweaks and parting thoughts&lt;/h3&gt;

&lt;p&gt;The migration is almost complete at this point. There&#39;s one minor thing I&#39;ve noticed while experimenting with .NET Core projects; if a new solution is created whose first project is a .NET Core class library or console application, the project files aren&#39;t put into the root of the solution - instead, they are in a &quot;src&quot; folder. Also, there is a &quot;global.json&quot; file in the solution root that enables.. &lt;em&gt;magic special things&lt;/em&gt;. If I&#39;m being honest, I haven&#39;t quite wrapped my head around all of the potential benefits of global.json (though there is an explanation of one of the benefits here; &lt;a href=&quot;https://ievangelist.github.io/blog/the-global-json/&quot;&gt;The power of the global.json&lt;/a&gt;). What I&#39;m getting around to saying is that I want my now-.NET-Core solution to look like a &quot;native&quot; .NET Core solution, so I tweaked the folder structure and the .sln file to be consistent with a solution that had been .NET Core from the start. I&#39;m a fan of consistency and I think it makes sense to have my .NET Core solution follow the same arrangement as everyone else&#39;s .NET Core solutions.&lt;/p&gt;

&lt;p&gt;Having gone through this whole process, I think that there&#39;s an important question to answer: &lt;strong&gt;Will I now switch to defaulting to supporting .NET Core for all new projects?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;.. and the answer is, today, if I&#39;m being honest.. no.&lt;/p&gt;

&lt;p&gt;There are just a few too many rough edges and question marks. The biggest one is the change that&#39;s going to happen away from &quot;project.json&quot; and to a variation of the &quot;.csproj&quot; format. I&#39;m sure that there will be some sort of simple migration tool but I&#39;d rather know &lt;em&gt;for sure&lt;/em&gt; what the implications are going to be around this change before I commit too much to .NET Core.&lt;/p&gt;

&lt;p&gt;I&#39;m also a bit annoyed that the Productivity Power Tools remove-and-sort-usings-on-save doesn&#39;t work with .NET Core projects (there&#39;s an &lt;a href=&quot;https://github.com/Microsoft/VS-PPT/issues/40&quot;&gt;issue on GitHub&lt;/a&gt; about this but it hasn&#39;t bee responded to since August 2016, so I&#39;m not sure if it will get fixed).&lt;/p&gt;

&lt;p&gt;Finally, I&#39;m sure I read an issue around analysers being included in NuGet packages for .NET Core - that they weren&#39;t getting loaded correctly. I can&#39;t find the issue now so I&#39;ve done some tests to try to confirm or deny the rumour.. I&#39;ve got a very simple project that includes an analyser and whose package targets both .NET 4.5 and netstandard1.6 and the analyser &lt;em&gt;does&lt;/em&gt; seem to install correctly and be included in the build process (see &lt;a href=&quot;https://github.com/ProductiveRage/ProductiveRage.SealedClassVerification&quot;&gt;ProductiveRage.SealedClassVerification&lt;/a&gt;) but I still have a few concerns; in .csproj files, analyser are all explicitly referenced (and may be enabled or disabled in the Solution Explorer by going into References/Analyzers) but I can&#39;t see how they&#39;re referenced in .NET Core projects (and they don&#39;t appear in the Solution Explorer). Another (minor) thing is that, while the analyser does get executed and any warnings displayed in the Error List in Visual Studio, there are no squigglies underlining the offending code. I don&#39;t know why that is and it makes me worry that the integration is perhaps a bit flakey. I&#39;m a big fan of analysers and so I want to be sure that they are fully supported*. Maybe this will get tidied up when the new project format comes about.. whenever that will be.&lt;/p&gt;

&lt;p&gt;* &lt;em&gt;(&lt;strong&gt;Update:&lt;/strong&gt; Having since added a code fix to the &quot;SealedClassVerification&quot; analyser, I&#39;ve realised that the no-squigglies-in-editor problem is worse than I first thought - it means that the lightbulb for the code fix does not appear in the editor and so the code fix can not be used. I also found the GitHub issue that I mentioned: &quot;&lt;a href=&quot;https://github.com/dotnet/roslyn-analyzers/issues/1028&quot;&gt;Analyzers fail on .NET Core projects&lt;/a&gt;&quot;, it says that improvements are on the way &quot;in .NET Core 1.1&quot; which should be released sometime this year.. maybe then will improve things)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I think that things are &lt;em&gt;close&lt;/em&gt; (and I like that Microsoft is making this all available early on and accepting feedback on it) but I don&#39;t think that it&#39;s quite ready enough for me to switch to it full time yet.&lt;/p&gt;

&lt;p&gt;Finally, should you be curious at all about the Full Text Indexer project that I&#39;ve been talking about, the source code is available here: &lt;a href=&quot;https://bitbucket.org/DanRoberts/full-text-indexer&quot;&gt;bitbucket.org/DanRoberts/full-text-indexer&lt;/a&gt; and there are a range of old posts that I wrote about how it works (see &quot;&lt;a href=&quot;http://www.productiverage.com/the-full-text-indexer-post-roundup&quot;&gt;The Full Text Indexer Post Round-up&lt;/a&gt;&quot;).&lt;/p&gt;</description>
			<pubDate>Sun, 13 Nov 2016 13:38:00 GMT</pubDate>
		</item>
		<item>
			<title>When a disk cache performs better than an in-memory cache (befriending the .NET GC)</title>
            <link>http://www.productiverage.com/when-a-disk-cache-performs-better-than-an-inmemory-cache-befriending-the-net-gc</link>
			<guid>http://www.productiverage.com/when-a-disk-cache-performs-better-than-an-inmemory-cache-befriending-the-net-gc</guid>
			<description>&lt;h3&gt;TL;DR (especially for Bud Goode)&lt;/h3&gt;

&lt;p&gt;The .NET garbage collector is a complex beast. Occasionally it might infuriate but remember that it&#39;s keeping you from the misery of manual memory management and that you&#39;re better to consider it an ally than a foe.&lt;/p&gt;

&lt;p&gt;Sometimes, ways to improve its performance seem counter-intuitive, such as intentionally keeping objects around that will have to be considered by each of the already-expensive gen 2 collections, even when we have no intention of letting those objects go (aka. object pooling) and such as using disk caching instead of in-memory caching, despite an in-memory cache &quot;obviously&quot; being more performant than having to hit the file system.&lt;/p&gt;

&lt;h3&gt;The deep dive&lt;/h3&gt;

&lt;p&gt;At work we have a service that handles queries from our hundreds of tourism websites and talks to the backend databases when, say, someone searches for Concerts in a particular location or wants to book a Hotel on a particular date. It caches many of the results of these queries for ten or fifteen minutes, which takes a lot of load away from the database servers and greatly reduces the average response times for users of the web sites. It handles a few million requests a day - so it&#39;s hardly Google but it&#39;s also doing enough work to be interesting from a performance point of view.&lt;/p&gt;

&lt;p&gt;The load has been spread over a pair of servers for a few years now, initially for redundancy. However, there was a point at which it became clear that a single server could no longer handle the load. When there were too many concurrent requests, individual requests would take longer to be processed, which resulted in the number of concurrent requests going up and up and the individual request times following suit until requests started timing out. Over time, two servers became four servers and there is concern now that two servers could not reliably handle all of the load.&lt;/p&gt;

&lt;p&gt;On top of this, the memory usage of the service on each of the servers appears to slowly-but-surely increase over time until it gets high enough and, for want of a more technical term, freaks out. The thread count in the service builds and builds as more request are backing up, waiting to be processed. The requests times get longer and longer. Using PerfMon, it looks like several CPU cores are tied up entirely on garbage collection (since we&#39;re using the &quot;server mode&quot; GC configuration, there is a separate managed heap - and a separate collection thread - for each processor). Strangely, at this point, the cores don&#39;t appear to be max&#39;ing out, the average CPU usage for the server is reliatively low, though the &quot;% time in GC&quot; is high. Every few weeks, it seems like one of the servers would need the service restarting on it due to a &quot;memory blow up&quot;.&lt;/p&gt;

&lt;p&gt;The time finally came that we could no longer continue to brush this under the rug - this problem was not going to go away and the occasional &quot;uh-oh, someone is going to have to restart the service again&quot; was no longer acceptable; not only was there a minute or two downtime for the actual service restart, there was also half an hour or so leading up to it during which response times were getting unacceptably long.&lt;/p&gt;

&lt;h3&gt;Blaming the GC&lt;/h3&gt;

&lt;p&gt;It would seem all too easy to blame things on the garbage collector, say that the fault lies there and that there&#39;s nothing we can do about it other than avoiding giving it more load than it can deal with (in other words, throw more servers at the problem). But I recently read &lt;a href=&quot;https://twitter.com/ben_a_adams/status/767174657048993792&quot;&gt;a tweet&lt;/a&gt; that said&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Blaming perf issues on Garbage Collection is like blaming your hangover on your liver... Its the thing that&#39;s saving you from your code&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;(Courtesy of &lt;a href=&quot;https://twitter.com/ben_a_adams/status/767174657048993792&quot;&gt;Ben Adams&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;.. which helped motivate me into trying to find a better solution. On the whole, I&#39;m very grateful that the .NET garbage collector works as well as it does. In the majority of cases, you just don&#39;t have to worry about it. But sometimes you do. Clearly, the data service that I&#39;m talking about is one of those cases.&lt;/p&gt;

&lt;p&gt;The garbage collector uses a range of factors to decide when to collect - some are simple, such as the amount of free memory available in the system (if there&#39;s a lot then there&#39;s less pressure to collect) and the available processor time (if the system is busy doing other work then it would ideal to wait until it&#39;s quieter before throwing GC work on top of the &quot;real work&quot;). Some factors are more complex - for example, if gen 2 collections occur that release zero (or very little) memory then the GC will take this into account and try to collect it less often (since gen 2 collections are the most expensive, it makes sense for the GC to avoid them if possible; if it finds that few references are being released from gen 2 each collections then there&#39;s little point doing the collection work).&lt;/p&gt;

&lt;p&gt;However, there is a limit to how much the GC can deal with things with &lt;a href=&quot;https://en.wikipedia.org/wiki/Clarke%27s_three_laws#Clarke.27s_third_law&quot;&gt;by magic&lt;/a&gt;. Sometimes you need to work &lt;em&gt;with&lt;/em&gt; the garbage collector, rather than just presuming that it will be able to deal with anything you throw at it.&lt;/p&gt;

&lt;p&gt;One way to help is to simply make fewer allocations. The less allocations that are made, the less work that there is for the garbage collector to do.&lt;/p&gt;

&lt;p&gt;Another approach is take on board one of Ben Watson&#39;s key principles for enabling &quot;&lt;a href=&quot;http://www.philosophicalgeek.com/2012/06/04/4-essential-tips-for-high-performance-garbage-collection-on-servers/&quot;&gt;high-performance garbage collection on servers&lt;/a&gt;&quot;, which is:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Objects Live Briefly or Forever&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In order to think about how I can make the GC&#39;s life easier, it&#39;s worth taking a step back and describing a little bit more about what this troublesome service has to do.&lt;/p&gt;

&lt;h3&gt;What the service deals with (and why this might not be GC-friendly)&lt;/h3&gt;

&lt;p&gt;At its core, this service receives requests from websites, gets the required data for that request from an external source or from cache (if from an external source, such as a database, then the results will be cached for subsequent requests), massages the data into the desired form and sends it back to the client. For the majority of the time, the results include only &quot;stubs&quot; of the real data - a unique identifier, the name and its location (latitude, longitude). When the results of a query are cached, such as &quot;get me all of the hotels in Liverpool that are in the city centre and have at least a three star rating, ordered by rating (descending)&quot;, the query and the resulting stubs are cached. The response to the client will include all of those stubs but it will also include full product details for one page of data - if the client says that it wants to show the first page of results to the user and each page shows ten items, then the first ten entries in the result set will be full products and the remaining &quot;n - 10&quot; entries will be stubs. There is a separate service that is responsible for retrieving full product details for given stubs.&lt;/p&gt;

&lt;p&gt;The quickest way that the service may deliver these results is if the query results are stored in cache. In which case, the requests will be dealt with using the following steps:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The request is deserialised&lt;/li&gt;
&lt;li&gt;Ordered stubs corresponding to the query are retrieved from the &quot;Query Cache&quot;&lt;/li&gt;
&lt;li&gt;Full product details are retrieved for the first page (as specified by the request) of results  - this involves serialising a request for ten unique identifiers, sending it to the external product service and then receiving the details for those products back (which means that there&#39;s a step that involves deserialisation of full product records when the data comes over from the external service)&lt;/li&gt;
&lt;li&gt;The response (which consists of 10 full products and &quot;n - 10&quot; stubs) is serialised to be sent over the wire back to the client&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The core of the service was written in (and has been in use since) 2009, which has a spread of advantages and disadvantages. On the less-good side, it uses .NET remoting (at the time, our servers only had .NET 2.0 installed and so newer technologies such as WCF were out of reach) and much of the serialisation uses the &lt;strong&gt;BinaryFormatter&lt;/strong&gt; (which is unlikely to be anyone&#39;s go-to these days if they are interested in performance). On the other hand, over the years it&#39;s been proven to be largely reliable and it&#39;s been through a few optimisiation drives since the business is so reliant on it. So the places where serialisation performance is most important have had the &lt;strong&gt;BinaryFormatter&lt;/strong&gt; replaced; anywhere that the stubs are serialised/deserialised, for example, uses custom methods to read/write the fixed fields in the stub type. Similarly, the &quot;full product&quot; records are serialised using custom routines (which is a double win when responding to a request since the full product instances must be deserialised when they are received from the external product service and then &lt;em&gt;re-&lt;/em&gt;serialised to be included in the final response to the client, so that&#39;s twice that use of the slow &lt;strong&gt;BinaryFormatter&lt;/strong&gt; is avoided).&lt;/p&gt;

&lt;p&gt;What I&#39;m trying to say here is that any &quot;low hanging fruit&quot; in terms of performance hotspots within the service code had been improved in the past. It genuinely did seem like it was the garbage collector that was responsible for much of the performance problem. (I did use the &lt;a href=&quot;http://www.red-gate.com/products/dotnet-development/ants-performance-profiler/&quot;&gt;ANTS Performance Profiler&lt;/a&gt; on a local installation of the service under load to confirm this but it didn&#39;t reveal anything exciting). So it was firmly in the direction of the garbage collector that I faced.&lt;/p&gt;

&lt;p&gt;I&#39;ve written much of this service&#39;s code, so I&#39;m familiar with its general structure as well as many of the finer details. With this knowledge, I captured a batch* of sample requests and set up a test environment that I could replay these requests against (using &lt;a href=&quot;https://github.com/ProductiveRage/SqlProxyAndReplay&quot;&gt;SqlProxyAndReplay&lt;/a&gt; to remove the database from the equation).&lt;/p&gt;

&lt;p&gt;* &lt;em&gt;(The test queries were taken from real web site logs and replayed at the same rate and level of concurrency - so they should be a reasonable approximation of real life load)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The plan being to try to tweak the code that was likely to offend the GC the most and measure after each change to see how it affected the work that the collector had to do. The first candidates were:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The &quot;Query Cache&quot; needs to retrieve from, add to and remove from a structure that will be accessed concurrently be multiple threads. The very first implementation was a dictionary that required a lock for every read or write access. This was changed so that a lock was only required for write actions, which would clone the dictionary and overwrite the internal reference. Read actions wouldn&#39;t require a lock since no dictionary would ever change. However, this clone-for-every-write could mean a lot of churn.&lt;/li&gt;
&lt;li&gt;The custom serialisation uses binary data reader and writer classes. Each individual property value is serialised into a byte array (the &lt;strong&gt;BitConverter&lt;/strong&gt; is used for many types of values and the &lt;strong&gt;UTF8Encoder&lt;/strong&gt; is used for strings) and then these bytes are added to a &lt;strong&gt;List&amp;lt;byte&amp;gt;&lt;/strong&gt; (and the reverse is done to deserialise; sections of the list are extracted into arrays and then translated back into property values). This means that there are a lot of arrays being allocated when serialising or deserialising.&lt;/li&gt;
&lt;li&gt;When serialising/deserialising the full product records, it seems very possible that these records could be over 85,000 bytes of serialised data, which would mean that there would be lots of byte arrays created on the Large Object Heap (where &quot;lots&quot; depends upon how many requests a second are being handled, how many full product records need to be retrieved for the requests and how many of those records were more than 85,000 bytes when serialised). Allocations to the Large Object Heap can be a source of headaches, which I&#39;ll go into in a little more detail later on.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;Step 1: Bin the custom &quot;free-reading dictionary&quot;&lt;/h3&gt;

&lt;p&gt;There&#39;s a &lt;a href=&quot;https://msdn.microsoft.com/en-us/library/dd287191(v=vs.110).aspx&quot;&gt;ConcurrentDictionary&lt;/a&gt; in .NET these days, which should improve things when compared to the custom structure we were using. Using it means that read and write actions both lock again but the locks are much more granular (they only affect subsets of the data, rather than there being a single huge lock around the entire reference) and so there is less likely to be contention between operations.&lt;/p&gt;

&lt;p&gt;The batch of test queries were run against this change and the garbage collection frequency performance counters were captured. A few runs were performed and the results averaged and.. er.. annoyingly, I&#39;ve lost my notes relating to this change! There were less collections required for each generation, which was promising. Thankfully I &lt;em&gt;do&lt;/em&gt; have some useful information for the next changes :)&lt;/p&gt;

&lt;h3&gt;Step 2: Bin the custom binary reader and writer classes&lt;/h3&gt;

&lt;p&gt;The .NET library has &lt;a href=&quot;https://msdn.microsoft.com/en-us/library/system.io.binaryreader(v=vs.110).aspx&quot;&gt;BinaryReader&lt;/a&gt; and &lt;a href=&quot;https://msdn.microsoft.com/en-us/library/system.io.binarywriter(v=vs.110).aspx&quot;&gt;BinaryWriter&lt;/a&gt; classes that take a stream and read/write to it in a more efficient manner than the custom reader/writer classes used before (which allocated at least one array for every single property read or write). These aren&#39;t new classes, I just wasn&#39;t aware of them when I wrote the custom versions all that time ago.&lt;/p&gt;

&lt;p&gt;The tests were repeated with this change and, compared to only the Query Cache change, there were on average &lt;strong&gt;56% as many gen 0 collections, 60% as many gen 1 collections and 59% as many gen 2 collections&lt;/strong&gt;.&lt;/p&gt;

&lt;h3&gt;Step 3: Pooling large byte arrays used in serialisation/deserialisation&lt;/h3&gt;

&lt;p&gt;Time to talk about the Large Object Heap. The garbage collector is much happier dealing with &quot;small objects&quot; (which are decided to be those less than 85000 bytes, based upon &quot;a bunch of benchmarks&quot; according to this &lt;a href=&quot;http://stackoverflow.com/a/8953503/3813189&quot;&gt;excellent Stack Overflow answer by Hans Passant&lt;/a&gt;). With small objects, it will allocate them freely and then, after collections, compact the heap for objects that survive the collection. If the heaps are not compacted, then any gaps in between &quot;live&quot; objects (live objects are those that the GC finds to still be in use) could only be used to slot in newly allocated objects if they fit in the gaps. As objects are allocated and then tidied up, it can become more and more difficult to find somewhere to fit new allocations - it might be necessary to look at &lt;em&gt;many&lt;/em&gt; small gaps before finding one that a new object will fit in (this problem is referred to as being caused by fragmentation of the heap). Compacting the heap moves all of the objects so that they&#39;re pushed up together, with no gaps, and is relatively cheap when dealing with small objects since each individual memory operation is cheap. However, copying big chunks of memory around (such as the live objects in the Large Object Heap), which is what would be required to compact the Large Object Heap, is much harder work. Following the same sort of logic (that large objects are more expensive to deal with), the Large Object Heap is only collected during a gen 2 collection.&lt;/p&gt;

&lt;p&gt;If a lot of allocations are made to the Large Object Heap then memory can appear to spiral out of control (because the Large Object Heap is only collected in gen 2 and because it&#39;s not compacted) and the pressure on the GC will increase. Unfortunately, this can be done quite easily when frequently serialising/deserialising to arrays that break the 85,000 byte limit.&lt;/p&gt;

&lt;p&gt;One solution is to &quot;pool&quot; those byte arrays. In other words, to maintain a set of arrays and to reuse them, rather than creating new ones each time (which the GC will have to work hard to tidy up after). It&#39;s not difficult to imagine that this could easily become a very complicated task - whatever is responsible for pooling those arrays would need be thread safe and it would have to apply some sensible logic to when and how to reuse arrays; Should &lt;em&gt;all&lt;/em&gt; arrays be reused? Should only large arrays be reused? Should &lt;em&gt;all&lt;/em&gt; large arrays be reused? Will there be any limits to the pool? What if the limits are exceeded and more arrays are required?&lt;/p&gt;

&lt;p&gt;Interestingly, I read last year about something that might be ideal for the job in the book &lt;a href=&quot;http://www.writinghighperf.net/&quot;&gt;Writing High-Performance .NET Code&lt;/a&gt; (written by Ben Watson, who I quoted earlier - it&#39;s a book I highly recommend, btw). I&#39;m going to lift the overview completely from the blog post &lt;a href=&quot;http://www.philosophicalgeek.com/2015/02/06/announcing-microsoft-io-recycablememorystream/&quot;&gt;Announcing Microsoft.IO.RecycableMemoryStream&lt;/a&gt; (which is a quote lifted from the book) -&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;In one application that suffered from too many LOH allocations, we discovered that if we pooled a single type of object, we could eliminate 99% of all problems with the LOH. This was MemoryStream, which we used for serialization and transmitting bits over the network. The actual implementation is more complex than just keeping a queue of MemoryStream objects because of the need to avoid fragmentation, but conceptually, that is exactly what it is. Every time a MemoryStream object was disposed, it was put back in the pool for reuse.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;That sounds like a very similar use case to what I have. Lots of serialisation/deserialisation for transmitting and receiving data from other servers, with byte arrays large enough to be allocated on the Large Object Heap. All of them being wrapped in &lt;strong&gt;MemoryStream&lt;/strong&gt;s (at least, &lt;strong&gt;MemoryStream&lt;/strong&gt;s were used for serialisation of these large objects after Step 2, above, was implemented).&lt;/p&gt;

&lt;p&gt;So this definitely seemed worth looking into.&lt;/p&gt;

&lt;p&gt;Just to recap precisely why pooling large objects might help; pooling them means keeping hold of them in memory, which seems like the opposite of what we want to do if we want to relieve memory pressure. However, the big benefit is that the Large Object Heap fragmentation will be less of a problem because we&#39;re no longer allocating large objects and then throwing them away and then trying to allocate &lt;em&gt;further&lt;/em&gt; large objects somewhere (such as in a gap that the GC has removed dead objects from or possibly resorting to tacking them on the end of the heap); instead, a &lt;strong&gt;MemoryStream&lt;/strong&gt; (and its large backing array) may be reused after it&#39;s been created once and returned to the pool, so the work to try to find a place to allocate a new large object is not required. This still feels somewhat counterintuitive because it means that there will be more objects that the garbage collector has to consider when it does a gen 2 collection and we&#39;re trying to give the GC as little work as possible - &lt;em&gt;particularly&lt;/em&gt; in gen 2, since collections there are most expensive. This is where the GC&#39;s self-tuning comes in, though. If we&#39;re trying to get towards a position where not many objects make it into gen 2 unless then are going to live forever (as pooled objects do) then the GC will be in a position where it has to evaluate the gen 2 heap but - ideally - find very little to remove. If it consistently finds very little to do then it will reduce the frequency of the gen 2 collections. So, even though it might feel like we&#39;re making the collectors life more difficult by keeping these objects alive on the gen 2 heap, we&#39;re actually making it easier.&lt;/p&gt;

&lt;p&gt;With this change, after running the tests again, there were 61% as many gen 0 collections as after only Step 2, 53% as many gen 1 collections and 45% as many gen 2 collections. This means that Step 2 and Step 3 combined resulted in &lt;strong&gt;34% as many gen 0 collections than after only the changes in Step 1, 32% as many gen 1 collections and 27% as many gen 2&lt;/strong&gt;. This seemed very promising.&lt;/p&gt;

&lt;h3&gt;Testing under increased load&lt;/h3&gt;

&lt;p&gt;The sample data that I&#39;d been using so far wasn&#39;t particularly large, it was around 10k requests that would complete in around ten minutes. This is comparable to the request/sec that the production servers deal with during the day. While each run took place, after the changes made above, the CPU usage averaged around 40% and  the &quot;% time in GC&quot; averaged 2.5%. I had a feeling, though, that it would be while the server was having a really hard time that the original issues would occur. 40% average CPU usage is nowhere near running flat out and that remaining 60% provides a lot of head room for the garbage collector to come and do what it wants whenever it wants.&lt;/p&gt;

&lt;p&gt;So I increased the load and duration. Not massively, but enough that the previous code started to get a little hot under the collar - 100k requests over an hour or so.&lt;/p&gt;

&lt;p&gt;This sample load was run against both the new and the old versions of the service  (where the old version was the code as it was before Steps 1, 2 and 3 from above were applied to it) and the performance metrics compared between the two. On average, the new version required only &lt;strong&gt;84% as much CPU to be used, spent only 30% as much time in the GC, performed 62% as many gen 0 collections, 36% as many gen 1 collections and 22% as many gen 2 collection&lt;/strong&gt;. Things were still looking promising.&lt;/p&gt;

&lt;h3&gt;Testing for the breaking point&lt;/h3&gt;

&lt;p&gt;At this point, it was feeling like a success.&lt;/p&gt;

&lt;p&gt;To stretch things further, though, I thought that I&#39;d see how it responded if I played the requests as fast as I could. In normal operation throughout the day, each server doesn&#39;t have to deal with much more than an average of 12 requests per second. There will be the odd peak of double that, but they tend to be short-lived. There will be busier times of day where the average rate may be more like 16 requests per second, but not usually for more than a few hours. I was only using a single computer to generate load in this case but that was sufficient to create a sustained load of 35-40 requests per second. I figured that if the service would deal with this then we&#39;d be doing great.&lt;/p&gt;

&lt;p&gt;And for about forty minutes, things &lt;em&gt;do&lt;/em&gt; go great. The server is busy, it&#39;s serving a lot (relative to a normal load) of requests, the gen 0 heap peaks and troughs the most, the gen 1 heap blips up and down with less drama, the gen 2 heap makes slower steps up then drops back down then very gently climbs then steps up then is steady then steps up slightly then drops down, carrying on merrily enough.&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;Gen 2 heap &#39;blow up&#39;&quot; src=&quot;http://www.productiverage.com/Content/Images/Posts/Memory Blow Up - Gen 2 Heap.jpg&quot; class=&quot;HalfWidth&quot;&gt;&lt;/p&gt;

&lt;p&gt;Until, at some point, the gen 2 heap starts curving up dramatically, followed by many steep steps upward, then a slightly pathetic dip immediately followed by steep steps upward. Having barely hit a gigabyte in size while gently building up and dropping earlier, it&#39;s now got to around 4 gig in a very short space of time. Here, it flatlines. During this steep climb, requests have gotten slower and slower and, at this flatline, they are no longer processed. This state continues for a couple of minutes, after which some work appears to attempt to continue, though the gen 2 heap doesn&#39;t drop in size at all. Some unusual errors are seen in the logs, such as:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Timeout expired.  The timeout period elapsed prior to obtaining a connection from the pool.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;It&#39;s as if, during this time, &lt;em&gt;everything&lt;/em&gt; within the service stopped. This isn&#39;t a query timeout that occurred because the database server was taking too long, this error suggests that a SqlConnection was requested (with .NET pools internally) and then the world stopped for some time.. after which, the request-for-a-connection gave up since it had been so long since it asked for it.&lt;/p&gt;

&lt;p&gt;I had thought that the point of the GC server mode was to avoid this sort of thing; even if a collection for one heap was taking a long time, each core has its own separate heap (and this server has four cores - it&#39;s not a real server, they&#39;re all virtualised, but that shouldn&#39;t make a huge difference). Could all of the heaps really have got jammed up simultaneously? Hopefully from everything I&#39;ve written above, it&#39;s clear that there are a lot of subtleties to the intricate nature of the garbage collector and so it wouldn&#39;t surprise me if I&#39;d not quite got the whole picture with server mode (or if I was maybe expecting a little too much!).&lt;/p&gt;

&lt;p&gt;Incidentally, after this &quot;flatline period&quot;, as requests appear to (slowly) begin being served again, the gen 2 heap grows to over 5 gig and then another flatline period is entered. This one much longer. So long, in fact, that I gave up waiting for it. Maybe my attention span is a bit short but I think that after more than five minutes of being completely stuck it&#39;s probably not much use even if the service &lt;em&gt;does&lt;/em&gt; start going again.&lt;/p&gt;

&lt;p&gt;The symptoms of this condition sound identical to the occasional &quot;memory blow up&quot; that was seen with the old version of the code on the live servers. It would seem that the changes so far had not provided a magic bullet.&lt;/p&gt;

&lt;h3&gt;Looking for clues&lt;/h3&gt;

&lt;p&gt;&lt;img alt=&quot;GC CPU time during the &#39;blow up&#39;&quot; src=&quot;http://www.productiverage.com/Content/Images/Posts/Memory Blow Up - GC CPU.jpg&quot; class=&quot;HalfWidth&quot;&gt;&lt;/p&gt;

&lt;p&gt;I wanted some insight into what was going on during these periods of apparent inactivity - well, it seemed like &lt;em&gt;my&lt;/em&gt; code was inactive, though it appeared that the GC was being very busy. The &quot;% time in GC&quot; would spike around during work but seem to get more frenzied in its peaking in the lead up to the gen 2 heap size flat line, then it too would flat line in sympathy. After the first flat line period, it would remain higher but spike up and down, then it would flatline again when the gen 2 heap size flatlined, at a higher level than the time before.&lt;/p&gt;

&lt;p&gt;I initially presumed that there must be something in the requests that caused this behaviour. So, if I skipped the first {whatever} thousand requests then I should be able to get this to happen sooner. Not so - skipping 10k requests still meant that I had to wait the same period of time for the behaviour to present itself. Skipping 20k, the same. If I skipped too many then the requests would complete without blowing up at all.&lt;/p&gt;

&lt;p&gt;My next step was to try to use &lt;a href=&quot;http://www.red-gate.com/products/dotnet-development/ants-memory-profiler/&quot;&gt;ANTS Memory Profiler&lt;/a&gt; and to take a couple of snapshots as the blowout started occurring. Unfortunately, by the time that the gen 2 heap size started climbing sharply, it would quickly get too big for the profiler to snapshot. There&#39;s a hard limit in the software as to how big of a memory dump it will try to process (&quot;for performance reasons&quot;). There&#39;s an option to take less information about each object so that larger dumps may be taken but even enabling that didn&#39;t work. In retrospect, it might have been worth reducing the memory in the virtual box and trying to reproduce the issue then - hopefully ANTS would have been able to deal with it then (everything got stuck when the gen 2 heap reached around four gig out of a total six gig of RAM, if the server only had four gig total then the available memory would be exhausted and the GC would presumably throw a tantrum much earlier, with a much smaller gen 2 heap).&lt;/p&gt;

&lt;p&gt;After that I tried using &lt;a href=&quot;https://blogs.msdn.microsoft.com/dotnet/2012/10/09/improving-your-apps-performance-with-perfview/&quot;&gt;PerfView&lt;/a&gt; since it&#39;s discussed and recommended in the &quot;Writing High-Performance .NET Code&quot; book. I managed to take a snapshot using that, freezing the process while doing so in order to prevent the heaps growing even more (taking the snapshot took almost two hours). When I loaded the dump file into PerfView to analyse, it appeared to show very little information about what types were in use (certainly it didn&#39;t appear to show the long list of types seen in all of the screenshots and tutorial videos about PerfView). There is a small row of information at the top of the heap alloc stack window that shows a summary. This showed 99% unreachable memory. This means that most of the memory is actually ready to be reclaimed by the collector (ie. that its roots are unreachable) and so I presumed that I wouldn&#39;t be able to find out much information about it. I tried finding confirmation for this online but didn&#39;t come up with much when searching for &quot;99% unreachable memory PerfView&quot;. Another regret, looking back, is that I didn&#39;t try a bit harder to unearth information through PerfView. To be completely honest, though, I was losing patience.&lt;/p&gt;

&lt;h3&gt;Giving up and guessing (I prefer to call it intuition)&lt;/h3&gt;

&lt;p&gt;I was frustrated now. I was frustrated with what I was seeing, I was frustrated because I didn&#39;t understand precisely what triggered it and I was frustrated that I couldn&#39;t get any tools to tell me what was going awry. So I thought I&#39;d just take a stab in the dark and see what happened.&lt;/p&gt;

&lt;p&gt;In my defence, it was more sort of an educated guess. It seemed like what the service was asking of the garbage collector was something that the collector would (given enough time) decide it didn&#39;t like. I didn&#39;t feel like it was just allocation churn, my gut* told me that references that were very short lived were not the problem, even if there were a lot of them coming into existence and then disappearing again while the request rate was high. It felt like it was all going to lie with those ten / fifteen minute caches. If the GC likes references to live for very short periods of time or to live &lt;em&gt;forever&lt;/em&gt; then this is the worst case for it. It&#39;s particularly bad since there may be many (ie. 1000s of) sets of cached results in memory at any time and each result set could potentially hold many stubs (again, 1000s).&lt;/p&gt;

&lt;p&gt;* &lt;em&gt;(I say &quot;my gut told me&quot; but I think that what that really means is that my sub-conscious, having been stuffed full with a million articles about garbage collection, was just regurgitating information I&#39;d read..)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The logical step, then, would be to move that cache out of process. Maybe Redis, Memcached.. something like that. This would mean that any Query Cache lookup would involve a leap out to another process. Some sort of cache key would have to be generated, any results from the other process would have to be deserialised and then compared against the original search criteria (unless the cache was just a serialised representation of the entire search criteria then there would always be a change of cache key collision, so a single cache key might actually correspond to results from multiple different searches). This seemed like a lot of extra work, compared to just accessing cached references in memory.. &lt;em&gt;but it&#39;s this just-get-bang-it-in-memory approach that has gotten me into trouble in the first place!&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;At this point, I was in no way certain that this would solve my problems and so thinking about setting up an external cache service was starting to feel like an exercise in &lt;a href=&quot;http://sethgodin.typepad.com/seths_blog/2005/03/dont_shave_that.html&quot;&gt;yak shaving&lt;/a&gt;. So I went for the simplest alternative, I implemented a disk cache layer. If I was going to use an external cache then I&#39;d still need a way to serialise the data that would need caching (so that I could send and receive it over the wire) and I&#39;d still need a way to generate cache keys from the search criteria (by hashing the options, basically). I would also have to do that if I was going to just stash the cache values on disk. There would be a few minor complications with a disk cache rather than an off-the-shelf external cache (such as ensuring that old cache files are deleted if they&#39;re not accessed again within a reasonable amount of time) but most of the work to implement a disk cache would come in handy if the hypothesis was proved and a general purpose out-of-process cache for these ten-to-fifteen-minute cache items seemed to help.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(Just in case it&#39;s not completely obvious why a disk cache might work here, it&#39;s because the data isn&#39;t stored in memory for long periods of time any more - any time that the cached data is read from disk into memory, the in-memory representation only lives for the live of the request that the cached data is helping with - it then is free to be collected, meaning that it should never get out of gen 0).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;So I changed the Query Cache so that it didn&#39;t maintain a &lt;strong&gt;ConcurrentDictionary&lt;/strong&gt; of data (meaning, unfortunately, that the work I did for &quot;Step 1&quot; earlier was a waste of time) and, instead, had a simple &lt;strong&gt;ICache&lt;/strong&gt; dependency injected into it. Simple in that it would only have options to read or write serialised data (as byte arrays) for particular keys - the deserialisation and subsequent checking of an &quot;ExpiresAt&quot; time would be handled within the Query Cache class. The &lt;strong&gt;ICache&lt;/strong&gt; implementation read and wrote files on disk, mapping the cache keys onto file names and running a single background thread to tidy up old files that hadn&#39;t been touched for a while. Writing an alternative &lt;strong&gt;ICache&lt;/strong&gt; implementation to talk to Redis would be very easy.&lt;/p&gt;

&lt;p&gt;With this change, I was able to run the entire 100k request sample without issue. In fact, the service has been updated in production using this disk cache. While there are some challenges and compromises with a disk cache*, it&#39;s working &lt;em&gt;well enough&lt;/em&gt; for now that we&#39;re going to leave it be. If it seems like, in the future, that the overhead of persisting to disk is a bottleneck and that a dedicated external cache could significantly improve the performance of individual requests or the overall throughput of the system, then we may change to using one. However, right now, that would just be one more moving part. The advantage of the disk cache is that it&#39;s very simple.&lt;/p&gt;

&lt;p&gt;* &lt;em&gt;(File IO of this type will suffer contention issues but this is a read-only service and so the worst case is  that some database hits that could theoretically have been avoided are processed; if a request comes in whose results are not available in cache then it will get the data live and then try to write to a cache file - if another request comes in whose search criteria gets hashed to the same key then it won&#39;t be possible to read the data for that key while the writing from the first request is taking place)&lt;/em&gt;&lt;/p&gt;

&lt;h3&gt;In conclusion&lt;/h3&gt;

&lt;p&gt;It has now been a couple of weeks that this code has been in production. Over that time, all of the gen 0, 1 and 2 Small Object Heaps have appeared to breathe in and out in a healthy fashion, as has the Large Object Heap. There has been no indication of the slow-memory-usage-climb-to-oblivion that would be seen before.&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;GC Memory Graph&quot; src=&quot;http://www.productiverage.com/Content/Images/Posts/GC Memory Graph.png&quot; class=&quot;HalfWidth&quot;&gt;&lt;/p&gt;

&lt;p&gt;The experience has been very interesting for me, it&#39;s given me a chance to expand my understanding of the garbage collector and to apply what I already knew about it. It would have been the icing on the cake to find out more about just what was happening in the process when it was having one of its &quot;blow ups&quot;, but I&#39;m more glad that it doesn&#39;t look likely to happen again than I am curious as to what was in its mind at the time! It&#39;s given me a fresh appreciation of the garbage collector and it&#39;s served as a reminder that it really is my buddy and not my enemy.&lt;/p&gt;

&lt;p&gt;It&#39;s also gratifying that this service continues to get the love it needs to grow and develop. It doesn&#39;t seem to be particularly uncommon for code to be written that doesn&#39;t expect to be in use more than two years in the future (sometimes simply because a project is released and &quot;handed off&quot; to a client, never to be maintained or updated again - necessitating its replacement in the not-too-distant-future as the real world moves further and further away from what the original solution is able to do).&lt;/p&gt;

&lt;p&gt;I wrote such a large portion of the service code myself that I have to bear the blame for the bad bits as well as the glory for the successes. Those custom non-locking-for-read-but-fully-cloning-for-write dictionaries (replaced in &quot;Step 1&quot; with the more modern &lt;strong&gt;ConcurrentDictionary&lt;/strong&gt;) were my idea and implementation and seemed fanstastic at the time - but I&#39;m not upset in the slightest to have seen the back of them now! It&#39;s a great opoortunity to look back over the years and see not only how technology has moved on since then but also my own knowledge. I very much intend to see it continuing!&lt;/p&gt;</description>
			<pubDate>Wed, 21 Sep 2016 20:12:00 GMT</pubDate>
		</item>
		<item>
			<title>Performance tuning a Bridge.NET / React app</title>
            <link>http://www.productiverage.com/performance-tuning-a-bridgenet-react-app</link>
			<guid>http://www.productiverage.com/performance-tuning-a-bridgenet-react-app</guid>
			<description>&lt;p&gt;On the whole, React is fast. And, on the whole, writing a web application&#39;s code in C# using &lt;a href=&quot;http://bridge.net/&quot;&gt;Bridge.NET&lt;/a&gt; has little overhead compared to writing it directly in JavaScript since Bridge generates sensible JavaScript.&lt;/p&gt;

&lt;p&gt;However, I recently wanted to convince myself that performance would not be an issue with the sort of projects that we&#39;ll be writing at work. We have some applications that are key to the business and yet have unfortunately been left to wither into a barely-maintinable state. The plan is to, over time, rewrite sections of the application using Bridge and React so that the application continues to work at all times but the old code is pruned away. This means that we need to be sure that any crazy forms that existed in the old codebase will work fine in the new architecture. In particular, there is a configuration page that allows a user to select options from a list of almost 1,000 checkboxes. Is this good UI? Most probably not. Do we need to be able to support such configurations in the future? Unfortunately, most probably yes. With a classic server-based MVC application, this would involve 1,000 checkboxes being rendered on the page and then a ginormous form post to send the changes back when the user clicks Save. In a React app, this sort of form will require virtual re-renders each time that a checkbox is clicked on.&lt;/p&gt;

&lt;p&gt;I thought I&#39;d actually go with something slightly more demanding - 5,000 rows on a form where each row has two text boxes and a checkbox. If this can be handled easily then the worst case scenario that we have in mind for our rewrites (1,000 checkboxes) will be a walk in the park.&lt;/p&gt;

&lt;p&gt;So I whipped up a sample app and started using the Chrome profiler.. and the news was not good.&lt;/p&gt;

&lt;p&gt;The total time recorded by the profiler was 838ms to deal with the changing of a single checkbox. It&#39;s said that &lt;a href=&quot;https://www.nngroup.com/articles/response-times-3-important-limits/&quot;&gt;100ms is &quot;the limit for having the user feel that the system is reacting instantaneously&quot;&lt;/a&gt; and 838ms is not just in the same ballpark. What&#39;s even worse is that this delay is experienced not only when a checkbox state is changed but also when any change is applied to one of the text boxes. Waiting almost a second for a checkbox to change is bad but waiting that long for each key press to be registered while typing is unbearable.&lt;/p&gt;

&lt;h3&gt;Examining the test app&lt;/h3&gt;

&lt;p&gt;The test app is fairly simple (and will contain no surprises if you&#39;ve read my &lt;a href=&quot;http://www.productiverage.com/writing-react-apps-using-bridgenet-the-dan-way-from-first-principles&quot;&gt;Writing React apps using Bridge.NET - The Dan Way&lt;/a&gt; three part mini-series). However, the performance improvements that I&#39;m going to cover will be in versions of libraries that I haven&#39;t yet released - namely, &lt;a href=&quot;https://github.com/ProductiveRage/Bridge.React&quot;&gt;Bridge.React&lt;/a&gt;, &lt;a href=&quot;https://github.com/ProductiveRage/Bridge.Immutable&quot;&gt;ProductiveRage.Immutable&lt;/a&gt; and &lt;a href=&quot;https://github.com/ProductiveRage/Bridge.Immutable.Extensions&quot;&gt;ProductiveRage.Immutable.Extensions&lt;/a&gt;. The ProductiveRage.Immutable.Extensions library includes types that I commonly use when writing Bridge / React apps (such as &lt;strong&gt;RequestId&lt;/strong&gt; and &lt;strong&gt;NonBlankTrimmedString&lt;/strong&gt;). So you won&#39;t yet be able to try out the changes that I&#39;m going to discuss but (hopefully!) the process of identifying what changes to make will be useful.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(I&#39;m planning to release the updates to these libraries around the time that Bridge 15.0 comes out, which should hopefully be this month - this will include the change to using Roslyn for parsing the C#, rather than NRefactory, and so C# 6 syntax will finally be supported, which is wonderful news).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;One of the types that will be available in ProductiveRage.Immutable.Extensions is &lt;strong&gt;CommonProps&amp;lt;T&amp;gt;&lt;/strong&gt;. It&#39;s extremely common for component classes to require the same sort of information - what the initial state is, how to record requests to change that state, what class name to apply to the component, whether it should be in a disabled state or not and what key the component has (for cases where it appears as part of a set of dynamic child components).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public sealed class CommonProps&amp;lt;T&amp;gt; : IAmImmutable
{
    public CommonProps(
        T state,
        Action&amp;lt;T&amp;gt; onChange,
        Optional&amp;lt;ClassName&amp;gt; className,
        bool disabled,
        Optional&amp;lt;Any&amp;lt;string, int&amp;gt;&amp;gt; key)
    {
        this.CtorSet(_ =&amp;gt; _.State, state);
        this.CtorSet(_ =&amp;gt; _.OnChange, onChange);
        this.CtorSet(_ =&amp;gt; _.ClassName, className);
        this.CtorSet(_ =&amp;gt; _.Disabled, disabled);
        this.CtorSet(_ =&amp;gt; _.Key, key);
    }

    public T State { get; private set; }
    public Action&amp;lt;T&amp;gt; OnChange { get; private set; }
    public Optional&amp;lt;ClassName&amp;gt; ClassName { get; private set; }
    public bool Disabled { get; private set; }
    public Optional&amp;lt;Any&amp;lt;string, int&amp;gt;&amp;gt; Key { get; private set; }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you have a custom text box component then you want to be able to set the initial text value and to be informed when the user is performing an action that changes the text value. If you have a row in a table that shows a message (such as in the application built up in the three part series) then each row needs to have state describing what to show in the &quot;Content&quot; text box and what to show in the &quot;Author&quot; text box. When the user tries to change of those values, the row needs to have a way to say that the current message state is changing. As a final example, if there is a Message table component then the initial state will be a set of messages to render and the &quot;OnChange&quot; delegate will be used whenever a user wants to change a value in an existing row or when they want to remove a row or when they want to add a row. So it&#39;s a very common pattern and having a generic class to describe it means that there&#39;s less code to write for each component, since they can use this common class rather than each component having their &lt;em&gt;own&lt;/em&gt; props class.&lt;/p&gt;

&lt;p&gt;There are some static factory methods to make initialising &lt;strong&gt;CommonProps&amp;lt;T&amp;gt;&lt;/strong&gt; instances easier:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public static class CommonProps
{
    public static CommonProps&amp;lt;T&amp;gt; For&amp;lt;T&amp;gt;(
        T state,
        Action&amp;lt;T&amp;gt; onChange,
        Optional&amp;lt;ClassName&amp;gt; className,
        bool disabled)
    {
        return new CommonProps&amp;lt;T&amp;gt;(
            state,
            onChange,
            className,
            disabled,
            Optional&amp;lt;Any&amp;lt;string, int&amp;gt;&amp;gt;.Missing
        );
    }

    public static CommonProps&amp;lt;T&amp;gt; For&amp;lt;T&amp;gt;(
        T state,
        Action&amp;lt;T&amp;gt; onChange,
        Optional&amp;lt;ClassName&amp;gt; className,
        bool disabled)
        Any&amp;lt;string, int&amp;gt; key)
    {
        return new CommonProps&amp;lt;T&amp;gt;(state, onChange, className, disabled, key);
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With that in mind, the code below should be easy to understand. For simplicity, state changes are handled directly by the container component (there is no Dispatcher) and all that the app does is render 5,000 rows and allow the user to change either text box in each row or the checkbox that each row has. It might seem like a lot of code but that&#39;s partly due to the way that the lines are wrapped to fit in the blog post and it&#39;s partly because I&#39;ve included &lt;em&gt;all&lt;/em&gt; of the non-shared-library code from the app, which is important so that we can talk about what is and isn&#39;t worth altering.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public static class App
{
    [Ready]
    public static void Main()
    {
        React.Render(
            new AppContainer(),
            Document.GetElementById(&quot;main&quot;)
        );
    }
}

public sealed class AppContainer : Component&amp;lt;object, AppContainer.State&amp;gt;
{
    public AppContainer() : base(null) { }

    protected override State GetInitialState()
    {
        return new State(
            Enumerable.Range(1, 5000)
                .Select(i =&amp;gt; Saved.For(
                    i.ToString(),
                    new MessageEditState(&quot;Title&quot; + i, &quot;Content&quot; + i, isAwesome: false)))
                .ToSet()
        );
    }

    public override ReactElement Render()
    {
        return DOM.Div(
            new Attributes { ClassName = &quot;wrapper&quot; },
            new MessageTable(
                state.Messages,
                updatedMessages =&amp;gt; SetState(new State(updatedMessages)),
                className: new ClassName(&quot;messages&quot;),
                disabled: false
            )
        );
    }

    public sealed class State : IAmImmutable
    {
        public State(Set&amp;lt;Saved&amp;lt;MessageEditState&amp;gt;&amp;gt; messages)
        {
            this.CtorSet(_ =&amp;gt; _.Messages, messages);
        }

        public Set&amp;lt;Saved&amp;lt;MessageEditState&amp;gt;&amp;gt; Messages { get; private set; }
    }
}

public sealed class Saved&amp;lt;T&amp;gt; : IAmImmutable
{
    public Saved(string id, T value)
    {
        this.CtorSet(_ =&amp;gt; _.Id, id);
        this.CtorSet(_ =&amp;gt; _.Value, value);
    }

    public string Id { get; private set; }
    public T Value { get; private set; }
}

public static class Saved
{
    public static Saved&amp;lt;T&amp;gt; For&amp;lt;T&amp;gt;(string id, T value)
    {
        return new Saved&amp;lt;T&amp;gt;(id, value);
    }
}

public sealed class MessageEditState : IAmImmutable
{
    public MessageEditState(string title, string content, bool isAwesome)
    {
        this.CtorSet(_ =&amp;gt; _.Title, title);
        this.CtorSet(_ =&amp;gt; _.Content, content);
        this.CtorSet(_ =&amp;gt; _.IsAwesome, isAwesome);
    }

    public string Title { get; private set; }
    public string Content { get; private set; }
    public bool IsAwesome { get; private set; }
}

public sealed class MessageTable : PureComponent&amp;lt;CommonProps&amp;lt;Set&amp;lt;Saved&amp;lt;MessageEditState&amp;gt;&amp;gt;&amp;gt;&amp;gt;
{
    public MessageTable(
        Set&amp;lt;Saved&amp;lt;MessageEditState&amp;gt;&amp;gt; state,
        Action&amp;lt;Set&amp;lt;Saved&amp;lt;MessageEditState&amp;gt;&amp;gt;&amp;gt; onChange,
        Optional&amp;lt;ClassName&amp;gt; className,
        bool disabled)
            : base(CommonProps.For(state, onChange, className, disabled)) { }

    public override ReactElement Render()
    {
        return DOM.Div(
            new Attributes { ClassName = props.ClassName.ToNullableString() },
            props.State.Select((savedMessage, index) =&amp;gt; new MessageRow(
                savedMessage.Value,
                updatedMessage =&amp;gt; props.OnChange(
                    props.State.SetValue(index, props.State[index].With(_ =&amp;gt; _.Value, updatedMessage))
                ),
                className: null,
                disabled: false,
                key: savedMessage.Id
            ))
        );
    }
}

public sealed class MessageRow : PureComponent&amp;lt;CommonProps&amp;lt;MessageEditState&amp;gt;&amp;gt;
{
    public MessageRow(
        MessageEditState state,
        Action&amp;lt;MessageEditState&amp;gt; onChange,
        Optional&amp;lt;ClassName&amp;gt; className,
        bool disabled,
        Any&amp;lt;string, int&amp;gt; key)
            : base(CommonProps.For(state, onChange, className, disabled, key)) { }

    public override ReactElement Render()
    {
        return DOM.Div(new Attributes { ClassName = props.ClassName.ToNullableString() },
            props.TextBoxFor(_ =&amp;gt; _.Title, &quot;title&quot;),
            props.TextBoxFor(_ =&amp;gt; _.Content, &quot;content&quot;),
            props.CheckboxFor(_ =&amp;gt; _.IsAwesome, &quot;is-awesome&quot;)
        );
    }
}

public static class CommonPropsRenderer
{
    public static ReactElement TextBoxFor&amp;lt;T&amp;gt;(
        this CommonProps&amp;lt;T&amp;gt; props,
        [PropertyIdentifier]Func&amp;lt;T, string&amp;gt; propertyIdentifier,
        string className)
            where T : IAmImmutable
    {
        if (props == null)
            throw new ArgumentNullException(&quot;props&quot;);
        if (propertyIdentifier == null)
            throw new ArgumentNullException(&quot;propertyIdentifier&quot;);

        return DOM.Input(new InputAttributes
        {
            ClassName = className,
            Value = propertyIdentifier(props.State),
            OnChange = e =&amp;gt; props.OnChange(props.State.With(propertyIdentifier, e.CurrentTarget.Value))
        });
    }

    public static ReactElement CheckboxFor&amp;lt;T&amp;gt;(
        this CommonProps&amp;lt;T&amp;gt; props,
        [PropertyIdentifier]Func&amp;lt;T, bool&amp;gt; propertyIdentifier,
        string className)
            where T : IAmImmutable
    {
        if (props == null)
            throw new ArgumentNullException(&quot;props&quot;);
        if (propertyIdentifier == null)
            throw new ArgumentNullException(&quot;propertyIdentifier&quot;);

        return DOM.Input(new InputAttributes
        {
            Type = InputType.Checkbox,
            ClassName = className,
            Checked = propertyIdentifier(props.State),
            OnChange = e =&amp;gt; props.OnChange(props.State.With(propertyIdentifier, e.CurrentTarget.Checked))
        });
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Except for the top-level &lt;strong&gt;AppContainer&lt;/strong&gt;, every component is derived from &lt;strong&gt;PureComponent&amp;lt;T&amp;gt;&lt;/strong&gt; which means that they automatically get implementations for React&#39;s &quot;&lt;a href=&quot;https://facebook.github.io/react/docs/component-specs.html#updating-shouldcomponentupdate&quot;&gt;shouldComponentUpdate&lt;/a&gt;&quot; component life cycle method. This means that if a component needs to be re-rendered by the virtual DOM and if the new props settings are the same as its current props settings then the component will tell React &quot;I&#39;m not going to change, you do not need to re-render me (nor any of my child components)&quot;. I had originally hoped that this would mean that everything would be blazing fast without any additional work. However, as I&#39;ve already said, this was not to be the case.&lt;/p&gt;

&lt;p&gt;Before I get stuck in, it&#39;s worth bearing in mind that this really is a &lt;em&gt;worst case&lt;/em&gt; scenario. If there was a page that required 5,000 entry rows spread over ten different tables then changing any single row would only require the containing table to re-render, the other nine would not need to (the &lt;strong&gt;PureComponent&amp;lt;T&amp;gt;&lt;/strong&gt;&#39;s &quot;shouldComponentUpdate&quot; logic would take take of that). The difficulty here is that all 5,000 rows are in a &lt;em&gt;single&lt;/em&gt; table and so changing any value in any row requires that the table potentially re-render all of its rows. I can&#39;t imagine very many UIs where presenting a user with so many rows simultaneously would be a particularly pleasant experience. Perhaps a spreadsheet of some sort? If you needed to present an interface with tens of thousands of inputs, there are ways to make it faster sucher as &quot;chunking up&quot; groups of rows (so that a change to any single row only requires the other rows in the group potentially to re-render and not any other group). A more complicated (but highly efficient) approach would be to work out what data is currently visible in the browser window and to only update that.&lt;/p&gt;

&lt;p&gt;Rather than considering these alternatives at this point, though, I want to see what we can do with the sample app as it&#39;s presented.&lt;/p&gt;

&lt;h3&gt;Profiling&lt;/h3&gt;

&lt;p&gt;&lt;img alt=&quot;Initial timings (not good)&quot; src=&quot;http://www.productiverage.com/Content/Images/Posts/BridgeReactProfiling1.png&quot; class=&quot;NoBorder HalfWidth&quot;&gt;&lt;/p&gt;

&lt;p&gt;The first thing to do was to start measuring and digging. I loaded the page in Chrome, opened the dev tools, went to the Profiles tab, clicked &quot;Start CPU profiling&quot;, clicked a checkbox and then &quot;Stop CPU profiling&quot;. The result is shown here. There is a natural split between two processes - the &quot;Render&quot; method of the &lt;strong&gt;MessageTable&lt;/strong&gt; and the &quot;receiveComponent&quot; / &quot;updateComponent&quot; within React. I know that it&#39;s the &lt;strong&gt;MessageTable&lt;/strong&gt;&#39;s Render method because it calls &quot;select&quot; (the LINQ function) and that will be where the &lt;strong&gt;MessageTable&lt;/strong&gt; creates each &lt;strong&gt;MessageRow&lt;/strong&gt;. I&#39;m going to concentrate there first since that&#39;s where most of the time is taken and it&#39;s also what I have the most direct control over.&lt;/p&gt;

&lt;p&gt;Just one thing to check first, though - I&#39;m using the development build of the React library at this point, which has some overhead compared to the production version (since it performs more checks and does more work in order to provide more helpful warnings, where required). Changing to the production build trims some time off; the &lt;strong&gt;MessageTable&lt;/strong&gt; &quot;Render&quot; method still takes 609ms but &quot;receiveComponent&quot; takes about half as much time, now 128ms. Clearly, the production build is not going to magically solve all of my problems.&lt;/p&gt;

&lt;p&gt;The Chrome dev tools allow you to zoom in on sections of the profiler results, so I tried to make sense of what I could see under the &quot;Render&quot; side. The problem was that it seemed like there were lots of nested calls where none were individually very expensive, it seemed like a cumulative problem with just how many components there were. There were a lot of calls to &quot;constructor&quot;, which suggested to me that there may be some overhead in creating Bridge classes. To try to test this theory, I added a new option to the React bindings to enable components to be created by providing a static function rather than creating a component class that is derived from &lt;strong&gt;Component&amp;lt;TProps, TState&amp;gt;&lt;/strong&gt; or &lt;strong&gt;PureComponent&amp;lt;TProps&amp;gt;&lt;/strong&gt;. This allows &lt;strong&gt;MessageRow&lt;/strong&gt; to be rewritten as:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public static class MessageRow
{
    [Name(&quot;MessageRow&quot;)]
    public static ReactElement Render(CommonProps&amp;lt;MessageEditState&amp;gt; props)
    {
        return DOM.Div(new Attributes { ClassName = props.ClassName.ToNullableString() },
            props.TextBoxFor(_ =&amp;gt; _.Title, &quot;title&quot;),
            props.TextBoxFor(_ =&amp;gt; _.Content, &quot;content&quot;),
            props.CheckboxFor(_ =&amp;gt; _.IsAwesome, &quot;is-awesome&quot;)
        );
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;which requires &lt;strong&gt;MessageTable&lt;/strong&gt; to be changed to:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public sealed class MessageTable : PureComponent&amp;lt;CommonProps&amp;lt;Set&amp;lt;Saved&amp;lt;MessageEditState&amp;gt;&amp;gt;&amp;gt;&amp;gt;
{
    public MessageTable(
        Set&amp;lt;Saved&amp;lt;MessageEditState&amp;gt;&amp;gt; state,
        Action&amp;lt;Set&amp;lt;Saved&amp;lt;MessageEditState&amp;gt;&amp;gt;&amp;gt; onChange,
        Optional&amp;lt;ClassName&amp;gt; className,
        bool disabled)
            : base(CommonProps.For(state, onChange, className, disabled)) { }

    public override ReactElement Render()
    {
        return DOM.Div(
            new Attributes { ClassName = props.ClassName.ToNullableString() },
            props.State.Select((savedMessage, index) =&amp;gt; StaticComponent.Pure(
                MessageRow.Render,
                CommonProps.For(
                    savedMessage.Value,
                    updatedMessage =&amp;gt; props.OnChange(
                        props.State.SetValue(index, props.State[index].With(_ =&amp;gt; _.Value, updatedMessage))
                    ),
                    className: null,
                    disabled: false,
                    key: savedMessage.Id
                )
            ))
        );
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This way, there are 5,000 &lt;strong&gt;MessageRow&lt;/strong&gt; constructor calls saved each time that the &lt;strong&gt;MessageTable&lt;/strong&gt; needs to re-render. (Under the hood, there is still an object created for each row but it&#39;s a very lightweight JavaScript object).&lt;/p&gt;

&lt;p&gt;This reduced the &quot;Render&quot; time to 496ms (it didn&#39;t affect &quot;receiveComponent&quot;, but I didn&#39;t expect it to). This was a good start and made me want to look further into the cost of class instantiation in Bridge.&lt;/p&gt;

&lt;h3&gt;Bridge generic classes are more expensive&lt;/h3&gt;

&lt;p&gt;I whipped up a quick test to try creating lots of instances of a class, like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public static class App
{
    [Ready]
    public static void Main()
    {
        var x = new MyClass[10000];
        var timer = Stopwatch.StartNew();
        for (var i = 0; i &amp;lt; x.Length; i++)
            x[i] = new MyClass(&quot;test&quot;);
        timer.Stop();
        Console.WriteLine(timer.ElapsedMilliseconds + &quot;ms&quot;);
    }
}

public class MyClass
{
    public MyClass(string value)
    {
        Value = value;
    }
    public string Value { get; private set; }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That only reported 3ms, which didn&#39;t seem like it could be the source of the problem.&lt;/p&gt;

&lt;p&gt;Next I tried going one step more complicated. The &lt;strong&gt;MessageRow&lt;/strong&gt; class that I&#39;ve replaced with a static function was derived from &lt;strong&gt;PureComponent&amp;lt;T&amp;gt;&lt;/strong&gt;, which means that each &lt;strong&gt;MessageRow&lt;/strong&gt; instantiation also involved an instantiation of a generic base class. Clearly &lt;em&gt;something&lt;/em&gt; is still taking up a lot time.. since the &lt;strong&gt;CommonProps&amp;lt;T&amp;gt;&lt;/strong&gt; class used for &lt;strong&gt;MessageRow&lt;/strong&gt; props was a generic type, maybe it&#39;s something specifically to  do with generic types.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public static class App
{
    [Ready]
    public static void Main()
    {
        var x = new MyClass&amp;lt;string&amp;gt;[10000];
        var timer = Stopwatch.StartNew();
        for (var i = 0; i &amp;lt; x.Length; i++)
            x[i] = new MyClass&amp;lt;string&amp;gt;(&quot;test&quot;);
        timer.Stop();
        Console.WriteLine(timer.ElapsedMilliseconds + &quot;ms&quot;);
    }
}

public class MyClass&amp;lt;T&amp;gt;
{
    public MyClass(T value)
    {
        Value = value;
    }
    public T Value { get; private set; }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This time it reported 35ms. Still not an earth-shattering duration in isolation but a big step up from the non-generic class&#39; 3ms.&lt;/p&gt;

&lt;p&gt;One of the nice things about Bridge is that it allows you to tweak the way that the JavaScript is generated. By default, it will strike a good balance between creating reasonable JavaScript while also creating code that is faithful to the C# representation. For example, the &lt;strong&gt;MyClass&amp;lt;T&amp;gt;&lt;/strong&gt; class will get the following JavaScript definition:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Bridge.define(&#39;Demo.MyClass$1&#39;, function (T) { return {
    config: {
        properties: {
            Value: Bridge.getDefaultValue(T)
        }
    },
    constructor: function (value) {
        this.setValue(value);
    }
}; });
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It&#39;s important that the type param &quot;T&quot; be available as a reference at runtime in case you ever need to access it (such as via a call to &quot;default(T)&quot; or when needing to instantiate another generic type whose type param will also be &quot;T&quot;). If the type &quot;T&quot; was not known to the runtime then it wouldn&#39;t be possible for the JavaScript code to do things like create a &quot;default(T)&quot; value appropriate to whatever &quot;T&quot; is; it should be null for a reference type, zero for a numeric type and false for a boolean.&lt;/p&gt;

&lt;p&gt;However, this creation of a class that encapsulates the type parameters must incur some overhead. For comparison, the non-generic class is defined in JavaScript with the following (note the lack of the function that captures &quot;T&quot;) -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Bridge.define(&#39;Demo.MyClass&#39;, {
    config: {
        properties: {
            Value: null
        }
    },
    constructor: function (value) {
        this.setValue(value);
    }
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;One of the options that Bridge has to affect what JavaScript is emitted is the [IgnoreGeneric] attribute. If this is applied to a class then it &lt;em&gt;won&#39;t&lt;/em&gt; be given a JavaScript definition that includes the type parameter. This means that we can create a generic C# class (and continue to fully take advantage of the safety of the C# type system) but have Bridge generate a cheaper-to-instantiate JavaScript representation.&lt;/p&gt;

&lt;p&gt;There is one problem with this, though. The C# code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[IgnoreGeneric]
public class MyClass&amp;lt;T&amp;gt;
{
    public MyClass(T value)
    {
        Value = value;
    }
    public T Value { get; private set; }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;will result in the following JavaScript:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Bridge.define(&#39;Demo.MyClass$1&#39;, {
    config: {
        properties: {
            Value: Bridge.getDefaultValue(T)
        }
    },
    constructor: function (value) {
        this.setValue(value);
    }
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All properties are set to default values before any instances are created. This is important for cases where there are constructors where one or more properties are not explicitly set since they can&#39;t be left undefined. In C#, if you don&#39;t set a property on a class instance then it will be left as its default value (null for a reference type, zero for a number, etc..) and Bridge has to maintain this behaviour in JavaScript in order to be consistent. The problem here is that the type &quot;T&quot; is not available and so the &quot;Value&quot; property &lt;em&gt;can&#39;t&lt;/em&gt; reliably be set to the correct default value.&lt;/p&gt;

&lt;p&gt;Since I&#39;m considering tweaking the &lt;strong&gt;CommonProps&amp;lt;T&amp;gt;&lt;/strong&gt; class, this doesn&#39;t apply - every property will explicitly be set in the constructor and so I don&#39;t have to worry about the case of a property needing to be left with the default value for the type.&lt;/p&gt;

&lt;p&gt;Thankfully, Bridge has &lt;em&gt;another&lt;/em&gt; way to control the JavaScript that will be helpful. The [Template] attribute may be applied to property getters and setters and will change how these are represented. The default is for &quot;setValue(x)&quot; and &quot;getValue()&quot; methods to be created on the class (this may be seen in the above code, where &quot;this.setValue(value)&quot; is called in the constructor). If the getter is marked with [Template(&quot;value&quot;)] then anywhere that would previously have called &quot;getValue()&quot; will now simply access &quot;value&quot; and if the setter is marked with [Template(&quot;this.value&quot;)] then the property-setting (which only happens in the constructor for &lt;strong&gt;CommonProps&amp;lt;T&amp;gt;&lt;/strong&gt;) will not be a call to &quot;setValue&quot;, it will simply set &quot;this.value&quot;.&lt;/p&gt;

&lt;p&gt;To apply this to the &lt;strong&gt;MyClass&amp;lt;T&amp;gt;&lt;/strong&gt; class, the following C#:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[IgnoreGeneric]
public class MyClass&amp;lt;T&amp;gt;
{
    public MyClass(T value)
    {
        Value = value;
    }
    public T Value { [Template(&quot;value&quot;)]get; [Template(&quot;this.value&quot;)]private set; }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;would result in the following JavaScript:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Bridge.define(&#39;Demo.MyClass$1&#39;, {
    constructor: function (value) {
        this.value = value;
    }
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that the set-properties-to-default-values code is no longer present in the JavaScript class definition.&lt;/p&gt;

&lt;p&gt;Also, it&#39;s worth noting that this will affect anywhere that the property is accessed by code outside of the class. For example, if there is C# like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var x = new MyClass&amp;lt;string&amp;gt;(&quot;test&quot;);
Console.WriteLine(x.Value);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. then, instead of the property being accessed through a getter method -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var x = new Demo.MyClass$1(&quot;test&quot;);
Bridge.Console.log(x.getValue());
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. it will be accessed directly -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var x = new Demo.MyClass$1(&quot;test&quot;);
Bridge.Console.log(x.value);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This means that the JavaScript is a slightly less faithful representation of the C# code. However, the C# compiler is complete unaware of these changes and it will continue to enforce the type system in the same way that it always does. So (presuming you are writing all of your front end code in C#, using Bridge) you are not losing anything. In fact, there will be some more performance gains to be had by accessing properties directly like this - there is a small overhead to calling functions to return values (small, but not zero) as opposed to retrieving them directly.&lt;/p&gt;

&lt;p&gt;If this is applied to &lt;strong&gt;CommonProps&amp;lt;T&amp;gt;&lt;/strong&gt; then we get the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[IgnoreGeneric]
public sealed class CommonProps&amp;lt;T&amp;gt;
{
    public CommonProps(
        T state,
        Action&amp;lt;T&amp;gt; onChange,
        Optional&amp;lt;ClassName&amp;gt; className,
        bool disabled,
        Optional&amp;lt;Any&amp;lt;string, int&amp;gt;&amp;gt; key)
    {
        if (state == null)
            throw new ArgumentNullException(&quot;state&quot;);
        if (onChange == null)
            throw new ArgumentNullException(&quot;onChange&quot;);

        State = state;
        OnChange = onChange;
        ClassName = className;
        Disabled = disabled;
        Key = key;
    }

    public T State
    {
        [Template(&quot;state&quot;)]get; [Template(&quot;this.state&quot;)]private set;
    }
    public Action&amp;lt;T&amp;gt; OnChange
    {
        [Template(&quot;onChange&quot;)]get; [Template(&quot;this.onChange&quot;)]private set;
    }
    public Optional&amp;lt;ClassName&amp;gt; ClassName
    {
        [Template(&quot;className&quot;)]get; [Template(&quot;this.className&quot;)]private set;
    }
    public bool Disabled
    {
        [Template(&quot;disabled&quot;)]get; [Template(&quot;this.disabled&quot;)]private set;
    }
    public Optional&amp;lt;Any&amp;lt;string, int&amp;gt;&amp;gt; Key
    {
        [Template(&quot;key&quot;)]get; [Template(&quot;this.key&quot;)]private set; 
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In order to do this, &lt;strong&gt;CommonProps&amp;lt;T&amp;gt;&lt;/strong&gt; could no longer be an &lt;strong&gt;IAmImmutable&lt;/strong&gt; type since the &quot;CtorSet&quot; and &quot;With&quot; methods won&#39;t work with properties that rely upon any fancy shenanigans like [Template]. This isn&#39;t a huge deal with the props on components since they are always created fresh for every render, unlike the other data types that represent state. For example, when the title value of a single row is edited, a new &lt;strong&gt;MessageEditState&lt;/strong&gt; instance is created using something like the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;newMessage = currentMessage.With(_ =&amp;gt; _.Title, newTitle)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is important for two reasons. Firstly, if &quot;newTitle&quot; is the same as the current title (which can happen if the user does something to a text box that doesn&#39;t actually change ft&#39;s value - such as paste a value into the box that is the same as the current value; React will identfy this as an input change even though the value hasn&#39;t actually been altered) then a new message instance is &lt;em&gt;not&lt;/em&gt; created. When the &lt;strong&gt;MessageRow&lt;/strong&gt; is re-rendered, because the &lt;strong&gt;MessageEditState&lt;/strong&gt; reference won&#39;t have changed, the &lt;strong&gt;PureComponent&lt;/strong&gt; logic will tell React that there is no need to re-render the row, which saves React some work. Secondly, it&#39;s very convenient to be able to get a new instance of a data type with a single property changed in this manner - otherwise you would have to deal with the has-this-value-really-changed logic and either define &quot;With{x}&quot; methods for each individual property or call the constructor with the value that has changed &lt;em&gt;and&lt;/em&gt; all of the ones that haven&#39;t. Which gets old very quickly. (You &lt;em&gt;could&lt;/em&gt; use mutable data types but then you wouldn&#39;t be able perform inexpensive reference equality checks when trying to determine whether a component needs to re-render and so you end up contemplating expensive deep equality checks or you give up on implementing &quot;shouldComponentUpdate&quot; and force React to do much more work).&lt;/p&gt;

&lt;p&gt;One final note: the CtorSet method that &lt;strong&gt;IAmImmutable&lt;/strong&gt; types can use ensures that no value is ever null (if you have a property that may or may not have a value then use the &lt;strong&gt;Optional&amp;lt;T&amp;gt;&lt;/strong&gt; type - which can never be null itself since it&#39;s a struct). Since &lt;strong&gt;CommonProps&amp;lt;T&amp;gt;&lt;/strong&gt; isn&#39;t using CtorSet any more, the constructor needs to include explicit checks for null &quot;state&quot; and &quot;onChange&quot; constructor arguments.&lt;/p&gt;

&lt;p&gt;With this change to &lt;strong&gt;CommonProps&amp;lt;T&amp;gt;&lt;/strong&gt;, the &quot;Render&quot; time is now 124ms in the React development build. Interestingly, in the React production, the &quot;Render&quot; time is reduced to 69ms and the &quot;receiveComponent&quot; drops to 98ms. A combined 167ms is much better than the original 838ms.&lt;/p&gt;

&lt;p&gt;With these improvements, there is only a slightly perceptible delay felt when clicking a checkbox. Unfortunately, though, trying to type into a text box when there is a 167ms delay between key presses being recognised is not pleasant. So it&#39;s back to the profiler..&lt;/p&gt;

&lt;h3&gt;Optional&amp;lt;T&amp;gt;&lt;/h3&gt;

&lt;p&gt;Taking another snapshot with the profiler, I&#39;m still going to concentrate on the &quot;Render&quot; method (for the same reasons as before; it&#39;s still the slower part of the work and it&#39;s still what I can most easily control). This time I see a lot of calls to a generic constructor resulting from &quot;op_Implicit&quot; calls.&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;Unnecessary Optional instantiation&quot; src=&quot;http://www.productiverage.com/Content/Images/Posts/BridgeReactProfiling2.png&quot; class=&quot;NoBorder FullWidth&quot;&gt;&lt;/p&gt;

&lt;p&gt;The &quot;op_Implicit&quot; methods are the JavaScript representations of implicit operator methods in C#. So, where the &lt;strong&gt;Optional&amp;lt;T&amp;gt;&lt;/strong&gt; struct has an implicit operator from &quot;T&quot; -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public static implicit operator Optional&amp;lt;T&amp;gt;(T value)
{
    return new Optional&amp;lt;T&amp;gt;(value);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;the following JavaScript is generated:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;op_Implicit: function (value) {
    return new (ProductiveRage.Immutable.Optional$1(T)).$constructor1(value);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When a &lt;strong&gt;CommonProps&lt;/strong&gt; instance is created with a null &quot;className&quot; argument (which is the case for every &lt;strong&gt;MessageRow&lt;/strong&gt; in the sample app), each call to the &lt;strong&gt;CommonProps&lt;/strong&gt; &quot;For&quot; method requires the null reference to be implicitly cast to an &lt;strong&gt;Optional&amp;lt;ClassName&amp;gt;&lt;/strong&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public static CommonProps&amp;lt;T&amp;gt; For&amp;lt;T&amp;gt;(
    T state,
    Action&amp;lt;T&amp;gt; onChange,
    Optional&amp;lt;ClassName&amp;gt; className,
    bool disabled,
    Any&amp;lt;string, int&amp;gt; key)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Each implicit cast requires a call to the implicit operator, which creates a new &lt;strong&gt;Optional&amp;lt;ClassName&amp;gt;&lt;/strong&gt; instance. This feels like unnecessary work.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;Optional&amp;lt;T&amp;gt;&lt;/strong&gt; has a public static &quot;Missing&quot; property, so one way to avoid the creation of unnecessary instances would be to use&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;className: Optional&amp;lt;ClassName&amp;gt;.Missing
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;instead of&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;className: null
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But there were a few problems with this. Firstly, &lt;strong&gt;Optional&amp;lt;T&amp;gt;&lt;/strong&gt; is part of the ProductiveRage.Immutable library and I would like it to be as easy to use as possible. I think that it would be quite difficult to justify a significant performance cost in passing null as an Optional rather than &quot;Missing&quot; when there is an implicit cast to perform the translation. Secondly, the &quot;Missing&quot; property was implemented as&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    public static Optional&amp;lt;T&amp;gt; Missing { get { new Optional&amp;lt;T&amp;gt;(default(T), false); } }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. which means that a new instance is created each time it&#39;s called anyway, so actually the &quot;Missing&quot; property wouldn&#39;t magically solve anything.&lt;/p&gt;

&lt;p&gt;It would make more sense for the &quot;Missing&quot; property to be set only once, something like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public static Optional&amp;lt;T&amp;gt; Missing { get { return _missing; } }
private static Optional&amp;lt;T&amp;gt; _missing = new Optional&amp;lt;T&amp;gt;(default(T), false);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When I first wrote the &lt;strong&gt;Optional&amp;lt;T&amp;gt;&lt;/strong&gt; struct, that is how I did it. Unfortunately, there was a problem with Bridge 1.10 and I removed the private &quot;_missing&quot; field as a workaround. The Bridge Team have long since resolved that issue and so I can put the code back how I want it.&lt;/p&gt;

&lt;p&gt;This also allows for a tweak to the implicit operator method -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public static implicit operator Optional&amp;lt;T&amp;gt;(T value)
{
    if (value == null)
        return _missing;
    return new Optional&amp;lt;T&amp;gt;(value);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, one might presume, there would now be no unnecessary instantiations whether &quot;className: Optional&amp;lt;ClassName&amp;gt;.Missing&quot; &lt;em&gt;or&lt;/em&gt; &quot;className: null&quot; was specified. Unfortunately, we&#39;re not quite there yet..&lt;/p&gt;

&lt;p&gt;When structs are passed around in C#, they are copied. This is why they appear to be passed &quot;by value&quot; rather than &quot;by reference&quot; - if a mutable struct is instantiated in method F1 and passed to F2, any changes made to it in F2 are not visible in F1 since they both have different copies of the struct. To ensure consistency with .net, Bridge&#39;s JavaScript must do something similar - any time that a struct is passed around, it is copied. This means that a new instance &lt;em&gt;will&lt;/em&gt; be created each time that &quot;Missing&quot; or &quot;_missing&quot; is accessed. This is wasteful with the &lt;strong&gt;Optional&amp;lt;T&amp;gt;&lt;/strong&gt; struct since it&#39;s immutable; since nothing can alter its contents, there is no need to copy it when passing it around.&lt;/p&gt;

&lt;p&gt;Bridge has another workaround for this, the [Immutable] attribute. When applied to the &lt;strong&gt;Optional&amp;lt;T&amp;gt;&lt;/strong&gt; struct, the Bridge compiler will not copy instances when they are passed from one method to another. These changes reduce the &quot;Render&quot; time to 93ms in the React development build and 61ms in production.&lt;/p&gt;

&lt;p&gt;While this is an improvement, I can still see what looks like a lot of time spent on generic type &lt;em&gt;stuff&lt;/em&gt; in the profiler. Even though the op_Implicit calls for null values are sharing instances now, in order to get to the static op_Implicit method it is necessary to access the representation of the &lt;strong&gt;Optional&amp;lt;T&amp;gt;&lt;/strong&gt; struct for the particular type. And, I suspect, this incurs a similar cost to instantiating a new instance.&lt;/p&gt;

&lt;p&gt;To confirm this, I added [IgnoreGeneric] to &lt;strong&gt;Optional&amp;lt;T&amp;gt;&lt;/strong&gt;. This was not something I really wanted to do since it would require a minor change to the struct&#39;s public interface. There are two properties; IsDefined and Value. Currently there are two states - a state where IsDefined is true and Value has a specified &quot;T&quot; value and a state where IsDefined is false and Value has the default value of &quot;T&quot; (null for a reference type, zero for a number). With the [IgnoreGeneric] attribute, it would not be possible to set the default value of &quot;T&quot; for the &quot;Missing&quot; value state since &quot;T&quot; would not be available at runtime. If I was to apply [IgnoreGeneric] to the struct then &quot;Value&quot; would have to be considered undefined if IsDefined was false. This isn&#39;t a huge deal since I think that that&#39;s how it should have been interpreted anyway, really (an alternative would have been to be more aggressive and throw an exception from the Value property getter if IsDefined is false) but it&#39;s still a change.&lt;/p&gt;

&lt;p&gt;When I added [IgnoreGeneric] to the &lt;strong&gt;CommonProps&amp;lt;T&amp;gt;&lt;/strong&gt; class, I had to apply some workarounds to deal with the type &quot;T&quot; not being available at runtime. I had to do similar with &lt;strong&gt;Optional&amp;lt;T&amp;gt;&lt;/strong&gt;. The first change was that the following line clearly wouldn&#39;t work:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;private static Optional&amp;lt;T&amp;gt; _missing = new Optional&amp;lt;T&amp;gt;(default(T), false);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;so it was replaced with this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;private static Optional&amp;lt;T&amp;gt; _missing = new Optional&amp;lt;T&amp;gt;(Script.Write&amp;lt;T&amp;gt;(&quot;null&quot;), false);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &quot;Script.Write&amp;lt;T&amp;gt;&quot; method in Bridge is a way to directly emit JavaScript (simply &quot;null&quot; in this case) and to tell the C# type system that a value of type &quot;T&quot; is being returned. So, here, the &quot;T&quot; is only used by the C# compiler and does not have any impact on runtime. The compromise is that &quot;null&quot; is being used for the Value property of the &quot;Missing&quot; instance regardless of the type of &quot;T&quot;. So Value will be null even if &quot;T&quot; is an int or a bool in cases where IsDefined is false.&lt;/p&gt;

&lt;p&gt;The other change required completely removing the C# backing field for the Value property -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;private readonly T value;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The problem was that Bridge would generate a struct definition that would try to set &quot;value&quot; to default(T), which it would not be able to do since &quot;T&quot; would not be available at runtime.&lt;/p&gt;

&lt;p&gt;Instead, the value would be written directly by more raw JavaScript. The constructor changed from:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public Optional(T value) : this(value, value != null) { }
    this.isDefined = isDefined &amp;amp;&amp;amp; (value != null);
    this.value = value;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;to:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public Optional(T value) : this(value, value != null) { }
    this.isDefined = isDefined &amp;amp;&amp;amp; (value != null);
    Script.Write(&quot;this.value = {0}&quot;, value);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and the property getter changed from:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public T Value { get { return this.value; } }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;to:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public T Value { get { return Script.Write&amp;lt;T&amp;gt;(&quot;this.value&quot;); } }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, anywhere in the struct that the backing field was accessed was changed so that it went via the public &quot;Value&quot; property getter.&lt;/p&gt;

&lt;p&gt;This meant that there were no potential runtime errors waiting to occur within the struct (none of the code relied on access to the type &quot;T&quot;), that there was type safety for any code instantiating or accessing the struct in C# &lt;em&gt;and&lt;/em&gt; it meant that the struct could have [IgnoreGeneric] applied and hence (theoretically) allow the application to work more efficiently.&lt;/p&gt;

&lt;p&gt;It worked. Using the development build of React, the &quot;Render&quot; time of the &lt;strong&gt;MessageTable&lt;/strong&gt; was now 36ms and the &quot;receiveComponent&quot; time 141ms. With the production build, &quot;Render&quot; took &quot;9ms&quot; and &quot;receiveComponent&quot; 49ms.&lt;/p&gt;

&lt;p&gt;That&#39;s sufficiently fast that there is no perceived delay while typing into the text boxes. And, to put things back into context, the original &quot;worst case scenario&quot; that I was planning for was to deal with up to 1,000 checkboxes. I&#39;ve been measuring the time for 5,000 rows that include two text boxes &lt;em&gt;and&lt;/em&gt; a checkbox. If the sample app was changed to render only 1,000 rows then the React production build handles changes to elements by spending 5ms in &quot;Render&quot; and 17ms in &quot;receiveComponent&quot;. This means that there is no chance of perceptible lag in typing and certainly no perceptible delay in checking or unchecking a checkbox.&lt;/p&gt;

&lt;h3&gt;To summarise&lt;/h3&gt;

&lt;p&gt;I think that it&#39;s fair to call this a success! There are several things that I&#39;ve particuarly enjoyed in this investigation. Firstly, it&#39;s been a good reminder of just how powerful the dev tools are that come free with browsers these days. I was using Chrome but I believe that IE and Firefox have equivalent functionality. Secondly, the options that the Bridge Team have made available are really well thought out and very clever when you examine them - in isolation, each seems quite simple but it&#39;s the recognition that &lt;em&gt;sometimes&lt;/em&gt; it might be beneficial to have more control over the generated JavaScript that helps make Bridge so powerful and to enable me to do what I&#39;ve done here. Thirdly, almost all of the changes that I&#39;ve talked about here were made to my &quot;Bridge.React&quot;, &quot;ProductiveRage.Immutable&quot;, &quot;ProductiveRage.Immutable.Extensions&quot; libraries. That means that, when I make these changes live, anyone using those libraries will automatically reap the benefit. The only change that I made to the sample app was to change the &lt;strong&gt;MessageRow&lt;/strong&gt; implementation from being a component class to being a static function.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note: I tried reverting &lt;strong&gt;MessageRow&lt;/strong&gt; back to being a component class and the &quot;Render&quot; time was still only 20ms when editing one of 1,000 rows (compared to 5ms when &lt;strong&gt;MessageRow&lt;/strong&gt; is implemented as a static function). The time spent by React in &quot;receiveComponent&quot; was unaffected. This means that simply updating the Bridge.React, ProductiveRage.Immutable and ProductiveRage.Immutable.Extensions packages could significantly improve the performance of complex applications with zero code changes.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This is one of the benefits of using libraries where the authors care about performance and strive to improve it over time. It reminds me of when the Bridge Team added compiler support for &quot;&lt;a href=&quot;http://bridge.net/bridge-1-11-released/#Lifted_Anonymous_Functions&quot;&gt;Lifted Anonyomous Functions&lt;/a&gt;&quot; (something I suggested after going on a bit of a JavaScript performance research binge a few months ago - but something that the team there deserve much credit for making work) and it reminds me of articles that I&#39;ve read about React which talk about how there are many optimisations yet to be made that their current API will make possible (see &quot;&lt;a href=&quot;https://github.com/acdlite/react-fiber-architecture&quot;&gt;React Fiber Architecture&lt;/a&gt;&quot;); all that we&#39;ll have to do in the future is upgrade the version of the library being used and get more performance!&lt;/p&gt;

&lt;h3&gt;Update: The Bridge Team ruin my fun&lt;/h3&gt;

&lt;p&gt;I&#39;ve been researching and writing this post over the space of a couple of weeks. Once I had observed that generic classes are slower to instantiate in Bridge than non-generic classes, and while I was looking into the workarounds required sometimes in order to use [IgnoreGeneric], I raised a bug on the &lt;a href=&quot;http://forums.bridge.net/forum/bridge-net-pro/bugs&quot;&gt;Bridge Forums&lt;/a&gt; relating to properties that are initially to default(T) (which fails when &quot;T&quot; is not available at runtime).&lt;/p&gt;

&lt;p&gt;While looking into the issue for me, they noted that they found a way to optimise the instantiation of generic types (looking at the &lt;a href=&quot;https://github.com/bridgedotnet/Bridge/pull/1740/files&quot;&gt;pull request&lt;/a&gt; it seems like the work required to form a new specialisation of a class / struct for a given &quot;T&quot; is now cached rather than being repeated each time that a new &lt;strong&gt;Whatever&amp;lt;T&amp;gt;&lt;/strong&gt; is created).&lt;/p&gt;

&lt;p&gt;The good news is that this means that there will very soon be almost zero overhead to generic types in Bridge! The bad news is that many of the findings documented here are unnecessary.. However, that&#39;s the sort of bad news that I&#39;m happy to accept! The compromise around &lt;strong&gt;Optional&amp;lt;T&amp;gt;&lt;/strong&gt;&#39;s &quot;Value&quot; property (for cases where &quot;IsDefined&quot; is false) will no longer be necessary. And I won&#39;t have to worry so much in the future; if I&#39;m creating a library class, should I be avoiding generics (or using [IgnoreGeneric]) in case it&#39;s used in an expensive loop anywhere?&lt;/p&gt;

&lt;p&gt;Despite being out of date even before being published, I&#39;ll leave this post here for posterity. I had a lot of fun digging into performance tuning my Bridge / React app. And, in a roundabout way, I feel like I contributed to the optimisation (which I imagine will makes its way into the next release of Bridge) that everyone using Bridge can benefit from! I&#39;m going to call that a win.&lt;/p&gt;</description>
			<pubDate>Tue, 06 Sep 2016 22:04:00 GMT</pubDate>
		</item>
		<item>
			<title>Retrieving Performance Counter from a remote PC using C#</title>
            <link>http://www.productiverage.com/retrieving-performance-counter-from-a-remote-pc-using-c-sharp</link>
			<guid>http://www.productiverage.com/retrieving-performance-counter-from-a-remote-pc-using-c-sharp</guid>
			<description>&lt;p&gt;&lt;a href=&quot;http://www.productiverage.com/why-is-saving-performance-monitor-perfmon-settings-so-difficult-these-days&quot;&gt;PerfMon&lt;/a&gt; can be an invaluable tool for monitoring performance counters on the local or remote computer. It allows you to graph the information live and it allows you to write the data away to disk for future analysis.&lt;/p&gt;

&lt;p&gt;However, for some performance investigation that I was doing, I wanted something slightly different to what PerfMon offers. I was testing a service under load, a service that was being hosted on a dedicated box for the performance investigation - and I was testing it by generating the load from another dedicated server. Since nothing else would be hitting the service host box, what I wanted to do for each test run was to restart the service on the host, hit it with the sample load and record the processor time, % time in GC, number of garbage collections at each generation and some other metrics until the work was fully processed - at that point, there would be no more information to gather for that particular run. The experiment could be repeated a few times and the results filed away, brought back out to compare to the same load being run after some performance tweaks had been made to the code.&lt;/p&gt;

&lt;p&gt;It wouldn&#39;t be the end of the world if I had to do this manually - configure PerfMon to write the counter data to disk somewhere, restart the service before each run and then extract the data from the PerfMon logs that relate to the time period that just passed.. but it&#39;s tedious work that I don&#39;t want to bother with; I want to deploy a change then run-test-and-gather-data with a single button press. Better than that, I want to be able to perform multiple runs without any manual intervention - I want to deploy the new code then have the test harness restart the service, replay the test load, record the counter data in a file and then repeat as many times as desired.&lt;/p&gt;

&lt;h3&gt;Restarting the service&lt;/h3&gt;

&lt;p&gt;This part is easy, we can use a method such as this -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;private static void Restart(string server, string serviceName)
{
  if (string.IsNullOrWhiteSpace(server))
    throw new ArgumentException($&quot;Null/blank {nameof(server)} specified&quot;);
  if (string.IsNullOrWhiteSpace(serviceName))
    throw new ArgumentException($&quot;Null/blank {nameof(serviceName)} specified&quot;);

  // Add a reference to System.ServiceProcess to make ServiceController available
  using (var serviceController = new ServiceController(serviceName, server))
  {
    serviceController.Stop();
    serviceController.WaitForStatus(ServiceControllerStatus.Stopped);
    serviceController.Start();
    serviceController.WaitForStatus(ServiceControllerStatus.Running);
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;Querying a performance counter remotely&lt;/h3&gt;

&lt;p&gt;This bit is a little trickier.. I started with code from an article &lt;a href=&quot;http://haishibai.blogspot.co.uk/2010/02/tiy-collect-remote-performance-counters.html&quot;&gt;TIY – Collect remote performance counters using C#&lt;/a&gt; which sounded &lt;em&gt;exactly&lt;/em&gt; like what I wanted. Unfortunately, I was getting an error with the lines&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;IntPtr userHandle = new IntPtr(0);
LogonUser(
  &quot;UserA&quot;,
  &quot;DomainA&quot;,
  &quot;PasswordA&quot;,
  LOGON32_LOGON_INTERACTIVE,
  LOGON32_PROVIDER_DEFAULT,
  ref userHandle
);
WindowsIdentity identity = new WindowsIdentity(userHandle);
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;Token can not be zero&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This essentially meant that LogonUser had failed and so the &quot;userHandle&quot; reference had not been set (and left as a zero pointer). The code should really have checked the LogonUser return value -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var logonSuccess = LogonUser(
  &quot;UserA&quot;,
  &quot;DomainA&quot;,
  &quot;PasswordA&quot;,
  LOGON32_LOGON_INTERACTIVE,
  LOGON32_PROVIDER_DEFAULT,
  ref userHandle
);
if (!logonSuccess)
  throw new Exception(&quot;LogonUser failed&quot;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. but that wouldn&#39;t actually fix the failure.&lt;/p&gt;

&lt;p&gt;The underlying problem was explained by another article &lt;a href=&quot;https://platinumdogs.me/2008/10/30/net-c-impersonation-with-network-credentials/&quot;&gt;.NET (C#) Impersonation with Network Credentials&lt;/a&gt; that explains that&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;If you require the impersonated logon to have network credentials, you must select LOGON32_LOGON_NEW_CREDENTIALS as your logon type, which requires that you select LOGON32_PROVIDER_WINNT50 as the logon provider type&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Once I got the proof-of-concept working from these two articles, I fleshed things out into the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;using System;
using System.Collections.Generic;
using System.Diagnostics;
using System.Linq;
using System.Runtime.InteropServices;
using System.Security.Principal;

namespace PerformanceCounterCapture
{
  public sealed class PerformanceCounterRetriever : IDisposable
  {
    private const int LOGON32_LOGON_NEW_CREDENTIALS = 9;
    private const int LOGON32_PROVIDER_WINNT50 = 3;

    [DllImport(&quot;advapi32.dll&quot;, CharSet = CharSet.Auto)]
    private static extern bool LogonUser(
      string lpszUserName,
      string lpszDomain,
      string lpszPassword,
      int dwLogonType,
      int dwLogonProvider,
      ref IntPtr phToken);

    private WindowsIdentity _identity;
    private WindowsImpersonationContext _context;
    private bool _disposed;
    private readonly string _server;
    public PerformanceCounterRetriever(string server, string domain, string user, string password)
    {
      if (string.IsNullOrWhiteSpace(server))
        throw new ArgumentException($&quot;Null/blank {nameof(server)} specified&quot;);
      if (string.IsNullOrWhiteSpace(domain))
        throw new ArgumentException($&quot;Null/blank {nameof(domain)} specified&quot;);
      if (string.IsNullOrWhiteSpace(user))
        throw new ArgumentException($&quot;Null/blank {nameof(user)} specified&quot;);
      if (password == null)
        throw new ArgumentNullException(nameof(password));

      try
      {
        var userHandle = new IntPtr(0);
        var logonSuccess = LogonUser(
          user,
          domain,
          password,
          LOGON32_LOGON_NEW_CREDENTIALS,
          LOGON32_PROVIDER_WINNT50,
          ref userHandle
        );
        if (!logonSuccess)
          throw new Exception(&quot;LogonUser failed&quot;);
        _identity = new WindowsIdentity(userHandle);
        _context = _identity.Impersonate();
        _server = server;
        _disposed = false;
      }
      finally
      {
        Dispose();
      }
    }
    ~PerformanceCounterRetriever()
    {
      Dispose(false);
    }

    public IEnumerable&amp;lt;float&amp;gt; Get(
      string categoryName,
      string counterName,
      string optionalInstanceName = null)
    {
      if (string.IsNullOrWhiteSpace(categoryName))
        throw new ArgumentException($&quot;Null/blank {nameof(categoryName)} specified&quot;);
      if (string.IsNullOrWhiteSpace(counterName))
        throw new ArgumentException($&quot;Null/blank {nameof(counterName)} specified&quot;);

      var counters = new List&amp;lt;PerformanceCounter&amp;gt;();
      var category = new PerformanceCounterCategory(categoryName, _server);
      foreach (var instanceName in category.GetInstanceNames())
      {
        if ((optionalInstanceName == null) || (instanceName == optionalInstanceName))
          counters.Add(new PerformanceCounter(categoryName, counterName, instanceName, _server));
      }
      if (!counters.Any())
        yield break;

      while (true)
      {
        foreach (var c in counters)
          yield return c.NextValue();
      }
    }

    public void Dispose()
    {
      Dispose(true);
      GC.SuppressFinalize(this);
    }

    private void Dispose(bool disposing)
    {
      if (_disposed)
        return;

      if (_identity != null)
      {
        _identity.Dispose();
        _identity = null;
      }

      if (_context != null)
      {
        _context.Undo();
        _context.Dispose();
        _context = null;
      }

      _disposed = true;
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This class may be used in the following way:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;using (var counterRetriever = new PerformanceCounterRetriever(&quot;TestBox&quot;, &quot;Home&quot;, &quot;Dan&quot;, &quot;password&quot;))
{
  foreach (var value in counterRetriever.Get(&quot;Process&quot;, &quot;% Processor Time&quot;, &quot;TestService&quot;))
  {
    Console.WriteLine(
      &quot;[{0}] TestService: % Processor Time = {1}&quot;,
      DateTime.Now.ToString(&quot;HH:mm:ss.fff&quot;),
      value
    );
    Thread.Sleep(1000);
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &quot;counterRetriever.Get&quot; call returns an &lt;strong&gt;IEnumerable&amp;lt;float&amp;gt;&lt;/strong&gt; which retrieves a new value every time that a new value is requested from the enumerable reference. The code above (very roughly) imitates PerfMon in that it reads a new &quot;% Processor Time&quot; value every second.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(Note: The code above never terminates since nothing breaks it out of the loop, which is not useful in many scenarios.. but I&#39;ll talk about dealing with that shortly)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This is a good first step. However, when I&#39;m analysing the results of my test runs, I want to know more than just how much processor time is being used by the service.&lt;/p&gt;

&lt;h3&gt;Querying multiple performance counters remotely&lt;/h3&gt;

&lt;p&gt;If I want to collect the data from multiple performance counters then I need to get multiple &lt;strong&gt;IEnumerable&amp;lt;float&amp;gt;&lt;/strong&gt; instances from multiple &quot;counterRetriever.Get&quot; calls and then retrieve a value from each before pausing and repeating.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;using (var counterRetriever = new PerformanceCounterRetriever(&quot;TestBox&quot;, &quot;Home&quot;, &quot;Dan&quot;, &quot;password&quot;))
{
  var processorTime = counterRetriever
    .Get(&quot;Process&quot;, &quot;% Processor Time&quot;, &quot;TestService&quot;)
    .GetEnumerator();
  var percentageTimeInGC = counterRetriever
    .Get(&quot;.NET CLR Memory&quot;, &quot;% Time in GC&quot;, &quot;TestService&quot;)
    .GetEnumerator();
  while (true)
  {
    processorTime.MoveNext();
    Console.WriteLine(
      &quot;[{0}] TestService: % Processor Time = {1}&quot;,
      DateTime.Now.ToString(&quot;HH:mm:ss.fff&quot;),
      processorTime.Current
    );
    percentageTimeInGC.MoveNext();
    Console.WriteLine(
      &quot;[{0}] TestService: % Time in GC = {1}&quot;,
      DateTime.Now.ToString(&quot;HH:mm:ss.fff&quot;),
      percentageTimeInGC.Current
    );
    Thread.Sleep(1000);
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This could be extended to do the job (in my case, there are seven counters that I&#39;m interested in so the above could be chopped and changed to record them all) but the code will get a bit verbose and &quot;noisy&quot; quite quickly.&lt;/p&gt;

&lt;h3&gt;Troublesome counters&lt;/h3&gt;

&lt;p&gt;There is also a problem with writing code like the above and presuming that you can track all performance counters in the same way. For example, I also want to track the number of garbage collections that have occurred at generations 0, 1 and 2 since the service was restarted. It probably doesn&#39;t make much sense to record the values of these every second; I don&#39;t really care if there had been a single gen 0 garbage collection after 1s and after 2s and after 3s and after 4s, I&#39;d much rather see that at 1s there had been a single gen 0 collection and then, at 4s, there had been a second. I want to know when these kinds of values change and I&#39;m not interested in the repeated values between the changes.&lt;/p&gt;

&lt;p&gt;As another example, I&#39;m also interested in capturing information about the rate at which bytes are allocated by the service, for which I can consult the &quot;Allocated Bytes/sec&quot; counter. However, this counter is only updated after a GC event and will report zero the result of the time. This doesn&#39;t mean that zero bytes per second really were being allocated each time that the counter reports zero, it&#39;s just that there is nothing that can accurately report a value for this counter &lt;em&gt;except&lt;/em&gt; immediately following a collection. For this counter, it&#39;s probably best for me to exclude zero values - particularly while a performance test is underway, since it is basically impossible that the service will ever be allocating &lt;em&gt;zero&lt;/em&gt; bytes per second while it&#39;s deserialising requests and processing them. As with the &quot;number of collections at gen {x}&quot; counters, it will be worth ignoring some of the counter values but it will be important to know &lt;em&gt;when&lt;/em&gt; the values that we do pay attention to were recorded (since, for the &quot;Allocated Bytes/sec&quot; counter, it should be possible to use this information to approximate the allocation rate at any given time).&lt;/p&gt;

&lt;h3&gt;A complete solution&lt;/h3&gt;

&lt;p&gt;To try to address all of these problems, I&#39;ve come up with the following. It&#39;s not the smallest code sample in the world but it should be easy to follow and understand if you need to extend it for your own purposes -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;using System;
using System.Collections.Generic;
using System.Linq;
using System.Threading;

namespace PerformanceCounterCapture
{
  public static class PerformanceCounterLogger
  {
    /// &amp;lt;summary&amp;gt;
    /// This will capture useful performance counter data until the specified cancellation token
    /// is set, at which point it will return the data (as such, it makes sense to call this from
    /// a background thread)
    /// &amp;lt;/summary&amp;gt;
    public static Results Log(
      string server,
      string domain,
      string user,
      string password,
      string serviceName,
      CancellationToken cancellationToken,
      TimeSpan timeBetweenCaptures)
    {
      if (string.IsNullOrWhiteSpace(server))
        throw new ArgumentException($&quot;Null/blank {nameof(server)} specified&quot;);
      if (string.IsNullOrWhiteSpace(domain))
        throw new ArgumentException($&quot;Null/blank {nameof(domain)} specified&quot;);
      if (string.IsNullOrWhiteSpace(user))
        throw new ArgumentException($&quot;Null/blank {nameof(user)} specified&quot;);
      if (password == null)
        throw new ArgumentNullException(nameof(password));
      if (string.IsNullOrWhiteSpace(serviceName))
        throw new ArgumentException($&quot;Null/blank {nameof(serviceName)} specified&quot;);
      if (cancellationToken == null)
        throw new ArgumentNullException(nameof(cancellationToken));
      if (timeBetweenCaptures.Ticks &amp;lt; 0)
        throw new ArgumentOutOfRangeException($&quot;{timeBetweenCaptures} must be a non-negative duration&quot;);

      // These lists will be populated periodically (according to timeBetweenCaptures) and, when the
      // cancellation token is set, they will all be included in the returned data for analysis
      var processorTimes = new List&amp;lt;Tuple&amp;lt;DateTime, float&amp;gt;&amp;gt;();
      var percentageGarbageCollectorTimes = new List&amp;lt;Tuple&amp;lt;DateTime, float&amp;gt;&amp;gt;();
      var numberOfGen0Collections = new List&amp;lt;Tuple&amp;lt;DateTime, float&amp;gt;&amp;gt;();
      var numberOfGen1Collections = new List&amp;lt;Tuple&amp;lt;DateTime, float&amp;gt;&amp;gt;();
      var numberOfGen2Collections = new List&amp;lt;Tuple&amp;lt;DateTime, float&amp;gt;&amp;gt;();
      var largeObjectHeapSize = new List&amp;lt;Tuple&amp;lt;DateTime, float&amp;gt;&amp;gt;();
      var allocatedBytesPerSeconds = new List&amp;lt;Tuple&amp;lt;DateTime, float&amp;gt;&amp;gt;();
      using (var performanceCounterRetriever = new PerformanceCounterRetriever(
                                                server, domain, user, password))
      {
        var performanceCountersToRecord = new[]
        {
          new PerformanceCounterDetails(
            &quot;Process&quot;,
            &quot;% Processor Time&quot;,
            serviceName,
            value =&amp;gt; processorTimes.Add(Tuple.Create(DateTime.Now, value))
          ),
          new PerformanceCounterDetails(
            &quot;.NET CLR Memory&quot;,
            &quot;% Time in GC&quot;,
            serviceName,
            value =&amp;gt; percentageGarbageCollectorTimes.Add(Tuple.Create(DateTime.Now, value))
          ),
          new PerformanceCounterDetails(
            &quot;.NET CLR Memory&quot;,
            &quot;# Gen 0 Collections&quot;,
            serviceName,
            value =&amp;gt; AddValueToListIfNew(numberOfGen0Collections, value)
          ),
          new PerformanceCounterDetails(
            &quot;.NET CLR Memory&quot;,
            &quot;# Gen 1 Collections&quot;,
            serviceName,
            value =&amp;gt; AddValueToListIfNew(numberOfGen1Collections, value)
          ),
          new PerformanceCounterDetails(
            &quot;.NET CLR Memory&quot;,
            &quot;# Gen 2 Collections&quot;,
            serviceName,
            value =&amp;gt; AddValueToListIfNew(numberOfGen2Collections, value)
          ),
          new PerformanceCounterDetails(
            &quot;.NET CLR Memory&quot;,
            &quot;Large Object Heap size&quot;,
            serviceName,
            value =&amp;gt; AddValueToListIfNew(largeObjectHeapSize, value)
          ),
          new PerformanceCounterDetails(
            &quot;.NET CLR Memory&quot;,
            &quot;Allocated Bytes/sec&quot;,
            serviceName,
            value =&amp;gt;
            {
              // This is only set after a GC event so there are lots of spurious zeroes that we
              // want to ignore (this value-ignoring is the main reason that the date that the
              // value was recorded is included in the result data, so that it&#39;s possible to
              // approximate values during the missing periods - which may be of variable
              // duration since the useful values recorded for this are related to GC events)
              if (value == 0)
                return;
              allocatedBytesPerSeconds.Add(Tuple.Create(DateTime.Now, value));
            }
          )
        };

        var allCounterEnumerators = performanceCountersToRecord
          .Select(counterDetails =&amp;gt; new {
            Feed =
              performanceCounterRetriever.Get(
                counterDetails.CategoryName,
                counterDetails.CounterName,
                counterDetails.OptionalInstanceName
              )
              .GetEnumerator(),
            ValueLogger = counterDetails.ValueLogger
          })
          .ToArray(); // Don&#39;t call GetFeed every time that we enumerate the set

        // Keep looping and populating the lists until the cancellation token is set - at that
        // point, return a result object that contains all of the data
        while (!cancellationToken.IsCancellationRequested)
        {
          foreach (var counterEnumerator in allCounterEnumerators)
          {
            counterEnumerator.Feed.MoveNext();
            var value = counterEnumerator.Feed.Current;
            counterEnumerator.ValueLogger(value);
          }
          if (!cancellationToken.IsCancellationRequested)
            Thread.Sleep(timeBetweenCaptures);
        }
        return new Results(
          processorTimes,
          percentageGarbageCollectorTimes,
          numberOfGen0Collections,
          numberOfGen1Collections,
          numberOfGen2Collections,
          largeObjectHeapSize,
          allocatedBytesPerSeconds
        );
      }
    }

    private static void AddValueToListIfNew(List&amp;lt;Tuple&amp;lt;DateTime, float&amp;gt;&amp;gt; values, float value)
    {
      if (values == null)
        throw new ArgumentNullException(nameof(value));

      if (!values.Any() || (values.Last().Item2 != value))
        values.Add(Tuple.Create(DateTime.Now, value));
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It also needs the following two classes for its internal initialisation and for returning results -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public sealed class PerformanceCounterDetails
{
  public PerformanceCounterDetails(
    string categoryName,
    string counterName,
    string optionalInstanceName,
    Action&amp;lt;float&amp;gt; valueLogger)
  {
    if (string.IsNullOrWhiteSpace(categoryName))
      throw new ArgumentException($&quot;Null/blank {nameof(categoryName)} specified&quot;);
    if (string.IsNullOrWhiteSpace(counterName))
      throw new ArgumentException($&quot;Null/blank {nameof(counterName)} specified&quot;);
    if (valueLogger == null)
      throw new ArgumentNullException(nameof(valueLogger));

    CategoryName = categoryName;
    CounterName = counterName;
    OptionalInstanceName = optionalInstanceName;
    ValueLogger = valueLogger;
  }

  public string CategoryName { get; }
  public string CounterName { get; }
  public string OptionalInstanceName { get; }
  public Action&amp;lt;float&amp;gt; ValueLogger { get; }
}

public sealed class Results
{
  public Results(
    IEnumerable&amp;lt;Tuple&amp;lt;DateTime, float&amp;gt;&amp;gt; processorTimes,
    IEnumerable&amp;lt;Tuple&amp;lt;DateTime, float&amp;gt;&amp;gt; percentageGarbageCollectorTimes,
    IEnumerable&amp;lt;Tuple&amp;lt;DateTime, float&amp;gt;&amp;gt; numberOfGen0Collections,
    IEnumerable&amp;lt;Tuple&amp;lt;DateTime, float&amp;gt;&amp;gt; numberOfGen1Collections,
    IEnumerable&amp;lt;Tuple&amp;lt;DateTime, float&amp;gt;&amp;gt; numberOfGen2Collections,
    IEnumerable&amp;lt;Tuple&amp;lt;DateTime, float&amp;gt;&amp;gt; largeObjectHeapSize,
    IEnumerable&amp;lt;Tuple&amp;lt;DateTime, float&amp;gt;&amp;gt; allocatedBytesPerSeconds)
  {
    if (processorTimes == null)
      throw new ArgumentNullException(nameof(processorTimes));
    if (percentageGarbageCollectorTimes == null)
      throw new ArgumentNullException(nameof(percentageGarbageCollectorTimes));
    if (numberOfGen0Collections == null)
      throw new ArgumentNullException(nameof(numberOfGen0Collections));
    if (numberOfGen1Collections == null)
      throw new ArgumentNullException(nameof(numberOfGen1Collections));
    if (numberOfGen2Collections == null)
      throw new ArgumentNullException(nameof(numberOfGen2Collections));
    if (largeObjectHeapSize == null)
      throw new ArgumentNullException(nameof(largeObjectHeapSize));
    if (allocatedBytesPerSeconds == null)
      throw new ArgumentNullException(nameof(allocatedBytesPerSeconds));

    ProcessorTimes = processorTimes;
    PercentageGarbageCollectorTimes = percentageGarbageCollectorTimes;
    NumberOfGen0Collections = numberOfGen0Collections;
    NumberOfGen1Collections = numberOfGen1Collections;
    NumberOfGen2Collections = numberOfGen2Collections;
    LargeObjectHeapSize = largeObjectHeapSize;
    AllocatedBytesPerSeconds = allocatedBytesPerSeconds;
  }

  public IEnumerable&amp;lt;Tuple&amp;lt;DateTime, float&amp;gt;&amp;gt; ProcessorTimes { get; }
  public IEnumerable&amp;lt;Tuple&amp;lt;DateTime, float&amp;gt;&amp;gt; PercentageGarbageCollectorTimes { get; }
  public IEnumerable&amp;lt;Tuple&amp;lt;DateTime, float&amp;gt;&amp;gt; NumberOfGen0Collections { get; }
  public IEnumerable&amp;lt;Tuple&amp;lt;DateTime, float&amp;gt;&amp;gt; NumberOfGen1Collections { get; }
  public IEnumerable&amp;lt;Tuple&amp;lt;DateTime, float&amp;gt;&amp;gt; NumberOfGen2Collections { get; }
  public IEnumerable&amp;lt;Tuple&amp;lt;DateTime, float&amp;gt;&amp;gt; LargeObjectHeapSize { get; }
  public IEnumerable&amp;lt;Tuple&amp;lt;DateTime, float&amp;gt;&amp;gt; AllocatedBytesPerSeconds { get; }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I don&#39;t think that there&#39;s a great deal that requires explaining in depth - the &lt;strong&gt;PerformanceCounterLogger&lt;/strong&gt; will periodically capture values from all of the counters that I&#39;m interested in, dealing with the special cases described earlier (garbage collection frequency, allocated bytes / sec, etc..) in a blocking manner. It continues to capture counter data until the cancellation token passed to it is set.&lt;/p&gt;

&lt;p&gt;That means that it makes sense to capture the performance counter data on a seperate thread. Something like the following (which is basically what I&#39;m using in my test runs) -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Restart the service on the test server
Restart(&quot;TestBox&quot;, &quot;TestService&quot;);

// Start recording performance counters on a separate thread
Results performanceCounterResults = null;
var cancellationTokenSource = new CancellationTokenSource();
var resultsReadyIndicator = new ManualResetEvent(initialState: false);
ThreadPool.QueueUserWorkItem(state =&amp;gt;
{
  performanceCounterResults = PerformanceCounterLogger.Log(
    &quot;TestBox&quot;,
    &quot;Home&quot;,
    &quot;Dan&quot;,
    &quot;password&quot;,
    &quot;TestService&quot;,
    cancellationTokenSource.Token,
    TimeSpan.FromSeconds(1)
  );
  resultsReadyIndicator.Set();
});

// TODO: Fire load at the server...........

// Tell the performance counters that it&#39;s time to stop capturing and wait for it to acknowledge
cancellationTokenSource.Cancel();
resultsReadyIndicator.WaitOne();

// TODO: Write the &quot;performanceCounterResults&quot; data away to analyse later...........
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are two big TODOs in the above code - somehow the request payload needs to be fired at the remote server that is being measured and somehow the performance counter results need to be compared between one run and the next. Obviously, these will vary from one project to the next and so they will be very dependent upon what kind of service that you&#39;re testing (and what performance counters you&#39;re interested in). In my case, I already had a decent system available for replaying old requests so that changes to the system could be examined, all I needed on top of that was a way to capture some performance counters to bring some &lt;em&gt;cold hard numbers&lt;/em&gt; into proceedings - if you&#39;re in a similar position then hopefully this post will be helpful!&lt;/p&gt;

&lt;h3&gt;Shameless plug&lt;/h3&gt;

&lt;p&gt;Although I had a system in place to replay historical requests in order to simulate load, there was a slight problem with this in that the service would read from a database and it was totally feasible that the data persisted there could vary from hour to hour (if not more frequently). This could mean that one performance run would not be directly comparable to the next - one run may return more or less results for a particular query, for example, or have to process some of those results in a different (ie. more or less expensive) manner.&lt;/p&gt;

&lt;p&gt;This would make meaningful comparisons difficult - really, each run should return precisely the same data as the next.&lt;/p&gt;

&lt;p&gt;For this particular service, a few things were in my favour on this front; the service was read only, its job is only to deliver data for rendering on various web sites and it does not have to perform any write operations. It also only specifies a database connection in a fairly limited number of places. This allowed me to add a config option to the service that would (when in a particular test mode) create database connections that get their data from a proxy service instead of going directly to the SQL database.&lt;/p&gt;

&lt;p&gt;The proxy service can be run in either &quot;record&quot; or &quot;replay&quot; mode. First, the service that is under test should have the batch of requests that the processing performance is being measured for replayed while the database proxy service is in &quot;record&quot; mode - this allows the proxy service to populate a cache on disk that contains all of the result sets for all of the database queries performed. After this, all subsequent performance runs are made with the proxy service in &quot;replay&quot; mode - in this configuration, the service will never hit the database and will always return data from its cache. This ensures that the data retrieved during each performance run is consistent, which makes it much easier to reach useful conclusions and make meaningful comparisons.&lt;/p&gt;

&lt;p&gt;The library that I wrote for this database proxy service is called &lt;a href=&quot;https://github.com/ProductiveRage/SqlProxyAndReplay&quot;&gt;SqlProxyAndReplay&lt;/a&gt; and is available on GitHub and via NuGet (the client needs &lt;a href=&quot;https://www.nuget.org/packages/ProductiveRage.SqlProxyAndReplay.Client&quot;&gt;ProductiveRage.SqlProxyAndReplay.Client&lt;/a&gt; and the server needs &lt;a href=&quot;https://www.nuget.org/packages/ProductiveRage.SqlProxyAndReplay.Service&quot;&gt;ProductiveRage.SqlProxyAndReplay.Service&lt;/a&gt; and &lt;a href=&quot;https://www.nuget.org/packages/ProductiveRage.SqlProxyAndReplay.Service.Example&quot;&gt;ProductiveRage.SqlProxyAndReplay.Service.Example&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;There are some caveats - under the hood, this uses a WCF (binary endpoint) service and it won&#39;t be as fast as hitting a database directly. And, as a .net library, there will be some garbage collection overhead since it will result in additional allocations. However, for testing how the &lt;em&gt;internals&lt;/em&gt; of a service (and not, say, tweaking individual SQL queries to try to eke out more performance) then this shouldn&#39;t be a huge problem since the overhead should be consistent from one run to the next. So long as you are measuring changes in performance runs &lt;em&gt;before&lt;/em&gt; you deploy an update and performance runs &lt;em&gt;after&lt;/em&gt; an update (hopefully improvements!) then the overhead of the database proxy shouldn&#39;t matter.&lt;/p&gt;

&lt;p&gt;Sometimes, of course, the database &lt;em&gt;is&lt;/em&gt; your bottle neck and so you want to capture real queries as they hit it so that you can performance tune them. There are already lot of good tools for this (you can get a long way by attaching SQL Profiler and looking for the most expensive or most frequent quite-expensive queries) but I hadn&#39;t found something useful for my use case, where I wanted to optimise what happened &lt;em&gt;after&lt;/em&gt; any database access and just wanted the database access layer to magically return consistent data time after time. At least, I couldn&#39;t find one that didn&#39;t entail significant work in writing some sort of mock / cached database access layer.&lt;/p&gt;

&lt;p&gt;While the &lt;a href=&quot;https://github.com/ProductiveRage/SqlProxyAndReplay&quot;&gt;SqlProxyAndReplay&lt;/a&gt; service / library may not be as useful if you have to test a service or application that needs to persist changes made to the backing store, I imagine that it&#39;s common for a lot of large scale applications to want to cache and optimise read operations and so this may well be useful for other people. The &lt;a href=&quot;https://github.com/ProductiveRage/SqlProxyAndReplay&quot;&gt;linked GitHub&lt;/a&gt; repo has more information in its README and there&#39;s a &quot;Tester&quot; console application to demonstrate it in action.&lt;/p&gt;</description>
			<pubDate>Wed, 10 Aug 2016 21:01:00 GMT</pubDate>
		</item>
		<item>
			<title>Why is saving Performance Monitor (PerfMon) settings so difficult these days?!</title>
            <link>http://www.productiverage.com/why-is-saving-performance-monitor-perfmon-settings-so-difficult-these-days</link>
			<guid>http://www.productiverage.com/why-is-saving-performance-monitor-perfmon-settings-so-difficult-these-days</guid>
			<description>&lt;p&gt;I&#39;ve been measuring and optimising a busy service recently at work and PerfMon is an invaluable tool in doing things like this - the service records its own performance counters about requests/second, cache-hits-and-misses/second and many other useful metrics, while Windows and .net also report on many helpful statistics such as CPU time per process, memory usage, bytes-allocated/second and frequency of garbage collections.&lt;/p&gt;

&lt;p&gt;Performance Monitor makes it really easy to add a set of counters and format their lines so that some are bold and thick (and, so, clear at a glance) while other may be made less obtrusive, so as not to confuse the graph too much.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.productiverage.com/Content/Images/Posts/PerfMon.png&quot; alt=&quot;Performance Monitor&quot;&gt;&lt;/p&gt;

&lt;p&gt;However, over the years the interface to this tool has had some changes made to it that I&#39;m not convinced are improvements. Back on Windows Server 2003, I&#39;m pretty sure that you could configure your view how you wanted it and then simply use File/Save to write an &quot;.mmc&quot; configuration file. Whenever you wanted, you could double-click that file and all of the counters would be there, configured just as you left them, quietly capturing data and displaying it how you want it. Unfortunately, that day has gone and it&#39;s not quite so easy.&lt;/p&gt;

&lt;p&gt;Never mind, I move with the times.&lt;/p&gt;

&lt;p&gt;There are a few options available to do the same sort of thing today. The first, and most obvious, is to right-click on the graph and choose &quot;Save Settings As&quot;. This saves a web page version of the current view that uses an ActiveX plugin (and so requires IE to display it and requires you to &quot;Allow blocked content&quot;). With this plugin you can do much of what you can in PerfMon - add or remove counters, highlight the currently-selected counter, change the formatting of the current counter, etc.. This option isn&#39;t terrible but it doesn&#39;t feel quite as solid as &lt;em&gt;real&lt;/em&gt; PerfMon.&lt;/p&gt;

&lt;p&gt;The second option sounds like a pretty reasonable idea; you can copy the current configuration to the clipboard, save it and then paste it back into a fresh PerfMon instance in the future (the content saved to the clipboard is basically the same content as is written away when you use &quot;Save Settings As&quot; to create the web page version). My biggest problem with this is that it doesn&#39;t work! I&#39;ve tried on several machines now (Windows Server 2012 and Windows 8.1) and I can successfully copy the content (I can verify this by pasting it into notepad) but when I click on the paste icon in PerfMon nothing happens. No error, no nothing. Maybe I&#39;m doing something stupid here, but I don&#39;t know what.&lt;/p&gt;

&lt;p&gt;There is a third option, I think, involving the &quot;Data Collector Sets&quot; section of the PerfMon tree view. However, I tried to remember what it was earlier today by playing around with the interface and I didn&#39;t get anywhere quickly.&lt;/p&gt;

&lt;p&gt;I use a fourth option these days, which is to start PerfMon using &quot;perfmon /sys&quot; (this works from the command line or from [Win]-[R]). This starts PerfMon in a kind of streamlined interface (the treeview down the left hand side of the application is notable by its absence, for example). But the really good bit about this mode is that the File menu now has two options - &quot;Save Settings As&quot; and &quot;LoadSettings&quot;. These work with &quot;.PerfMonCfg&quot; files and essentially make simple what I used to do in the old days; configure everything just so, save to the desktop for another day, open from the desktop on that day in the future and find everything just how I want it.&lt;/p&gt;

&lt;p&gt;Success!&lt;/p&gt;

&lt;h3&gt;Another little tweak&lt;/h3&gt;

&lt;p&gt;There is one thing that still annoys me, though. There doesn&#39;t seem to be any way to manually control the split between how much space is dedicated to the lower part of the display (that contains the names of the counters) and the upper half (the graph). If you add more than a couple of counters then the interface forces a vertical scroll bar onto the lower section - if you could manually make that section taller then the scroll bar would not be necessary.. but, alas, it appears that you can not.&lt;/p&gt;

&lt;p&gt;There is one trick to make it &lt;em&gt;slightly&lt;/em&gt; better, though. If the window is too narrow to show all of the information in that lower area then the horizontal scrollbar always appears on top of the last counter. If you can make the PerfMon window wide enough that you don&#39;t need the horizontal scrollbar then you can get one more counter to fit into view before the vertical scrollbar forces its way into play. This seems to allow up to nine counters to be displayed in the lower area with no scrolling required - if you need ten or more, though, then it seems like vertical scrolling is unavoidable :(&lt;/p&gt;</description>
			<pubDate>Thu, 14 Jul 2016 19:36:00 GMT</pubDate>
		</item>
		<item>
			<title>Creating a C# (&quot;Roslyn&quot;) Analyser - For beginners by a beginner</title>
            <link>http://www.productiverage.com/creating-a-c-sharp-roslyn-analyser-for-beginners-by-a-beginner</link>
			<guid>http://www.productiverage.com/creating-a-c-sharp-roslyn-analyser-for-beginners-by-a-beginner</guid>
			<description>&lt;p&gt;I&#39;ve been meaning to try writing a post about creating analysers for a little while now - they&#39;re a technology that I think has huge promise for improving code quality and they&#39;re something that I&#39;ve successfully played around with recently.. but I&#39;m still very much in the early phases of being proficient and they&#39;re not something that I can just sit down and bang out easily (ie. not without a lot of googling).&lt;/p&gt;

&lt;p&gt;So this won&#39;t be the post of an expert - but I&#39;m hoping to use that to my advantage since I hopefully remember the pain points all too well and can go through the sort of things that I try when I&#39;m hashing these out.&lt;/p&gt;

&lt;p&gt;Most of the analysers I&#39;ve been writing have been for libraries that work with &lt;a href=&quot;http://bridge.net/&quot;&gt;Bridge.NET&lt;/a&gt;, which introduces some of its own complications. I&#39;m hoping to talk about those problems and how to overcome them in a later post - this one will be a more general introduction.&lt;/p&gt;

&lt;h3&gt;Creating a fresh Analyser project&lt;/h3&gt;

&lt;p&gt;The easiest way to get started is to use a Microsoft template. To do this, first you need to install the Visual Studio 2016 SDK and to do &lt;em&gt;this&lt;/em&gt; you go to File / New / Project and then choose C# in the left navigation pane, click on Extensibility and then select &quot;Install the Visual Studio Extensibility Tools&quot; (you may already have it installed, it&#39;s an optional component of VS2015 - if you see no link to &quot;Install the Visual Studio Extensibility Tools&quot; then hopefully that&#39;s why). Next, from the same Extensibility section, you need to select &quot;Download the .NET Compiler Platform SDK&quot;. This will ensure that you have the project template installed that we&#39;re going to use and it installs some other helpful tools, such as the Syntax Visualizer (which we&#39;ll see in a moment).&lt;/p&gt;

&lt;p&gt;Now that you have the template and since you&#39;re already in File / New / Project / C# / Extensibility, select &quot;Analyzer with Code Fix (NuGet + VSIX)&quot; to create an example analyser solution. This will be a fully operational analyser, split into three projects - the analyser itself, a unit test library and a &quot;Vsix&quot; project. This last one would be used if you wanted to create an analyser that would be installed and applied to &lt;em&gt;all&lt;/em&gt; projects that you would ever open and &lt;em&gt;not&lt;/em&gt; apply to any specific library. What I&#39;ll be talking about here will be creating an analyser to work with a particular library (that would be distributed &lt;em&gt;with&lt;/em&gt; the library) - so that everyone consuming the library can benefit from it. As such, to keep things simple, delete the &quot;Vsix&quot; project now,&lt;/p&gt;

&lt;p&gt;The example analyser that this template installs does something very simple - it looks for class names that are not upper case and it warns about them. In terms of functionality, this is not particularly useful.. but in terms of education and illustrating how to get started it&#39;s a good jumping off point. In fact, the project includes not just an analyser but also a &quot;code fix&quot; - once a non-all-upper-case class name is identified and warned about, a quick fix will be offered in the IDE to change the name to match the upper case regime that it&#39;s pushing. Code fixes can be really helpful but I&#39;ll talk about them another day, I think that there already will be plenty to deal with in this post.&lt;/p&gt;

&lt;p&gt;The analyser class looks basically like this (I&#39;ve removed comments and replaced localisable strings with hard-coded strings, just to make it a little less to absorb all at once) -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;using System.Collections.Immutable;
using System.Linq;
using Microsoft.CodeAnalysis;
using Microsoft.CodeAnalysis.Diagnostics;

namespace ExampleAnalyser
{
    [DiagnosticAnalyzer(LanguageNames.CSharp)]
    public class ExampleAnalyserAnalyzer : DiagnosticAnalyzer
    {
        public const string DiagnosticId = &quot;ExampleAnalyser&quot;;
        private const string Category = &quot;Naming&quot;;
        private static readonly LocalizableString Title
            = &quot;Type name contains lowercase letters&quot;;
        private static readonly LocalizableString MessageFormat
            = &quot;Type name &#39;{0}&#39; contains lowercase letters&quot;;
        private static readonly LocalizableString Description
            = &quot;Type names should be all uppercase.&quot;;

        private static DiagnosticDescriptor Rule = new DiagnosticDescriptor(
            DiagnosticId,
            Title,
            MessageFormat,
            Category,
            DiagnosticSeverity.Warning,
            isEnabledByDefault: true,
            description: Description
        );

        public override ImmutableArray&amp;lt;DiagnosticDescriptor&amp;gt; SupportedDiagnostics
        {
            get { return ImmutableArray.Create(Rule); }
        }

        public override void Initialize(AnalysisContext context)
        {
            context.RegisterSymbolAction(AnalyzeSymbol, SymbolKind.NamedType);
        }

        private static void AnalyzeSymbol(SymbolAnalysisContext context)
        {
            var namedTypeSymbol = (INamedTypeSymbol)context.Symbol;
            if (namedTypeSymbol.Name.ToCharArray().Any(char.IsLower))
            {
                context.ReportDiagnostic(Diagnostic.Create(
                    Rule,
                    namedTypeSymbol.Locations[0],
                    namedTypeSymbol.Name
                ));
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To summarise what&#39;s in the above code:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Every analyser needs at least one rule that it will declare&lt;/strong&gt;, where a rule has various properties such as a Diagnostic Id, Category, Title, MessageFormat, Description and Severity. The two that are most immediately interesting are Severity (make it a Warning to point out a potential mistake or make it an Error to indicate a critical problem that will prevent a build from being completed) and MessageFormat, since MessageFormat is responsible for the text that will be displayed to the user in their Error List. MessageFormat supports string replacement; in the above example, you can see that there is a &quot;{0}&quot; placeholder in the MessageFormat - when &quot;Diagnostic.Create&quot; is called, the argument &quot;namedTypeSymbol.Name&quot; is injected into that &quot;{0}&quot; placeholder.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Every analyser needs to declare a &quot;SupportedDiagnostics&quot; value that lists all of the types of rule&lt;/strong&gt; that it is possible for the analyser to raise. This is vital in order for the analyser to work correctly at runtime. (If you create an analyser that has three different types of rule that it can report but you forget to declare one of the types in the &quot;SupportedDiagnostics&quot; property, there is actually an analyser that is installed with the template that points out the mistake to you - which is a great example of how analysers can protect you at compile time from potential runtime problems!)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Every analyser needs an &quot;Initialize&quot; method that registers what type of symbol (more on what this actually means in a moment) it&#39;s interested in&lt;/strong&gt; and provides a reference to a method that will perform the actual analysis&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The simple task of the class above is to look at any &quot;named type&quot; (ie. classes and structs) and inspect their name to ensure that they consist entirely of capital letters (remember, this example included in the &quot;Analyzer with Code Fix (NuGet + VSIX)&quot; template is simply for educational purposes and &lt;em&gt;not&lt;/em&gt; because it&#39;s believed that all class names should be SHOUTING_FORMAT! :) Any class that doesn&#39;t have an all-caps name will result in a warning in the Error List.&lt;/p&gt;

&lt;p&gt;To illustrate how this should work, the test project includes the following test method -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[TestMethod]
public void TestMethod2()
{
    var test = @&quot;
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;
using System.Diagnostics;

namespace ConsoleApplication1
{
    class TypeName
    {   
    }
}&quot;;
    var expected = new DiagnosticResult
    {
        Id = &quot;ExampleAnalyser&quot;,
        Message = String.Format(&quot;Type name &#39;{0}&#39; contains lowercase letters&quot;, &quot;TypeName&quot;),
        Severity = DiagnosticSeverity.Warning,
        Locations = new[] {
            new DiagnosticResultLocation(&quot;Test0.cs&quot;, 11, 15)
        }
    };

    VerifyCSharpDiagnostic(test, expected);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This makes it clear to see precisely what sort of thing the analyser is looking for but it also gives us another immediate benefit - we can actually execute the analyser and step through it in the debugger if we want to have a poke around with exactly what is in the &lt;strong&gt;SymbolAnalysisContext&lt;/strong&gt; reference or if we want to look at the properties of a particular &lt;strong&gt;INamedTypeSymbol&lt;/strong&gt; instance. This is as easy as putting a breakpoint into the &quot;AnalyzeSymbol&quot; method in the example analyser and then going back into the test class, right-clicking within &quot;TestMethod2&quot; and selecting &quot;Debug Tests&quot;.&lt;/p&gt;

&lt;p&gt;I want to introduce one other useful technique before moving on - the use of the &quot;Syntax Visualizer&quot;. An analyser works on an in-memory tree of nodes that represent the source code of the file that you&#39;re looking at*. In the unit test above, the named symbol &quot;TypeName&quot; is a child node of the &quot;TypeName&quot; class declaration, which is a child node of the &quot;ConsoleApplication1&quot; namespace, which is a child of a top-level construct called the &quot;CompilationUnit&quot;. Understanding the various types of node will be key to writing analysers and the Syntax Visualizer makes this a little bit easier.&lt;/p&gt;

&lt;p&gt;* &lt;em&gt;(Although an analyser starts by examining source code in a particular file, it&#39;s also possible to look up types and values that are referenced in that code that live elsewhere - to find out what namespace a class that is referenced exists in, for example, or to determine what arguments a method that is called that exists in a different library. These lookups are more expensive than looking solely at the content in the current file, however, and so should only be done if strictly necessary. We will see how to do this shortly. When looking only at content parsed from the current file, we are looking at the &quot;syntax tree&quot;. When looking up references elsewhere in the solution we accessing the &quot;semantic model&quot;).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Having installed the &quot;.NET Compiler Platform SDK&quot; earlier, you will now have access to this tool - go to View / Other Windows / Syntax Visualizer. This shows the syntax tree for any code within your project. So, if you click on the name &quot;TestMethod2&quot; then you will see that it is an &lt;strong&gt;IdentifierToken&lt;/strong&gt; (which is the name &quot;TestMethod2&quot;) that is a child node of a &lt;strong&gt;MethodDeclaration&lt;/strong&gt; which is a child node of a &lt;strong&gt;ClassDeclaration&lt;/strong&gt; which is a child node of a &lt;strong&gt;NamespaceDeclaration&lt;/strong&gt;, which is a child node of a &lt;strong&gt;CompilationUnit&lt;/strong&gt;. You can click on any of these nodes in the Syntax Visualiser to inspect some of the properties of the node and you can open further branches to inspect more - for example, there is a &quot;Block&quot; node that will appear shortly after the &lt;strong&gt;IdentifierToken&lt;/strong&gt; that you may click to reveal the nodes that represent the statements within the method.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.productiverage.com/Content/Images/Posts/SyntaxVisualizer.png&quot; alt=&quot;The Syntax Visualizer&quot;&gt;&lt;/p&gt;

&lt;h3&gt;Writing a real analyser&lt;/h3&gt;

&lt;p&gt;I&#39;m going to walk through an analyser that I created recently - starting from scratch and, hopefully, encountering the same problems that I did last time so that I can illustrate how to find out how to solve them.&lt;/p&gt;

&lt;p&gt;The analyser is part of my &lt;a href=&quot;https://www.nuget.org/packages/Bridge.React&quot;&gt;Bridge.React&lt;/a&gt; library but you won&#39;t need to know anything about React or Bridge to follow along.&lt;/p&gt;

&lt;p&gt;The root of the problem relates to the rendering of html &quot;select&quot; elements. There are three related properties to consider when rendering a &quot;select&quot; element; &quot;Multiple&quot;, &quot;Value&quot; and &quot;Values&quot;. Multiple is a boolean that indicates whether the elements supports only single selections (false) or zero, one or more selections (true). If rendering an element with pre-selected items then the &quot;Value&quot; or &quot;Values&quot; properties must be used. &quot;Value&quot; is a string while &quot;Values&quot; is a string array. If &quot;Multiple&quot; is false and &quot;Values&quot; is set then React will display a warning at runtime and ignore the value, similarly if &quot;Multiple&quot; is true and &quot;Value&quot; is set.&lt;/p&gt;

&lt;p&gt;I wanted an analyser that handled these simple cases -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// This is fine
return DOM.Select(new SelectAttributes { Multiple = false, Value = &quot;x&quot; };

// This is fine
return DOM.Select(new SelectAttributes { Multiple = true, Values = new [] { &quot;x&quot;, &quot;y&quot; } };

// Wrong (shouldn&#39;t use &quot;Value&quot; when &quot;Multiple&quot; is true)
return DOM.Select(new SelectAttributes { Multiple = true, Value = &quot;x&quot; };

// Wrong (shouldn&#39;t use &quot;Values&quot; when &quot;Multiple&quot; is false)
return DOM.Select(new SelectAttributes { Multiple = false, Values = new [] { &quot;x&quot;, &quot;y&quot; } };

// Wrong (shouldn&#39;t use &quot;Values&quot; when &quot;Multiple&quot; defaults to false)
return DOM.Select(new SelectAttributes { Values = new [] { &quot;x&quot;, &quot;y&quot; } };
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It&#39;s worth mentioning that I&#39;m &lt;em&gt;only&lt;/em&gt; considering these simple cases so this analyser won&#39;t be &quot;perfect&quot;. If &quot;Multiple&quot; is set according to a variable then I&#39;m not going to try to follow all possible code paths to ensure that it is never true/false if Values/Value is set. I&#39;m also not going to cater for the technically-valid case where someone instantiates a &lt;strong&gt;SelectAttributes&lt;/strong&gt; and sets &quot;Values&quot; on it initially (but leaves &quot;Multiple&quot; as false) and then sets &quot;Multiple&quot; to true on a later line of code. While this would be valid (there would be no runtime warning), I think that it would be clearer to set &quot;Multiple&quot; &lt;em&gt;and&lt;/em&gt; &quot;Values&quot; together. In this case, I&#39;m imposing what I believe to be a best practice on the consumer of my library - some analysers do this, some don&#39;t.&lt;/p&gt;

&lt;p&gt;To keep things as simple as possible for now, instead of trying to pull in the real Bridge.React library, we&#39;ll just create another class library project in the solution to work against - call it &quot;Bridge.React&quot; and rename the &quot;Class1.cs&quot; file that is automatically created as part of a class library project to &quot;SelectAttributes.cs&quot;. Change its contents to the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;namespace Bridge.React
{
    public sealed class SelectAttributes
    {
        public bool Multiple { private get; set; }
        public string Value { private get; set; }
        public string[] Values { private get; set; }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will be enough to start writing the analyser.&lt;/p&gt;

&lt;p&gt;What I want to do is to take the example analyser from the &quot;Analyzer with Code Fix (NuGet + VSIX)&quot; and change it to ensure that &lt;strong&gt;SelectAttributes&lt;/strong&gt; properties are always configured according to the rule outlined above. Before getting started on that, though, it seems like a good time to formalise the rules by decribing them with unit tests. We get many bonuses here - writing individual tests may help guide us through fixing them up one at a time and so help us focus on individual problems that the analyser has to solve. It will also provide us with a way to exercise the analyser and step through it with the debugger (which I find invaluable when I&#39;m not very familiar with a library or object model - when I &lt;em&gt;do&lt;/em&gt; have a good grasp on code then stepping through a debugger can feel very time-consuming but it can be helpful in cases like this, as I&#39;ll demonstrate shortly). Finally, the tests will help avoid regressions creeping in if I decide to refactor the analyser or extend its functionality in the future.&lt;/p&gt;

&lt;p&gt;So, replace the contents of &quot;UnitTest.cs&quot; with the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;using Microsoft.CodeAnalysis;
using Microsoft.CodeAnalysis.Diagnostics;
using Microsoft.VisualStudio.TestTools.UnitTesting;
using TestHelper;

namespace ExampleAnalyser.Test
{
    [TestClass]
    public class UnitTest : DiagnosticVerifier
    {
        [TestMethod]
        public void DoNotUseValueWhenMultipleIsTrue()
        {
            var testContent = @&quot;
                using Bridge.React;

                namespace TestCase
                {
                    public class Example
                    {
                        public void Go()
                        {
                            new SelectAttributes { Multiple = true, Value = &quot;&quot;1&quot;&quot; };
                        }
                    }
                }&quot;;

            var expected = new DiagnosticResult
            {
                Id = ExampleAnalyserAnalyzer.DiagnosticId,
                Message = &quot;If &#39;Multiple&#39; is true then the &#39;Values&#39; property should be used instead of &#39;Value&#39;&quot;,
                Severity = DiagnosticSeverity.Warning,
                Locations = new[]
                {
                    new DiagnosticResultLocation(&quot;Test0.cs&quot;, 10, 29)
                }
            };

            VerifyCSharpDiagnostic(testContent, expected);
        }

        [TestMethod]
        public void DoNotUseValuesWhenMultipleIsFalse()
        {
            var testContent = @&quot;
                using Bridge.React;

                namespace TestCase
                {
                    public class Example
                    {
                        public void Go()
                        {
                            new SelectAttributes { Multiple = false, Values = new[] { &quot;&quot;1&quot;&quot; } };
                        }
                    }
                }&quot;;

            var expected = new DiagnosticResult
            {
                Id = ExampleAnalyserAnalyzer.DiagnosticId,
                Message = &quot;If &#39;Multiple&#39; is false then the &#39;Value&#39; property should be used instead of &#39;Values&#39;&quot;,
                Severity = DiagnosticSeverity.Warning,
                Locations = new[]
                {
                    new DiagnosticResultLocation(&quot;Test0.cs&quot;, 10, 29)
                }
            };

            VerifyCSharpDiagnostic(testContent, expected);
        }

        [TestMethod]
        public void DoNotUseValueWhenMultipleDefaultsToFalse()
        {
            var testContent = @&quot;
                using Bridge.React;

                namespace TestCase
                {
                    public class Example
                    {
                        public void Go()
                        {
                            var x = new SelectAttributes { Values = new[] { &quot;&quot;1&quot;&quot; } };
                            x.Multiple = True;
                        }
                    }
                }&quot;;

            var expected = new DiagnosticResult
            {
                Id = ExampleAnalyserAnalyzer.DiagnosticId,
                Message = &quot;If &#39;Multiple&#39; is false then the &#39;Value&#39; property should be used instead of &#39;Values&#39;&quot;,
                Severity = DiagnosticSeverity.Warning,
                Locations = new[]
                {
                    new DiagnosticResultLocation(&quot;Test0.cs&quot;, 10, 37)
                }
            };

            VerifyCSharpDiagnostic(testContent, expected);
        }

        protected override DiagnosticAnalyzer GetCSharpDiagnosticAnalyzer()
        {
            return new ExampleAnalyserAnalyzer();
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now there&#39;s one more important thing to do before actually writing the analyser. When those unit tests run, the &quot;.NET Compiler Platform&quot; (referred to as &quot;Roslyn&quot;) will parse and compile those code snippets in memory. This means that the code snippets need to actually be able to compile! Currently they won&#39;t because Roslyn won&#39;t know how to resolve the &quot;Bridge.React&quot; namespace that is referenced.&lt;/p&gt;

&lt;p&gt;This is quite easily fixed - the &lt;strong&gt;DiagnosticVerifier&lt;/strong&gt; class (which is part of the template that we started with) configures some environment options. That&#39;s why each test checks a file called &quot;Test0.cs&quot; - because Roslyn wants a filename to work with and that&#39;s what the &lt;strong&gt;DiagnosticVerifier&lt;/strong&gt; tells it to use. It also specifies what assemblies to include when building the project. So, if the code snippets referenced &quot;System&quot; or &quot;Sytem.Collections.Generic&quot; then those references will work fine. However, it doesn&#39;t initially know about the &quot;Bridge.React&quot; project, so we need to tell it to support it.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Add a reference to the &quot;Bridge.React&quot; project to the &quot;ExampleAnalayser.Test&quot; project&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Edit the file &quot;Helpers/DiagnosticVerifier.Helper.cs&quot; in the &quot;ExampleAnalayser.Test&quot; project and add the following near the top, where other &lt;strong&gt;MetadataReference&lt;/strong&gt; instances are created:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;private static readonly MetadataReference CSharpBridgeReactReference
    = MetadataReference.CreateFromFile(typeof(Bridge.React.SelectAttributes).Assembly.Location);
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Open all of the code regions in that file and add pass &quot;CSharpBridgeReactReference&quot; into the solution by adding an additional &quot;AddMetadataReference&quot; call. The &quot;CreateProject&quot; method should now look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;private static Project CreateProject(string[] sources, string language = LanguageNames.CSharp)
{
    string fileNamePrefix = DefaultFilePathPrefix;
    string fileExt = language == LanguageNames.CSharp
        ? CSharpDefaultFileExt
        : VisualBasicDefaultExt;
    var projectId = ProjectId.CreateNewId(debugName: TestProjectName);
    var solution = new AdhocWorkspace()
    .CurrentSolution
        .AddProject(projectId, TestProjectName, TestProjectName, language)
        .AddMetadataReference(projectId, CorlibReference)
        .AddMetadataReference(projectId, SystemCoreReference)
        .AddMetadataReference(projectId, CSharpSymbolsReference)
        .AddMetadataReference(projectId, CodeAnalysisReference)
        .AddMetadataReference(projectId, CSharpBridgeReactReference);
    int count = 0;
    foreach (var source in sources)
    {
        var newFileName = fileNamePrefix + count + &quot;.&quot; + fileExt;
        var documentId = DocumentId.CreateNewId(projectId, debugName: newFileName);
        solution = solution.AddDocument(documentId, newFileName, SourceText.From(source));
        count++;
    }
    return solution.GetProject(projectId);
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;&lt;em&gt;Really&lt;/em&gt; writing the analyser&lt;/h3&gt;

&lt;p&gt;Now that the groundwork is done and we&#39;ve decided what precisely needs doing (and documented it with tests), we need to write the actual code.&lt;/p&gt;

&lt;p&gt;Although I can use the debugger to inspect the syntax tree for the code snippets in the unit tests, at this point I think even that would be information overload. To begin with, just add the following line to one of the unit test methods - it doesn&#39;t matter which one because it will be deleted very shortly, it&#39;s just to have a bit of a poke around with the Syntax Visualizer:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var x = new Bridge.React.SelectAttributes { Multiple = true, Value = &quot;x&quot; };
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ensuring that the Syntax Visualizer is visible (View / Other Windows / Syntax Visualizer), clicking on &quot;Multiple&quot; shows the following:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.productiverage.com/Content/Images/Posts/ObjectInitializerExpression.png&quot; alt=&quot;ObjectInitializerExpression&quot;&gt;&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;IdentifierToken&lt;/strong&gt; is the &quot;Multiple&quot; property, which is part of a &lt;strong&gt;SimpleAssignment&lt;/strong&gt; (ie. &quot;Multiple = 1&quot;) which is a child of an &lt;strong&gt;ObjectInitializerExpression&lt;/strong&gt; (which is the curly brackets around the two properties being set) which is a child of an &lt;strong&gt;ObjectCreationExpression&lt;/strong&gt; (which is the entire statement that includes &quot;new Bridge.React.SelectAttributes&quot; &lt;em&gt;and&lt;/em&gt; the setting of the two properties) and that itself is part of a &lt;strong&gt;VariableDeclaration&lt;/strong&gt; (which sets &quot;x&quot; to be the result of the object creation). With the Syntax Visualizer, we could go all the way up to the top of the method and then to the class and then to the namespace and then to the top-level CompilationUnit. However, what we&#39;re most interested in is the &lt;strong&gt;ObjectInitializerExpression&lt;/strong&gt;, since that contains the properties that we want to verify.&lt;/p&gt;

&lt;p&gt;So, how do we alter the analyser class that we currently have in order to identify object initialisers such as this?&lt;/p&gt;

&lt;p&gt;Currently the example analyser class has an &quot;Initialize&quot; method that looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public override void Initialize(AnalysisContext context)
{
    context.RegisterSymbolAction(AnalyzeSymbol, SymbolKind.NamedType);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The first thing to try would be to see what other options are in the &quot;SymbolKind&quot; enum. However, this contains things like &quot;Alias&quot;, &quot;Event&quot;, &quot;Method&quot;, &quot;NamedType&quot;, &quot;Property&quot; which don&#39;t bear much resemblance to &lt;strong&gt;ObjectInitializerExpression&lt;/strong&gt;. Without any better plan, I recommend turning to Google. If &quot;SymbolKind&quot; doesn&#39;t seem to have what we want, maybe there&#39;s something else that we can extract from the &lt;strong&gt;AnalysisContext&lt;/strong&gt; instance that the &quot;Initialize&quot; method has.&lt;/p&gt;

&lt;p&gt;Googling for &lt;a href=&quot;https://www.google.co.uk/search?q=AnalysisContext+ObjectInitializerExpression&quot;&gt;&quot;AnalysisContext ObjectInitializerExpression&quot;&lt;/a&gt; doesn&#39;t actually return that many results. However, the second one &lt;a href=&quot;https://github.com/mjsabby/RoslynClrHeapAllocationAnalyzer/blob/master/ClrHeapAllocationsAnalyzer/ExplicitAllocationAnalyzer.cs&quot;&gt;RoslynClrHeapAllocationAnalyzer/ExplicitAllocationAnalyzer.cs&lt;/a&gt; has some code that looks promising:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public override void Initialize(AnalysisContext context)
{
    var kinds = new[]
    {
        SyntaxKind.ObjectCreationExpression,
        SyntaxKind.AnonymousObjectCreationExpression,
        SyntaxKind.ArrayInitializerExpression,
        SyntaxKind.CollectionInitializerExpression,
        SyntaxKind.ComplexElementInitializerExpression,
        SyntaxKind.ObjectInitializerExpression,
        SyntaxKind.ArrayCreationExpression,
        SyntaxKind.ImplicitArrayCreationExpression,
        SyntaxKind.LetClause
    };
    context.RegisterSyntaxNodeAction(AnalyzeNode, kinds);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Instead of calling &quot;RegisterSymbolAction&quot; and passing a &quot;SymbolKind&quot; value, we can call &quot;RegisterSyntaxNodeAction&quot; and pass it an array of &quot;SyntaxKind&quot; values - where &quot;SyntaxKind&quot; is an enum that has an &quot;ObjectInitializerExpression&quot; value.&lt;/p&gt;

&lt;p&gt;Actually, by starting to change the &quot;Initialize&quot; method to&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public override void Initialize(AnalysisContext context)
{
    context.RegisterSyntaxNodeAction(AnalyzeSymbol,
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. it becomes clear that the method actually takes a params array and so it will be perfectly happy for us to specify only a single &quot;SyntaxKind&quot; value. &quot;Initialize&quot; now becomes:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public override void Initialize(AnalysisContext context)
{
    context.RegisterSyntaxNodeAction(
        AnalyzeSymbol,
        SyntaxKind.ObjectInitializerExpression
    );
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But the analyser project doesn&#39;t compile now - it complains about the type of one of the arguments of the call to &quot;SymbolAnalysisContext&quot;. It definitely takes a &quot;SyntaxKind&quot; enum as its second argument so it must be the first that is wrong. Intellisense indicates that it wants the first argument to be of type &lt;strong&gt;Action&amp;lt;SymbolAnalysisContext&amp;gt;&lt;/strong&gt; but the &quot;AnalyzeSymbol&quot; method currently takes a &lt;strong&gt;SyntaxNodeAnalysisContext&lt;/strong&gt; (and so is an &lt;strong&gt;Action&amp;lt;SymbolAnalysisContext&amp;gt;&lt;/strong&gt;, rather than an &lt;strong&gt;Action&amp;lt;SyntaxNodeAnalysisContext&amp;gt;&lt;/strong&gt;).&lt;/p&gt;

&lt;p&gt;This is easily fixed by changing the argument of the &quot;AnalyzeSymbol&quot; method. Doing so, however, will mean that &lt;em&gt;it&lt;/em&gt; causes a compile error because the example code was expecting a &lt;strong&gt;SymbolAnalysisContext&lt;/strong&gt; and we want to give it a &lt;strong&gt;SyntaxNodeAnalysisContext&lt;/strong&gt;. No matter, that code doesn&#39;t do what we want anyway! So change the method argument, delete its body and - while we&#39;re making changes - rename it to something better than &quot;AnalyzeSymbol&quot;, such as &quot;LookForInvalidSelectAttributeProperties&quot; -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public override void Initialize(AnalysisContext context)
{
    context.RegisterSyntaxNodeAction(
        LookForInvalidSelectAttributeProperties,
        SyntaxKind.ObjectInitializerExpression
    );
}

private static void LookForInvalidSelectAttributeProperties(SyntaxNodeAnalysisContext context)
{
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now that the basic structure is there, we can start work on the new &quot;LookForInvalidSelectAttributeProperties&quot; implementation. The &quot;context&quot; reference that is passed in has a &quot;Node&quot; property and this will match the SyntaxKind value that we passed to &quot;RegisterSyntaxNodeAction&quot;. So &quot;context.Node&quot; will be a reference to a node that represents an &quot;object initialisation&quot;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Sanity check: The &lt;strong&gt;SyntaxNode&lt;/strong&gt; class (which is the base node class) has a &quot;Kind()&quot; method that will return the &quot;SyntaxKind&quot; enum value that applies to the current node - so calling &quot;Kind()&quot; on &quot;context.Node&quot; here will return the &quot;ObjectInitializerExpression&quot; option from the &quot;SyntaxKind&quot; enum.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Now that we have a reference to an object initialisation node, we can go one of two ways. We want to ensure that the type being initialised is the &lt;strong&gt;SelectAttributes&lt;/strong&gt; class from the &quot;Bridge.React&quot; assembly and we want to check whether any invalid property combinations are being specified. The first task will involve getting the type name and then doing a lookup in the rest of the solution to work out where that type name comes from (to ensure that it is actually the &quot;Bridge.React&quot; &lt;strong&gt;SelectAttributes&lt;/strong&gt; class and not another class that exists somewhere with the same name). The second task only requires us to look at what properties are set by code in the syntax tree that we already have. This means that the first task is more expensive to perform than the second task and so we should try to deal with &quot;step two&quot; first since we will be able to avoid &quot;step one&quot; altogether if no invalid property combinations appear.&lt;/p&gt;

&lt;p&gt;So, to look for invalid property combinations first.. The Syntax Visualizer (as seen in the last image) shows that each individual property-setting is represented by a &quot;SimpleAssignmentExpression&quot; and that each of these is a direct child of the object initialisation. The &lt;strong&gt;SyntaxNode&lt;/strong&gt; class has a ChildNodes() method that will return all of the children, which seems like a good place to start. So, we might be able to do something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// This doesn&#39;t work,SimpleAssignmentExpressionSyntax isn&#39;t a real class :(
var propertyInitialisers = context.Node.ChildNodes()
    .OfType&amp;lt;SimpleAssignmentExpressionSyntax&amp;gt;();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. however, &quot;SimpleAssignmentExpressionSyntax&quot; is not a real type. I tried starting to type  out &quot;Simple&quot; to see if intellisense would pick up what the correct name was - but that didn&#39;t get me anywhere.&lt;/p&gt;

&lt;p&gt;Next, I resorted to deleting those last few lines (since they don&#39;t compile) and to just putting a breakpoint at the top of &quot;LookForInvalidSelectAttributeProperties&quot;. I then used Debug Tests on &quot;DoNotUseValueWhenMultipleIsTrue&quot;. The breakpoint is hit.. but I can&#39;t see the child nodes with QuickWatch because &quot;ChildNodes()&quot; is a method, not a property, and QuickWatch only shows you property values (it doesn&#39;t offter to execute methods and show you what is returned). So I go to the Immediate Window (Debug / Windows / Immediate), type the following and hit [Enter] -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;context.Node.ChildNodes().First().GetType().Name
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This displays &quot;AssignmentExpressionSyntax&quot;.&lt;/p&gt;

&lt;p&gt;This clue is enough to stop the debugger and go back to trying to populate the &quot;LookForInvalidSelectAttributeProperties&quot;. It may now start with:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var propertyInitialisers = context.Node.ChildNodes()
    .OfType&amp;lt;AssignmentExpressionSyntax&amp;gt;();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Using Go To Definition on &lt;strong&gt;AssignmentExpressionSyntax&lt;/strong&gt; shows that it has a &quot;Left&quot; and a &quot;Right&quot; property. These are the expressions that come either side of the operator, which is always an Equals sign when considering object property initialisations.&lt;/p&gt;

&lt;p&gt;The Syntax Visualizer shows that each &quot;SimpleAssignmentExpression&quot; has an &quot;IdentifierName&quot; on the left, so we should be able to get the property name from that.&lt;/p&gt;

&lt;p&gt;To try to work out what type &quot;IdentifierName&quot; relates to, I start typing &quot;Identifier&quot; and intellisense suggests &lt;strong&gt;IdentifierNameSyntax&lt;/strong&gt; (if it hadn&#39;t suggested anything helpful then I would have resorted to using Debug Tests again and inspecting types in the debugger). Having a poke around the &lt;strong&gt;IdentifierNameSyntax&lt;/strong&gt; class, I see that it has a property &quot;Identifier&quot; and that has a string property &quot;ValueText&quot;. This looks like the name of the property being set. Things are coming together. The start of the &quot;LookForInvalidSelectAttributeProperties&quot; can now look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var propertyInitialisers = context.Node.ChildNodes()
    .OfType&amp;lt;AssignmentExpressionSyntax&amp;gt;()
    .Select(propertyInitialiser =&amp;gt; new
    {
        PropertyName = ((IdentifierNameSyntax)propertyInitialiser.Left).Identifier.ValueText,
        ValueExpression = propertyInitialiser.Right
    });
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It&#39;s worth noting that we don&#39;t have to worry about the &quot;Left&quot; property ever being anything other than a simple identifier because assignments in object initialisers are only ever allow to be simple assignments. For example, the following would not compile:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var x = new MyClass { Name.Value = &quot;Ted };
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. because attempting to set nested properties in object initialisers does not compile in C#. Because it&#39;s not valid C#, we don&#39;t have to worry about it being passed through the analyser.&lt;/p&gt;

&lt;p&gt;Maybe it&#39;s worth adding another unit test around this - to ensure that invalid C# can&#39;t result in a load of edge cases that we need to be concerned about:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[TestMethod]
public void IgnoreInvalidPropertySetting()
{
    var testContent = @&quot;
        using Bridge.React;

        namespace TestCase
        {
            public class Example
            {
                public void Go()
                {
                    new SelectAttributes { Nested.Multiple = true };
                }
            }
        }&quot;;

    VerifyCSharpDiagnostic(testContent);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;Note: Calling the &quot;VerifyCSharpDiagnostic&quot; with no &quot;expected&quot; value means that the test expects that the analyser will not report any violated rules.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Now we can really move things along. We&#39;re interested in property initialisers where &quot;Multiple&quot; is clearly true or false (meaning it is set specifically to true or false &lt;em&gt;or&lt;/em&gt; it&#39;s not specified at all, leaving it with its default value of false). So, again using the Syntax Visualizer to work out how to tell whether an expression means a &quot;true&quot; constant or a &quot;false&quot; constant, I&#39;ve come up with this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var propertyInitialisers = context.Node.ChildNodes()
    .OfType&amp;lt;AssignmentExpressionSyntax&amp;gt;()
    .Select(propertyInitialiser =&amp;gt; new
    {
        PropertyName = ((IdentifierNameSyntax)propertyInitialiser.Left).Identifier.ValueText,
        ValueExpression = propertyInitialiser.Right
    });

var multiplePropertyInitialiser = propertyInitialisers.FirstOrDefault(
    propertyInitialiser =&amp;gt; propertyInitialiser.PropertyName == &quot;Multiple&quot;
);
bool multiplePropertyValue;
if (multiplePropertyInitialiser == null)
    multiplePropertyValue = false; // Defaults to false if not explicitlt set
else
{
    var multiplePropertyValueKind = multiplePropertyInitialiser.ValueExpression.Kind();
    if (multiplePropertyValueKind == SyntaxKind.TrueLiteralExpression)
        multiplePropertyValue = true;
    else if (multiplePropertyValueKind == SyntaxKind.FalseLiteralExpression)
        multiplePropertyValue = false;  
    else
    {
        // Only looking for very simple cases - where explicitly set to true or to false or not set at
        // all (defaulting to false). If it&#39;s set according to a method return value or a variable then
        // give up (this is just intended to catch obvious mistakes, not to perform deep and complex
        // analysis)
        return;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The next thing to do is to look for a &quot;Value&quot; or &quot;Values&quot; property being specified that is not appropriate for the &quot;Multiple&quot; value that we&#39;ve found.&lt;/p&gt;

&lt;p&gt;From the above code, it should be fairly clear that the way to do this is the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var valuePropertyIsSpecified = propertyInitialisers.Any(
    propertyInitialiser =&amp;gt; propertyInitialiser.PropertyName == &quot;Value&quot;
);
var valuesPropertyIsSpecified = propertyInitialisers.Any(
    propertyInitialiser =&amp;gt; propertyInitialiser.PropertyName == &quot;Values&quot;
);
if (!valuePropertyIsSpecified &amp;amp;&amp;amp; !valuesPropertyIsSpecified)
    return;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The final step is to ensure that the object initialisation that we&#39;re looking at is indeed for a &lt;strong&gt;SelectAttributes&lt;/strong&gt; instance. This is the bit that requires a lookup into the &quot;SemanticModel&quot; and which is more expensive than just looking at the current syntax tree because it needs the project to compile and to then work out what references to external code there may be.&lt;/p&gt;

&lt;p&gt;Knowing that I&#39;m going to be dealing with the full semantic model, I&#39;ll start by looking through the methods available on &quot;context.SemanticModel&quot; to see what might help me. Using the intellisense / documentation, it doesn&#39;t take long to find a &quot;GetTypeInfo&quot; method that takes an &lt;strong&gt;ObjectCreationExpression&lt;/strong&gt; instance - this is ideal because we have an &lt;strong&gt;ObjectInitializerExpressionSyntax&lt;/strong&gt; and we know that an &lt;strong&gt;ObjectInitializerExpressionSyntax&lt;/strong&gt; is a child of an &lt;strong&gt;ObjectCreationExpressionSyntax&lt;/strong&gt;, so it&#39;s easy for us to get an &lt;strong&gt;ObjectCreationExpression&lt;/strong&gt; (it&#39;s just the parent of &lt;strong&gt;ObjectInitializerExpressionSyntax&lt;/strong&gt; that we have).&lt;/p&gt;

&lt;p&gt;&quot;GetTypeInfo&quot; returns a &lt;strong&gt;TypeInfo&lt;/strong&gt; instance which has two properties; &quot;Type&quot; and &quot;ConvertedType&quot;. &quot;ConvertedType&quot; is (taken from the xml summary documentation):&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The type of the expression after it has undergone an implicit conversion&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;which shouldn&#39;t apply here, so we&#39;ll just look at &quot;Type&quot;. Note, though, that the documentation for &quot;Type&quot; says that:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;For expressions that do not have a type, null is returned. If the type could not be determined due to an error, than an IErrorTypeSymbol is returned.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Since this is an object creation expression, there should &lt;em&gt;always&lt;/em&gt; be a type returned (the type of the object being instantiated) but we do need to be careful about the error response. Here, it&#39;s fine to stop processing if there&#39;s an error - it might mean that there is a &quot;new SelectAttributes&quot; statements in the code being analysed but no &quot;Using Bridge.React;&quot; at the top of the file. We&#39;ll ignore these error cases and plan to only analyse valid code.&lt;/p&gt;

&lt;p&gt;This is the code that needs adding to ensure that the properties that we&#39;re looking at are for a Bridge.React.SelectAttributes -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var objectCreation = (ObjectCreationExpressionSyntax)context.Node.Parent;
var objectCreationTypeInfo = context.SemanticModel.GetTypeInfo(objectCreation);
if ((objectCreationTypeInfo.Type is IErrorTypeSymbol)
|| (objectCreationTypeInfo.Type.ContainingAssembly.Identity.Name != &quot;Bridge.React&quot;)
|| (objectCreationTypeInfo.Type.Name != &quot;SelectAttributes&quot;))
    return;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Having written this code, it strikes me as a good idea to add another test - one that ensures that we don&#39;t raise false positives about &quot;Multiple&quot; and &quot;Value&quot; / &quot;Values&quot; in cases where it&#39;s a different &lt;strong&gt;SelectAttributes&lt;/strong&gt; class, that is declared somewhere other than in &quot;Bridge.React&quot;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/// &amp;lt;summary&amp;gt;
/// Don&#39;t analyse a SelectAttributes initialisation that is for a different SelectAttributes class
/// (only target the SelectAttributes class that is part of the Bridge.React library)
/// &amp;lt;/summary&amp;gt;
[TestMethod]
public void OnlyTargetBridgeReactSelectAttributes()
{
    var testContent = @&quot;
        namespace TestCase
        {
            public class Example
            {
                public void Go()
                {
                    new SelectAttributes { Multiple = true, Value = &quot;&quot;x&quot;&quot; };
                }
            }

            public class SelectAttributes
            {
                public bool Multiple { get; set; }
                public string Value { get; set; }
            }
        }&quot;;

    VerifyCSharpDiagnostic(testContent);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we have all of the required information to display a warning for invalid &quot;Multiple&quot; / &quot;Value&quot; / &quot;Values&quot; combinations. What we &lt;em&gt;don&#39;t&lt;/em&gt; have is appropriate message content to display - we&#39;ve only got the warning content from the example analyser in the project template.&lt;/p&gt;

&lt;p&gt;So delete all of the code at the top of the analyser - the const and static strings, the &quot;Rule&quot; reference and the &quot;SupportedDiagnostics&quot; property and replace them with this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public const string DiagnosticId = &quot;Bridge.React&quot;;
private static readonly LocalizableString Title
    = &quot;Be careful to use the appropriate &#39;Value&#39; or &#39;Values&#39; property for the &#39;Multiple&#39; setting&quot;;
private static readonly LocalizableString MultipleWithValueMessage
    = &quot;If &#39;Multiple&#39; is true then the &#39;Values&#39; property should be used instead of &#39;Value&#39;&quot;;
private static readonly LocalizableString NoMultipleWithValuesMessage
    = &quot;If &#39;Multiple&#39; is false then the &#39;Value&#39; property should be used instead of &#39;Values&#39;&quot;;
private const string Category = &quot;Configuration&quot;;

private static DiagnosticDescriptor MultipleWithValueRule = new DiagnosticDescriptor(
    DiagnosticId,
    Title,
    MultipleWithValueMessage,
    Category,
    DiagnosticSeverity.Warning,
    isEnabledByDefault: true
);
private static DiagnosticDescriptor NoMultipleWithValuesRule = new DiagnosticDescriptor(
    DiagnosticId,
    Title,
    NoMultipleWithValuesMessage,
    Category,
    DiagnosticSeverity.Warning,
    isEnabledByDefault: true
);

public override ImmutableArray&amp;lt;DiagnosticDescriptor&amp;gt; SupportedDiagnostics
{
    get { return ImmutableArray.Create(MultipleWithValueRule, NoMultipleWithValuesRule); }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The final step, then, is to report rules when they are broken. The following needs adding to the end of the &quot;LookForInvalidSelectAttributeProperties&quot; method in order to complete it:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;if ((multiplePropertyValue == true) &amp;amp;&amp;amp; valuePropertyIsSpecified)
{
    context.ReportDiagnostic(Diagnostic.Create(
        MultipleWithValueRule,
        context.Node.GetLocation()
    ));
}
else if ((multiplePropertyValue == false) &amp;amp;&amp;amp; valuesPropertyIsSpecified)
{
    context.ReportDiagnostic(Diagnostic.Create(
        NoMultipleWithValuesRule,
        context.Node.GetLocation()
    ));
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;Localisation support&lt;/h3&gt;

&lt;p&gt;There&#39;s just one final thing to do now, which is more of a good practice than an essential - that is to replace the hard-coded strings in the analyser class with resources (that may potentially be translated into different languages one day). The project template includes a &quot;Resources.resx&quot; file, which is where we should move these strings to. Edit that file in Visual Studio and delete the existing entries and then add the following Name and Value pairs:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Name:&lt;/strong&gt; SelectAttributesAnalyserTitle&lt;/p&gt;
  
  &lt;p&gt;&lt;strong&gt;Value:&lt;/strong&gt; Be careful to use the appropriate &#39;Value&#39; or &#39;Values&#39; property for the &#39;Multiple&#39; setting&lt;/p&gt;
  
  &lt;p&gt;&lt;strong&gt;Name:&lt;/strong&gt; SelectAttributesAnalyserMultipleWithValueMessage&lt;/p&gt;
  
  &lt;p&gt;&lt;strong&gt;Value:&lt;/strong&gt; If &#39;Multiple&#39; is true then the &#39;Values&#39; property should be used instead of &#39;Value&#39;&lt;/p&gt;
  
  &lt;p&gt;&lt;strong&gt;Name:&lt;/strong&gt; SelectAttributesAnalyserNoMultipleWithValuesTitle&lt;/p&gt;
  
  &lt;p&gt;&lt;strong&gt;Value:&lt;/strong&gt; If &#39;Multiple&#39; is false then the &#39;Value&#39; property should be used instead of &#39;Values&#39;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;To make accessing these resources a little easier, add the following method to the bottom of the analyser class:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;private static LocalizableString GetLocalizableString(string nameOfLocalizableResource)
{
    return new LocalizableResourceString(
        nameOfLocalizableResource,
        Resources.ResourceManager,
        typeof(Resources)
    );
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, replace the three hard-coded string property initialisers with the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    private static readonly LocalizableString Title = GetLocalizableString(
        nameof(Resources.SelectAttributesAnalyserTitle)
    );
    private static readonly LocalizableString MultipleWithValueTitle = GetLocalizableString(
        nameof(Resources.SelectAttributesAnalyserMultipleWithValueMessage)
    );
    private static readonly LocalizableString NoMultipleWithValuesTitle = GetLocalizableString(
        nameof(Resources.SelectAttributesAnalyserNoMultipleWithValuesTitle)
    );
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;Summary&lt;/h3&gt;

&lt;p&gt;That completes the analyser. I&#39;ve included the complete source code for the final implementation below - now that it&#39;s written it doesn&#39;t look like much, which hopefully illustrates how powerful and complete the Roslyn library is. And, hopefully, it&#39;s shown that this powerful library doesn&#39;t need to be daunting because there&#39;s many resources out there for helping you understand how to use it; people have written a lot about it and so Googling for terms relating to what you want to do often yields helpful results, people have answered a lot of questions about it on Stack Overflow and so you will often find example and sample code there.&lt;/p&gt;

&lt;p&gt;If you&#39;re not sure what terms to use to try to search for help then using the Syntax Visualizer to explore your code can set you on the right path, as can writing a test or two and then examining the &quot;context.Node&quot; reference in the debugger (if you do this then ensure that you are building your project in Debug mode since Release builds may prevent your breakpoints from being hit and may optimise some of the variable references away, which will mean that you won&#39;t be able to use QuickWatch on them). Finally, don&#39;t forget that there is a lot of helpful information in the xml summary documentation that&#39;s available in Visual Studio when you examine the Roslyn classes and their methods - often the names of methods are descriptive enough to help you choose the appropriate one or, at least, give you a clue as to what direction to go in.&lt;/p&gt;

&lt;p&gt;This has really only scraped the surface of what analysers are capable of, it&#39;s a technology with huge capability and potential. I might talk about other uses for analysers (or talk about how particular analysers may be implemented) another day but two topics that I definitely &lt;em&gt;will&lt;/em&gt; talk about soon are &quot;code fixes&quot; and how to get analysers to work with &lt;a href=&quot;http://bridge.net/&quot;&gt;Bridge.NET&lt;/a&gt; libraries.&lt;/p&gt;

&lt;p&gt;Code fixes are interesting because they allow you to go beyond just saying &quot;this is wrong&quot; to saying &quot;this is how it may be fixed (automatically, by the IDE)&quot;. For example, if someone changed a &lt;strong&gt;SelectAttributes&lt;/strong&gt; instantiation to enable multiple selections - eg. started with:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;DOM.Select(
    new SelectAttributes { Value = selectedId },
    options
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. and changed it to:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;DOM.Select(
    new SelectAttributes { Multiple = true, Value = selectedId },
    options
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.. then the analyser could point out that the &quot;Value&quot; property should not be used now that &quot;Multiple&quot; is true but it could also offer to fix it up to the following &lt;em&gt;automatically&lt;/em&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;DOM.Select(
    new SelectAttributes { Multiple = true, Values = new[] { selectedId } },
    options
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There will be times that the warning from an analyser will require manual intervention to correct but there will also be times where the computer could easily correct it, so it&#39;s great having the ability to explain to the computer &lt;em&gt;how&lt;/em&gt; to do so and thus make life that bit easier for the person consuming your library.&lt;/p&gt;

&lt;p&gt;The reason that I also want to spend a little bit of time talking about making analysers work with Bridge.NET libraries soon is that it&#39;s something of a special case since Bridge projects don&#39;t have references to the standard .net System, System.Collections, etc.. assemblies because they are replaced by special versions of those libraries that have JavaScript translations. This means that you can&#39;t reference a Bridge library from a project that relies on the standard .net assemblies, which is a bit of a problem when you want to write a Roslyn analyser for types in a Bridge library (since the analyser project will rely on standard .net assemblies and the analyser will want to reference the Bridge library whose rules are to be applied by the analyser). But there are ways to get around it and I&#39;ll go through that another time.&lt;/p&gt;

&lt;h3&gt;The complete analyser&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;using System.Collections.Immutable;
using System.Linq;
using System.Reflection;
using Microsoft.CodeAnalysis;
using Microsoft.CodeAnalysis.CSharp;
using Microsoft.CodeAnalysis.CSharp.Syntax;
using Microsoft.CodeAnalysis.Diagnostics;

namespace ExampleAnalyser
{
    [DiagnosticAnalyzer(LanguageNames.CSharp)]
    public sealed class ExampleAnalyserAnalyzer : DiagnosticAnalyzer
    {
        public const string DiagnosticId = &quot;Bridge.React&quot;;
        private static readonly LocalizableString Title = GetLocalizableString(
            nameof(Resources.SelectAttributesAnalyserTitle)
        );
        private static readonly LocalizableString MultipleWithValueTitle = GetLocalizableString(
            nameof(Resources.SelectAttributesAnalyserMultipleWithValueMessage)
        );
        private static readonly LocalizableString NoMultipleWithValuesTitle = GetLocalizableString(
            nameof(Resources.SelectAttributesAnalyserNoMultipleWithValuesTitle)
        );
        private const string Category = &quot;Configuration&quot;;

        private static DiagnosticDescriptor MultipleWithValueRule = new DiagnosticDescriptor(
            DiagnosticId,
            Title,
            MultipleWithValueTitle,
            Category,
            DiagnosticSeverity.Warning,
            isEnabledByDefault: true
        );
        private static DiagnosticDescriptor NoMultipleWithValuesRule = new DiagnosticDescriptor(
            DiagnosticId,
            Title,
            NoMultipleWithValuesTitle,
            Category,
            DiagnosticSeverity.Warning,
            isEnabledByDefault: true
        );

        public override ImmutableArray&amp;lt;DiagnosticDescriptor&amp;gt; SupportedDiagnostics
        {
            get { return ImmutableArray.Create(MultipleWithValueRule, NoMultipleWithValuesRule); }
        }

        public override void Initialize(AnalysisContext context)
        {
            context.RegisterSyntaxNodeAction(
                LookForInvalidSelectAttributeProperties,
                SyntaxKind.ObjectInitializerExpression
            );
        }

        private static void LookForInvalidSelectAttributeProperties(SyntaxNodeAnalysisContext context)
        {
            var propertyInitialisers = context.Node.ChildNodes()
                .OfType&amp;lt;AssignmentExpressionSyntax&amp;gt;()
                .Select(propertyInitialiser =&amp;gt; new
                {
                    PropertyName = ((IdentifierNameSyntax)propertyInitialiser.Left).Identifier.ValueText,
                    ValueExpression = propertyInitialiser.Right
                });

            var multiplePropertyInitialiser = propertyInitialisers.FirstOrDefault(
                propertyInitialiser =&amp;gt; propertyInitialiser.PropertyName == &quot;Multiple&quot;
            );
            bool multiplePropertyValue;
            if (multiplePropertyInitialiser == null)
                multiplePropertyValue = false; // Defaults to false if not explicitlt set
            else
            {
                var multiplePropertyValueKind = multiplePropertyInitialiser.ValueExpression.Kind();
                if (multiplePropertyValueKind == SyntaxKind.TrueLiteralExpression)
                    multiplePropertyValue = true;
                else if (multiplePropertyValueKind == SyntaxKind.FalseLiteralExpression)
                    multiplePropertyValue = false;
                else
                {
                    // Only looking for very simple cases - where explicitly set to true or to false or
                    // not set at all (defaulting to false). If it&#39;s set according to a method return
                    // value or a variable then give up (this is just intended to catch obvious
                    // mistakes, not to perform deep and complex analysis)
                    return;
                }
            }

            var valuePropertyIsSpecified = propertyInitialisers.Any(
                propertyInitialiser =&amp;gt; propertyInitialiser.PropertyName == &quot;Value&quot;
            );
            var valuesPropertyIsSpecified = propertyInitialisers.Any(
                propertyInitialiser =&amp;gt; propertyInitialiser.PropertyName == &quot;Values&quot;
            );
            if (!valuePropertyIsSpecified &amp;amp;&amp;amp; !valuesPropertyIsSpecified)
                return;

            var objectCreation = (ObjectCreationExpressionSyntax)context.Node.Parent;
            var objectCreationTypeInfo = context.SemanticModel.GetTypeInfo(objectCreation);
            if ((objectCreationTypeInfo.Type is IErrorTypeSymbol)
            || (objectCreationTypeInfo.Type.ContainingAssembly.Identity.Name != &quot;Bridge.React&quot;)
            || (objectCreationTypeInfo.Type.Name != &quot;SelectAttributes&quot;))
                return;

            if ((multiplePropertyValue == true) &amp;amp;&amp;amp; valuePropertyIsSpecified)
            {
                context.ReportDiagnostic(Diagnostic.Create(
                    MultipleWithValueRule,
                    context.Node.GetLocation()
                ));
            }
            else if ((multiplePropertyValue == false) &amp;amp;&amp;amp; valuesPropertyIsSpecified)
            {
                context.ReportDiagnostic(Diagnostic.Create(
                    NoMultipleWithValuesRule,
                    context.Node.GetLocation()
                ));
            }
        }

        private static LocalizableString GetLocalizableString(string nameOfLocalizableResource)
        {
            return new LocalizableResourceString(
                nameOfLocalizableResource,
                Resources.ResourceManager,
                typeof(Resources)
            );
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;</description>
			<pubDate>Tue, 28 Jun 2016 07:49:00 GMT</pubDate>
		</item>
		<item>
			<title>A static type system is a wonderful message to the present and future - Supplementary</title>
            <link>http://www.productiverage.com/a-static-type-system-is-a-wonderful-message-to-the-present-and-future-supplementary</link>
			<guid>http://www.productiverage.com/a-static-type-system-is-a-wonderful-message-to-the-present-and-future-supplementary</guid>
			<description>&lt;p&gt;This is an extension of my post &quot;&lt;a href=&quot;http://www.productiverage.com/a-static-type-system-is-a-wonderful-message-to-the-present-and-future&quot;&gt;A static type system is a wonderful message to the present and future&lt;/a&gt;. I initially rolled this all together into a single article but then decided to break it into two to make the first part easier to consume.&lt;/p&gt;

&lt;p&gt;So, what else did I want to say? Rather than just saying &quot;static typing is better&quot;, I want to express some more detailed &quot;for&quot; and &quot;against&quot; arguments. Spoiler alert: despite the negatives, I still believe that static typing is worth the effort.&lt;/p&gt;

&lt;h3&gt;FTW&lt;/h3&gt;

&lt;p&gt;I find that the more that I take advantage of the type system, the more reliable that my code becomes - not only in terms of how well it lasts over the years, but how likely that it is to work the first time that it compiles. Going back to some code that I wrote a few years ago, there are various parts of a particular project that deal with internationalisation - some parts want to know what language that particular content is in while some parts of more specific and want to know what language &lt;em&gt;culture&lt;/em&gt; it&#39;s in; the difference between &quot;English&quot; (the language) and &quot;English UK&quot; / &quot;en-GB&quot; (the language culture). I wish now that, for that project, I&#39;d created a type (in C#, a struct would have been the natural choice) to represent a &lt;strong&gt;LanguageKey&lt;/strong&gt; and another for a &lt;strong&gt;LanguageCultureKey&lt;/strong&gt; as I encountered several places where it was confusing which was required - some parts of the code had method arguments named &quot;language&quot; that wanted a language key while others had arguments named &quot;language&quot; that wanted a language culture key. The two parts of the project were written by different people at different times and, in both cases, it seemed natural to them to presume that &quot;language&quot; could mean a language key (since nothing more specific was required) or could mean a language culture (since they presumed that nothing &lt;em&gt;less&lt;/em&gt; specific would ever be required). This is an example of a place where better argument naming would have helped because it would have been easier to spot if a language culture key was being passed where a language key was required. However, it would have been better again if the compiler would spot the wrong key type being passed - a human might miss it if a naming convention is relied upon, but the compiler will never miss an invalid type.&lt;/p&gt;

&lt;p&gt;Another example that I&#39;ve used in the past is that of React &quot;props&quot; validation - when creating React components (which are used to render DOM elements.. or OS components, if you&#39;re using &lt;a href=&quot;http://www.reactnative.com&quot;&gt;React Native&lt;/a&gt;), you must provide specific information for the component; if it&#39;s a Label, for example, then you must provide a text string and maybe a class name string. If you&#39;re using JavaScript with React then you will probably be providing the props reference using simple object notation, so you will have to be careful that you remember that the text string is named &quot;text&quot; and not &quot;labelText&quot;. The React library includes support for a &quot;propTypes&quot; object to be defined for a component - this performs validation at runtime, ensuring that required properties have values and that they are of the correct type. If a &lt;a href=&quot;http://www.productiverage.com/writing-react-apps-using-bridgenet-the-dan-way-from-first-principles&quot;&gt;strongly-typed language (such as C#)&lt;/a&gt; was used to create and consume React components, then this additional runtime validation would not be required as the component&#39;s &quot;props&quot; class would be declared as a class and all properties would have the appropriate types specified there. These would be validated at compile time, rather than having to wait until runtime. Returning to the &quot;&lt;a href=&quot;https://m.signalvnoise.com/provide-sharp-knives-cc0a22bf7934#.yv2771vf7&quot;&gt;Sharp Knives&lt;/a&gt;&quot; quote, this may be construed as being validation written for &quot;other programmers&quot; - as in, &quot;I don&#39;t want other programmers to try to use my component incorrectly&quot; - but, again, I&#39;m very happy to be the one of the &quot;other programmers&quot; in this case, it allows the type system to work as very-welcome documentation.&lt;/p&gt;

&lt;p&gt;While we&#39;re talking about React and component props, the React library always treats the props reference for a component as being immutable. If the props data needs to change then the component needs to be re-rendered with a new props reference. If you are writing your application in JavaScript then you need to respect this convention. However, if you choose to write your React application in a strongly-typed language then you may have your props classes represented by immutable types. This enforces this convention through the type system - &lt;em&gt;you&lt;/em&gt; (and anyone reviewing your code) don&#39;t have to keep a constant vigil against accidental mutations, the compiler will tell you if this is attempted (by refusing to build and pointing out where the mistake made).&lt;/p&gt;

&lt;p&gt;The common thread, for me, in all of the reasons why static typing is a good thing is that it enforces things that I want (or that I require) to be enforced, while providing invaluable information and documentation through the types. This makes code easier to reason about and code that is easier to reason about is easier to maintain and extend.&lt;/p&gt;

&lt;h3&gt;What static typing can&#39;t solve&lt;/h3&gt;

&lt;p&gt;It&#39;s not a silver bullet. But, then, nothing is. Static typing is a valuable tool that should be used &lt;em&gt;with&lt;/em&gt; automated test in order to create a reliable product.&lt;/p&gt;

&lt;p&gt;To take a simple example that will illustrate a variety of principles, the following is a LINQ call made in C# to take a set of &lt;strong&gt;EmployeeDetails&lt;/strong&gt; instances and determine the average age (we&#39;ll assume that &lt;strong&gt;EmployeeDetails&lt;/strong&gt; is a class with an integer Age property) -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var averageAge = employees.Average(employee =&amp;gt; employee.Age);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If we were implementing the &quot;Average&quot; function ourselves, then we would need to populate the following -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public static int Average&amp;lt;T&amp;gt;(this IEnumerable&amp;lt;T&amp;gt; source, Func&amp;lt;T, int&amp;gt; selector)
{
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Static typing gives us a lot of clues here. It ensures that anyone calling &quot;Average&quot; has to provide a set of values that may be enumerated and they have to provide a lambda that extracts an integer from each of those values. If the caller tried to provide a lambda that extracted a string (eg. the employee&#39;s name) from the values then it wouldn&#39;t compile. The type signature documents many of the the requirements of the method.&lt;/p&gt;

&lt;p&gt;However, the type system does not ensure that the implementation of &quot;Average&quot; is correct. It would be entirely possible to write an &quot;Average&quot; function that returned the &lt;em&gt;highest&lt;/em&gt; value, rather than the &lt;a href=&quot;https://en.wikipedia.org/wiki/Arithmetic_mean&quot;&gt;mean&lt;/a&gt; value.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This is what unit tests are for.&lt;/em&gt; Unit tests will ensure that the logic within a method is correct. It will ensure that 30 is returned from &quot;Average&quot; if a set of employees with ages 20, 30 and 40 are provided.&lt;/p&gt;

&lt;p&gt;The type system ensures that the code is not called with inappropriate data. If you didn&#39;t have a static type system then it would still be possible to write more unit tests around the code that called &quot;Average&quot; to ensure that it was always dealing with appropriate data - but this is an entire class of tests that are not required if you leverage static analysis*.&lt;/p&gt;

&lt;p&gt;Unfortunately, there &lt;em&gt;are&lt;/em&gt; limitations to what may be expressed in the type system. In the &quot;Average&quot; example above, there is no way (in C#) to express the fact that it&#39;s invalid for a null &quot;source&quot; or &quot;selector&quot; reference to be passed or a &quot;source&quot; reference that has zero elements (since there is no such thing as an average value if there are zero values) or a set of items where one of more of the values is null. Any of these cases of bad data will result in a runtime error. However, I believe that the solution to this is not to run away screaming from static typing because it&#39;s not perfect - in fact, I think that the answer is &lt;em&gt;more&lt;/em&gt; static analysis. &lt;a href=&quot;https://github.com/Microsoft/CodeContracts&quot;&gt;Code Contracts&lt;/a&gt; is a way to include these additional requirements in the type system; to say that &quot;source and selector may not be null&quot; and &quot;source may not be empty&quot; and &quot;source may not contain any null references&quot;. Again, this will be a way for someone consuming the code to have greater visibility of its requirements &lt;em&gt;and&lt;/em&gt; for the compiler to enforce them. I will be able to write stricter code to stop other people from making mistakes with it, and other people will be able to write stricter code to make it clearer to me how it should be used and prevent me from making mistakes or trying to use it in ways that is not supported. &lt;em&gt;I don&#39;t want the power to try to use code incorrectly&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;I think that there are two other obvious ways that static typing can&#39;t help and protect you..&lt;/p&gt;

&lt;p&gt;Firstly, when dealing with an external system there may be additional rules that you can not (and would not want to, for the sake of preventing duplication) describe in your code. Perhaps you have a data store that you pass updates to in order to persist changes made by a user - say the user wants to change the name of an employee in the application, so an &lt;strong&gt;UpdateEmployeeName&lt;/strong&gt; action must be sent to the data service. This action will have an integer &quot;Key&quot; property and a string &quot;Name&quot; property. This class structure ensures that data of the appropriate form is provided but it can &lt;em&gt;not&lt;/em&gt; ensure that the Key itself is valid - only the data store will know that. The type system is not an all-seeing-all-knowing magician, so it &lt;em&gt;will&lt;/em&gt; allow some invalid states to be represented (such as an update action for an entity key that doesn&#39;t exist). But the more invalid states that may &lt;em&gt;not&lt;/em&gt; be represented (such as not letting the key, which the data service requires to be an integer, be the string &quot;abc&quot; - for example) means that there are less possible errors states to test against and the code is more reliable (making it harder to write incorrect code will make the code more correct overall and hence more reliable).&lt;/p&gt;

&lt;p&gt;Secondly, if the type system is not taken advantage to the fullest extent then it can&#39;t help you to the fullest extent. I have worked on code in the past where a single class was used in many places to represent variations on the same data. Sometimes a &quot;Hotel&quot; instance would describe the entity key, the name, the description. Sometimes the &quot;Hotel&quot; instance would contain detailed information about the rooms in the hotel, sometimes the &quot;Rooms&quot; property would be null. Sometimes it would have its &quot;Address&quot; value populated, other times it would be null. It would depend upon the type of request that the &quot;Hotel&quot; instance was returned for.  This is a poor use of the type system - different response types should have been used, it should have been clear from the returned type what data would be present. The more often we&#39;re in a &quot;sometimes this, sometimes that&quot; situation, the less reliable that the code will be as it becomes easier to forget one of &quot;sometimes&quot; cases (again, I&#39;m talking from personal experience and not just worrying about how this may or may not affect &quot;other programmers&quot;). Unfortunately, not even the potential for a strong type system can make shitty code good.&lt;/p&gt;

&lt;p&gt;* &lt;em&gt;(It&#39;s probably worth stating that a static type system is one way that tooling can automatically identify mistakes for you but it&#39;s not the only way - &lt;a href=&quot;https://github.com/Microsoft/CodeContracts&quot;&gt;code contracts&lt;/a&gt; are a way to go beyond what C# can support &quot;out of the box&quot; but there are other approaches, such as what &lt;a href=&quot;http://www.gamasutra.com/view/news/128836/InDepth_Static_Code_Analysis.php&quot;&gt;John Carmark has written about static analysis of C++&lt;/a&gt; or how &lt;a href=&quot;http://flowtype.org/&quot;&gt;Facebook is analysing JavaScript without even requiring types to be explicitly declared&lt;/a&gt;&lt;/em&gt;)&lt;/p&gt;

&lt;h3&gt;Code Reviews&lt;/h3&gt;

&lt;p&gt;Another quote that stuck out for me in the &quot;&lt;a href=&quot;https://m.signalvnoise.com/provide-sharp-knives-cc0a22bf7934#.yv2771vf7&quot;&gt;Sharp Knives&lt;/a&gt;&quot; post was that&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;We enforce such good senses by convention, by nudges, and through education&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is very sensible advice. I think that one of the best ways for code quality to remain high is through developers working together - learning from each other and supporting each other. This is something that I&#39;ve found code reviews to be very effective for. If all code is reviewed, then all code is guaranteed to have been read by at least two people; the author and the reviewer. If the code is perfect, then that&#39;s where the review ends - on a high note. If the code needs work then any mistakes or improvements can be highlighted and addressed. As the recipient of a review that identifies a mistake that I&#39;ve made, I&#39;m happy! Well.. I&#39;m generally a bit annoyed with myself for making the mistake but I&#39;m glad that a colleague has identified it rather than it getting to an end user.&lt;/p&gt;

&lt;p&gt;As a code reviewer, I will be happy with code that I think requires no changes or if code needs to be rejected only once. I&#39;ve found that code that is rejected and then fixed up is much harder to re-review and that bugs more often slip through the re-review process. It&#39;s similar to the way in which you can more easily become blind to bugs in code that you&#39;ve just written than you are to someone else&#39;s code - you have a familarity with the code that you are reviewing for a second time &lt;em&gt;and&lt;/em&gt; someone has just told you that they have fixed it; I&#39;ve found that there is something psychological about that that makes it just that little bit harder to pick up on any subsequent mistakes. Thusly, I would prefer to limit the number of times that reviews bounce back and forth.&lt;/p&gt;

&lt;p&gt;I have found that a static type system encourages a stricter structure on the code and that conventions are clearer, not to mention the fact that the compiler can identify more issues - meaning that there should be fewer types of mistake that can get through to a review. There is, of course, a limit to what the type system can contribute on this front but &lt;em&gt;any&lt;/em&gt; mechanical checks that a computer could perform leave the reviewer more time (and mental capacity) to provide deeper insight; to offer guidance to a more junior developer or to suggest implementation tweaks to a peer.&lt;/p&gt;

&lt;h3&gt;A &quot;wonderful message&quot;&lt;/h3&gt;

&lt;p&gt;It&#39;s a theme that has picked up more and more weight for me over the years, that the computer should be able to help me tell it what to do - I should be able to leverage its strengths in order to multiply mine. As a developer, there is a lot of creativity required but also a huge quantity of seemingly banal details. The strength of a good abstraction comes from being able to &quot;hide away&quot; details that don&#39;t matter, leaving you with larger and more useful shapes to deal with, allowing you to think closer to the big picture. The more details that may be automatically verified, the less that you need to worry about them; freeing up more valuable mental space. Leaning on static analysis aids this, it allows the computer to do what it&#39;s good at and concentrate on the simple-and-boring rules, allowing you to become more effective. It&#39;s an incredibly powerful tool, the ability to actually limit certain things from being done allows you to do &lt;em&gt;more&lt;/em&gt; of what you should be doing.&lt;/p&gt;

&lt;p&gt;It can also be an invaluable form of documentation for people using your code (including you, in six months, when you&#39;ve forgotten the finer details). Good use of the type system allows for the requirements and the intent of code to be clearer. It&#39;s not just a way of communicating with the compiler, it&#39;s also a very helpful way to communicate with human consumers of your code.&lt;/p&gt;

&lt;p&gt;On a personal note, this marks my 100th post on this blog. The first (&lt;a href=&quot;http://www.productiverage.com/i-love-immutable-data&quot;&gt;I love Immutable Data&lt;/a&gt;) was written about five years ago and was &lt;em&gt;also&lt;/em&gt; (basically) about leveraging the type system - by defining immutable types and the benefits that they could have. I find it reassuring that, with all that I&#39;ve seen since then (and thinking back over the time since I first started writing code.. over 25 years ago) that this still feels like a good thing. In a time where it seems like everyone&#39;s crying about JavaScript fatigue (and the frequent off-the-cuff comments about React being &quot;&lt;a href=&quot;https://camo.githubusercontent.com/a85f7c2c03b36655323ec7a3250057233e82ef55/68747470733a2f2f692e696d6775722e636f6d2f695549497571622e6a7067&quot;&gt;so hot right now&lt;/a&gt;&quot;*), I&#39;m glad that there are still plenty of principles that stand the test of time.&lt;/p&gt;

&lt;p&gt;* &lt;em&gt;(Since I&#39;m feeling so brave and self-assured, I&#39;m going to say that I think that React *will* still be important five years from now - maybe I&#39;ll look back in 2021 and see how this statement has fared!)&lt;/em&gt;&lt;/p&gt;</description>
			<pubDate>Tue, 31 May 2016 21:34:00 GMT</pubDate>
		</item>

	</channel>

</rss>